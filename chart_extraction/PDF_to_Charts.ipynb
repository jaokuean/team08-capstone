{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chart Extraction \n",
    "## 1. Install all the prerequisite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdf2image\n",
    "!pip install pytesseract\n",
    "!pip install opencv-python\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use for checking files in dir\n",
    "import os\n",
    "\n",
    "# Extract each pdf page to image\n",
    "from pdf2image import convert_from_path, convert_from_bytes \n",
    "from pdf2image.exceptions import (\n",
    " PDFInfoNotInstalledError,\n",
    " PDFPageCountError,\n",
    " PDFSyntaxError\n",
    ")\n",
    "\n",
    "# Image Processing\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import pytesseract\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Program Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal parameters for graph detection\n",
    "set_column_gap = 50\n",
    "set_height_limit = 180\n",
    "set_width_limit = 180\n",
    "set_area_limit = 100000\n",
    "\n",
    "set_scale_factor = 3.2 # Extend horizontal and vertical axis of bounding boxes\n",
    "scale_horizontal = set_scale_factor*64\n",
    "scale_vertical = set_scale_factor*64\n",
    "\n",
    "def zero_runs(a):\n",
    "    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n",
    "    iszero = np.concatenate(([0], np.equal(a, 0).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    return ranges\n",
    "\n",
    "# Method to check if extracted article is valid\n",
    "def checkDim(height, area):\n",
    "    #print(f\"{height} x {width} = {area}\")\n",
    "    if(height <= set_height_limit):\n",
    "        return False\n",
    "    if(area <= set_area_limit):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def process_Num(text):\n",
    "    no_punct = re.sub('[^0-9\\n\\.]', ' ', text)\n",
    "    res = no_punct.split()\n",
    "    my_list = set(res)  \n",
    "    to_delete = [\".\"]\n",
    "    \n",
    "    my_list.difference_update(to_delete)\n",
    "    results = list(my_list)\n",
    "    return results\n",
    "\n",
    "def process_text(text):\n",
    "    no_punct = re.sub('[^a-zA-Z\\n\\.]', ' ', text)\n",
    "    res = no_punct.split()\n",
    "    my_list = set(res)  \n",
    "    to_delete = [\".\"]\n",
    "    \n",
    "    my_list.difference_update(to_delete)\n",
    "    results = list(my_list)\n",
    "    return results\n",
    "\n",
    "def find_keywords(text):\n",
    "    no_punct = re.sub('[^a-zA-Z0-9\\n\\.]', ' ', text)\n",
    "    # list of keywords\n",
    "    dictionary = ['carbon','ghg','emission',\n",
    "                 'emissions',\"scope\", \"WACI\",\"net-zero\",\n",
    "                 'energy','water','waste','coal','power','green','paper','consumption','renewable',\n",
    "                 'breakdown','loans','tonnes', 'tons', 'kWh', 'kg', 'kilogram', 'kilowatt hour', \n",
    "                   'gigajoules', 'GJ', 'litre', 'liter', 'CO2e', 'tCO', 't CO', 'MWh', \n",
    "                   'megawatt hour', '%', 'cubic metres', 'per employee','m3', 'co2','o2'\n",
    "                    ,'million', 'total','trillion','set'\n",
    "               ]\n",
    "    res = set(no_punct.lower().split())\n",
    "    newlength = len(res)\n",
    "    res.difference_update(dictionary)\n",
    "    results = list(res)\n",
    "    value = newlength - len(results) \n",
    "    return value\n",
    "\n",
    "def count_clean_text(text):\n",
    "    no_punct = re.sub('[^a-zA-Z\\n\\.]', ' ', text)\n",
    "    res = no_punct.split()\n",
    "    return len(res)\n",
    "\n",
    "def count_clean_num(text):\n",
    "    no_punct = re.sub('[^0-9\\n\\.]', ' ', text)\n",
    "    res = no_punct.split()\n",
    "    return len(res)\n",
    "\n",
    "def getTotalLen(text):\n",
    "    no_punct = re.sub('[^a-zA-Z0-9\\n\\.]', ' ', text) # remove punctuations\n",
    "    res = no_punct.split()\n",
    "    return len(res)\n",
    "\n",
    "def filter_relevance(filter_img):\n",
    "    try:\n",
    "        text = pytesseract.image_to_string(filter_img)\n",
    "    except: \n",
    "        print(\"ERROR at Textserract\")\n",
    "        return\n",
    "    \n",
    "    # Filter images with too much text\n",
    "    # Total length of text\n",
    "    total_len = getTotalLen(text)\n",
    "    \n",
    "    # Filter images with too little or no keywords\n",
    "    # Total unique ketywords found\n",
    "    keywords = find_keywords(text)\n",
    "    \n",
    "    # Get total unique string\n",
    "    textonly_len = len(process_text(text))\n",
    "    \n",
    "    # Get total unique numbers/digits\n",
    "    numonly_len = len(process_Num(text))\n",
    "    \n",
    "    # Get text to total text ratio = clean_text/total text\n",
    "    try:\n",
    "        tt_ratio = count_clean_text(text)/total_len\n",
    "    except:\n",
    "        tt_ratio = 0\n",
    "        \n",
    "    # Basic shape descriptive data\n",
    "    height = filter_img.shape[0] \n",
    "    width = filter_img.shape[1]\n",
    "    channels = filter_img.shape[2]# number of components used to represent each pixel.\n",
    "    area = height * width\n",
    "\n",
    "    # num to area ratio = total num to whole image \n",
    "    na_ratio = count_clean_num(text)/area * 10**5\n",
    "    \n",
    "    # Color processing\n",
    "    # Filter by BW instead of colors, cause too similar\n",
    "    gray = cv2.cvtColor(filter_img, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV,11,2)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,1))\n",
    "    dilate = cv2.dilate(thresh, kernel, iterations=1)\n",
    "    # Count total white and black ratio\n",
    "    white_pix = np.sum(dilate == 255)\n",
    "    black_pix = np.sum(dilate == 0)\n",
    "    # b/w ratio\n",
    "    bw_ratio = black_pix/white_pix\n",
    "    \n",
    "    if(keywords>0 and tt_ratio < 0.99 and bw_ratio>14 and textonly_len<90 and numonly_len>1 and na_ratio>0.45 ):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def filterImage(img): \n",
    "    try:\n",
    "        text = pytesseract.image_to_string(img)\n",
    "    except: \n",
    "        print(\"ERROR at Textserract\")\n",
    "        return\n",
    "    \n",
    "    # Filter images with too much text\n",
    "    # Total length of text\n",
    "    total_len = getTotalLen(text)\n",
    "    \n",
    "    # Basic shape descriptive data\n",
    "    height = img.shape[0] \n",
    "    width = img.shape[1]\n",
    "    channels = img.shape[2]# number of components used to represent each pixel.\n",
    "    area = height * width\n",
    "    # text to area ratio = total text to whole image \n",
    "    ta_ratio = count_clean_text(text)/area * 10**5\n",
    "    \n",
    "    # Color processing\n",
    "    # Filter by BW instead of colors, cause too similar\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV,11,2)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,1))\n",
    "    dilate = cv2.dilate(thresh, kernel, iterations=1)\n",
    "    dilate_raw = cv2.mean(dilate)[::-1]\n",
    "    dilated_region = int(dilate_raw[3])\n",
    "    \n",
    "    # Count total white pixels\n",
    "    white_pix = np.sum(dilate == 255)\n",
    "    \n",
    "    # region dilated + area captured\n",
    "    if((7 < dilated_region<26)and (7000<white_pix<90000) and total_len < 68 and ta_ratio <10):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def scale_image(x_top, y_top, x_bot, y_bot ,result):\n",
    "\n",
    "    # Extend bounding lines\n",
    "    new_x_top = x_top -int(scale_horizontal)\n",
    "    new_y_top = y_top -int(scale_vertical)\n",
    "    new_x_bot = x_bot +int(scale_horizontal)\n",
    "    new_y_bot = y_bot +int(scale_vertical)\n",
    "\n",
    "    # To prevent error on -ve values\n",
    "    if(new_x_top < 0):\n",
    "        new_x_top = 0\n",
    "    if(new_y_top < 0):\n",
    "        new_y_top = 0\n",
    "    if(new_x_bot < 0):\n",
    "        new_x_bot = 0\n",
    "    if(new_y_bot < 0):\n",
    "        new_y_bot = 0\n",
    "\n",
    "    # Re-calculate bounding lines\n",
    "    new_width = new_x_bot - new_x_top\n",
    "    new_height = new_y_bot - new_y_top\n",
    "\n",
    "    # if condition pass, crop image and output\n",
    "    X, Y, W, H = new_x_top, new_y_top, new_width, new_height\n",
    "\n",
    "    cropped_image = result[Y:Y+H, X:X+W] \n",
    "\n",
    "    return cropped_image\n",
    "    \n",
    "def process_image(img, pageNum, task,  img_list, fileHeader):\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # turn img to grey\n",
    "    img_gray_inverted = 255 - img_gray # Invert back to normal\n",
    "\n",
    "    row_means = cv2.reduce(img_gray_inverted, 1, cv2.REDUCE_AVG, dtype=cv2.CV_32F).flatten()\n",
    "    row_gaps = zero_runs(row_means)\n",
    "    row_cutpoints = (row_gaps[:,0] + row_gaps[:,1] - 1) / 2\n",
    "    \n",
    "    bounding_boxes = []\n",
    "    for n,(start,end) in enumerate(zip(row_cutpoints, row_cutpoints[1:])):\n",
    "        line = img[int(start):int(end)]\n",
    "        line_gray_inverted = img_gray_inverted[int(start):int(end)]\n",
    "\n",
    "        column_means = cv2.reduce(line_gray_inverted, 0, cv2.REDUCE_AVG, dtype=cv2.CV_32F).flatten()\n",
    "        column_gaps = zero_runs(column_means)\n",
    "        column_gap_sizes = column_gaps[:,1] - column_gaps[:,0]\n",
    "        column_cutpoints = (column_gaps[:,0] + column_gaps[:,1] - 1) / 2\n",
    "\n",
    "        filtered_cutpoints = column_cutpoints[column_gap_sizes > set_column_gap]\n",
    "\n",
    "        for xstart,xend in zip(filtered_cutpoints, filtered_cutpoints[1:]):\n",
    "            bounding_boxes.append(((int(xstart), int(start)), (int(xend), int(end))))\n",
    "\n",
    "    count = 0\n",
    "    result = img.copy()\n",
    "\n",
    "    for bounding_box in bounding_boxes:\n",
    "        count = count + 1 # count number of images extracted\n",
    "\n",
    "        x_top = bounding_box[0][0]\n",
    "        y_top = bounding_box[0][1]\n",
    "        x_bot = bounding_box[1][0]\n",
    "        y_bot = bounding_box[1][1]\n",
    "        \n",
    "        height = y_bot - y_top # height of image extracted\n",
    "        width = x_bot - x_top # width of image extracted\n",
    "        area = height * width # area of image extracted\n",
    "        \n",
    "        X, Y, W, H = x_top, y_top, width, height\n",
    "        # Primary Filtering\n",
    "        if(checkDim(height, area) == True):\n",
    "            print(f\"============ PASSED for AREA FILTER {pageNum}_{task}_{count}===================\")\n",
    "            # if condition pass, crop image and output\n",
    "\n",
    "            cropped_image = result[Y:Y+H, X:X+W]\n",
    "\n",
    "            article_output_file = fileHeader + \"/filter_1/page\" + str(pageNum) + \"_\"+str(task)+\"_\" +str(count) +\".png\"\n",
    "            \n",
    "            cv2.imwrite(article_output_file, cropped_image)\n",
    "            \n",
    "            # For data collection\n",
    "            # article_output_file_2 = \"ChartExtraction_Output/out/filter_1/page\" + str(pageNum) + \"_\"+str(task)+\"_\" +str(count) +\".png\"\n",
    "            # cv2.imwrite(article_output_file_2, cropped_image)\n",
    "\n",
    "            # Black-White Filtering \n",
    "            filter_img = cv2.imread(article_output_file, cv2.IMREAD_COLOR)\n",
    "            if filterImage(filter_img) == True: \n",
    "                scale_image_size = scale_image( x_top, y_top, x_bot, y_bot ,result) \n",
    "                \n",
    "                article_output_file = fileHeader + \"/filter_2/page\" + str(pageNum) + \"_\"+str(task)+\"_\" +str(count) +\".png\"\n",
    "                try:\n",
    "                    cv2.imwrite(article_output_file, scale_image_size)\n",
    "                except:\n",
    "                    print(\"ERROR at Black-White Filter\")\n",
    "                    print(scale_image_size)\n",
    "                filter_img = cv2.imread(article_output_file, cv2.IMREAD_COLOR)\n",
    "                # Relevance Filtering\n",
    "                if filter_relevance(filter_img) == True: \n",
    "                    print(f\"============ ACCEPTED {pageNum}_{task}_{count}===================\") \n",
    "                    ROI_image_path = fileHeader + \"/ROI_\" +str(pageNum)+ \"_\"+ str(task) + \"_\"+ str(count) + \".png\"\n",
    "                    cv2.imwrite(ROI_image_path, filter_img)\n",
    "                    img_list.append(ROI_image_path)\n",
    "                else:\n",
    "                    print(f\"============ REJECTED at RELEVANCE FILTER {pageNum}_{task}_{count}===================\")\n",
    "            else:\n",
    "                print(f\"============ REJECTED for COLOR FILTER {pageNum}_{task}_{count}===================\") \n",
    "        else:\n",
    "            print(f\"============ REJECTED for AREA FILTER {pageNum}_{task}_{count}===================\")\n",
    "\n",
    "    return img_list\n",
    "\n",
    "def isLandscape(h,w):\n",
    "    if(w>h):\n",
    "        print(\"landscape\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"portrait\")\n",
    "        return False\n",
    "    \n",
    "def page_to_articles(pageNum, fileHeader):\n",
    "    print(f\"=>starting on page {pageNum} : {fileHeader}\")\n",
    "    img_list = []\n",
    "    img = cv2.imread(fileHeader+'/pages/%s.png' %pageNum, cv2.IMREAD_COLOR) # Identify img\n",
    "    h,w,c = img.shape\n",
    "    if(isLandscape(h,w) == True):\n",
    "        width_cutoff = w // 2\n",
    "        s1 = img[:, :width_cutoff]\n",
    "        s2 = img[:, width_cutoff:]        \n",
    "        process_image(s1, pageNum, \"a\", img_list, fileHeader)\n",
    "        process_image(s2, pageNum, \"b\", img_list, fileHeader)\n",
    "    else:\n",
    "        process_image(img, pageNum,\"0\", img_list, fileHeader)\n",
    "        \n",
    "    return img_list\n",
    "    \n",
    "def chart_extraction(url, pages, copy_to_path):\n",
    "    # check if URL is pdf\n",
    "    if \".pdf\" not in url:\n",
    "        print(\"URL is not a PDF.\")\n",
    "        return \"nan\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except:\n",
    "        print(\"Requests failed.\")\n",
    "        return \"nan\"\n",
    "    \n",
    "    i = 1\n",
    "    image_path_obj = {}\n",
    "    \n",
    "    # Check if pdf is landscape or portrait\n",
    "    \n",
    "    \n",
    "    # Convert relevant pages to images for processing \n",
    "    response = requests.get(url, timeout=30)\n",
    "    images = convert_from_bytes(response.content)\n",
    "       \n",
    "    for i, image in enumerate(images):\n",
    "        target_file = copy_to_path+\"/pages\"# Create dir for page output\n",
    "        if not os.path.exists(target_file):\n",
    "            # file exists\n",
    "            os.mkdir(target_file)\n",
    "            \n",
    "        if str(i) not in pages:\n",
    "            continue\n",
    "        print(f\"==> Convert page {i} of pdf to image...\")\n",
    "        image.save(f\"{target_file}/{str(i)}.png\")\n",
    "        \n",
    "    target_file_a = copy_to_path+\"/filter_1\"# Create dir for chart extraction output\n",
    "    if not os.path.exists(target_file_a):\n",
    "        # file does not exists \n",
    "        os.makedirs(target_file_a) \n",
    "        \n",
    "    target_file_b = copy_to_path+\"/filter_2\"# Create dir for test outputs \n",
    "    if not os.path.exists(target_file_b):\n",
    "        # file does not exists \n",
    "        os.makedirs(target_file_b)  \n",
    "    try: \n",
    "        for page in pages:\n",
    "            print(f\"==> Now doing page {page} ...\")\n",
    "            img_list = page_to_articles(page, copy_to_path)\n",
    "            print(f\"==> Finished Page {page} ...\\n\")\n",
    "            image_path_obj[str(page)] = img_list\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR at page_to_article: \")\n",
    "        if hasattr(e, 'message'):\n",
    "            print(e.message)\n",
    "        else:\n",
    "            print(e)\n",
    "        image_path_obj[str(page)] = \"nan\"\n",
    "\n",
    "    return image_path_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "#test_set = open('preprocessed/test_set.json',)\n",
    "\n",
    "def run_extraction_main(img_path, out_folder):\n",
    "    f = open(img_path,)\n",
    "\n",
    "    data = json.load(f)\n",
    "\n",
    "    source = out_folder + \"/\"\n",
    "    if not os.path.exists(source):\n",
    "        # file exists\n",
    "        os.mkdir(source)\n",
    "\n",
    "    json_lst = []\n",
    "    # x_reports to look at for testing purposes.\n",
    "    x_report = 50\n",
    "    for i in data[:x_report]: # Load first x_reports from json\n",
    "        company = i['company']\n",
    "        year = i['year']\n",
    "        pdf_url = i['url']\n",
    "        pages = []\n",
    "        for j in i['filtered_report_tables_direct']:\n",
    "            pages.append(j)\n",
    "\n",
    "        json_obj = {}\n",
    "        json_obj['company'] = company\n",
    "        json_obj['year'] = year\n",
    "        json_obj['pdf_url'] = pdf_url\n",
    "\n",
    "        path = source + company + '_' + year\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            # file exists\n",
    "            os.mkdir(path)\n",
    "\n",
    "        try:\n",
    "            json_obj['images_path'] = chart_extraction(pdf_url, pages, path)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR at chart_extraction: \")\n",
    "            if hasattr(e, 'message'):\n",
    "                print(e.message)\n",
    "            else:\n",
    "                print(e)\n",
    "            json_obj['images_path'] = \"nan\"\n",
    "\n",
    "        json_lst.append(json_obj)\n",
    "        print(f\"++++++++++ Report done ++++++++++\")\n",
    "\n",
    "    print(f\"================================ JOB COMPLETE =======================================\")\n",
    "\n",
    "    # Closing file\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Starting point of program\n",
    "- Before you start, create a folder in your current dir to store your output. (For e.g ChartExtraction_Output)\n",
    "- Ensure you have enough space to store images to run. (About >3gb is recommended for all FI, >1gb for one FI)\n",
    "- The program will create folders and input the image inside for processing.\n",
    "- use 'ls' below to check your local dir now and check the folder you create is in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = \"ChartExtraction_Output\" # name of your folder output\n",
    "if not os.path.exists(out_folder):\n",
    "    os.mkdir(out_folder)\n",
    "else:\n",
    "    print(\"===> Folder exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT \n",
    "# MAIN INPUT: Can only read .json file that XM created (For e.g all_asian_banks_preprocessed.json )\n",
    "# MAIN OUTPUT: ROI images\n",
    "# Please specify data source (Only .json format with same structure)\n",
    "\n",
    "data_source_file = \"preprocessed/\" # Replace with .json type data source\n",
    "for filename in os.listdir(data_source_file):\n",
    "    if filename.endswith(\".json\"):\n",
    "        img_path = os.path.join(data_source_file, filename)\n",
    "        run_extraction_main(img_path, out_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil # high-level folder management library\n",
    "def rm_processing_folders(out_folder):\n",
    "    total_ROI = 0\n",
    "    if not os.path.exists(out_folder):\n",
    "        print(\"===> Folder do not exist\")\n",
    "    else:\n",
    "        hidden_file = out_folder+\"/.DS_Store\"\n",
    "        if os.path.exists(hidden_file):\n",
    "            os.remove(hidden_file)\n",
    "        \n",
    "        removed_folders = []\n",
    "        for filename in os.listdir(out_folder):\n",
    "            print(f\"[Cleaning {filename}]\")   \n",
    "            img_path = os.path.join(out_folder, filename)\n",
    "            removed_folders = []\n",
    "            for sub_filename in os.listdir(img_path):\n",
    "                if sub_filename.endswith(\"filter_1\") or sub_filename.endswith(\"filter_2\") or sub_filename.endswith(\"pages\"):\n",
    "                    delete_path = os.path.join(img_path, sub_filename)\n",
    "                    # print(f\" ====>{delete_path}\")\n",
    "                    removed_folders.append(delete_path)\n",
    "                    shutil.rmtree(delete_path)\n",
    "            \n",
    "            # Calculate total ROI\n",
    "            roi_count = len(os.listdir(os.path.join(out_folder, filename)))\n",
    "            total_ROI = total_ROI + roi_count\n",
    "            \n",
    "            print(f\"=> ROI count: {roi_count} \")\n",
    "            print(f\"=> Succesfuly removed: {removed_folders}\\n\") \n",
    "            \n",
    "    return total_ROI\n",
    "\n",
    "# Delete processing folders\n",
    "# out_folder = \"ChartExtraction_Output\" \n",
    "try:\n",
    "    ROI_count = rm_processing_folders(out_folder)\n",
    "    print(f\"[Summary Report]\\n Time Completed: {datetime.datetime.now}  \\nTotal ROI: {ROI_count}\")\n",
    "except Exception as e: \n",
    "    print(f\"Fail to remove: {e}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The program ends when this code is runned\n",
    "# Approx timing: 30-45 mins for all asian banks (40 reports) \n",
    "with open('chart_output.json', 'w') as f:\n",
    "    json.dump(json_lst, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
