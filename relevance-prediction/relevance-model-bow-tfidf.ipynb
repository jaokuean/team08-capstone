{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xinminaw/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/xinminaw/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from preprocessing import *\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, cross_validate\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = pd.read_csv('../data/all_labelled_17Oct.csv')\n",
    "df1 = pd.read_csv('../data/labelled_data/all_labelled_17Oct.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns = ['index', 'sentence', 'relevance', 'carbon_class']\n",
    "df1['cleaned_sentence'] = df1['sentence'].apply(clean_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split labelled set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = \\\n",
    "              np.split(df1.sample(frac=1, random_state=4103), \n",
    "                       [int(.6*len(df1)), int(.8*len(df1))])\n",
    "trainval =pd.concat([train, val])# Split labelled set\n",
    "labels = [train.relevance, val.relevance, test.relevance, trainval.relevance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_params = {\n",
    "    \"analyzer\": [\"word\"],\n",
    "    \"lowercase\": [True],\n",
    "    \"ngram_range\": [(1,1), (1,2), (1,3)],\n",
    "    \"max_df\": [0.25, 0.5, 1.0],\n",
    "    \"min_df\": [1, 10, 20]\n",
    "}\n",
    "vect_paramgrid = list(ParameterGrid(vect_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_methods = {'type': ['bow_raw', 'tfidf_clean']} #, 'processing': ['clean','raw']}\n",
    "vect_methods_paramgrid = list(ParameterGrid(vect_methods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "logreg_params = { \"C\": [0.1, 0.5, 1.0, 5], \n",
    "                 \"solver\": [\"lbfgs\", \"newton-cg\"], \n",
    "                 \"penalty\": [\"l2\", \"none\"],\n",
    "                 \"class_weight\": [\"balanced\", None]}\n",
    "logreg_paramgrid = list(ParameterGrid(logreg_params))\n",
    "\n",
    "# naive bayes\n",
    "nb_params = {\"alpha\": [0, 0.001, 0.01, 0.1, 0.25, 0.5, 1]}\n",
    "nb_paramgrid = list(ParameterGrid(nb_params))\n",
    "\n",
    "# svm\n",
    "svm_params = { \"C\": [0.1, 0.5, 1, 5],\n",
    "    \"kernel\": [\"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"gamma\": [\"scale\", \"auto\"],\n",
    "    \"class_weight\": [\"balanced\", None] \n",
    "}\n",
    "\n",
    "svm_paramgrid = list(ParameterGrid(svm_params))\n",
    "\n",
    "# rf\n",
    "rf_params = [\n",
    "    {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"class_weight\": [\"balanced\", \"balanced_subsample\", None],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        \"min_samples_leaf\": [1, 2, 4]\n",
    "    }\n",
    "]\n",
    "\n",
    "rf_paramgrid = list(ParameterGrid(rf_params))\n",
    "\n",
    "# dummy classifier\n",
    "dummy_params = { \"strategy\": [\"prior\"] }\n",
    "dummy_paramgrid = list(ParameterGrid(dummy_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversampling the data\n",
    "def oversample_ros(X,y):\n",
    "    ros = RandomOverSampler(random_state = 4103)\n",
    "    X, y = ros.fit_resample(X, y)\n",
    "    return X,y\n",
    "\n",
    "def oversample_smote(X,y):\n",
    "    smote = SMOTE(random_state = 4103)\n",
    "    X, y = smote.fit_resample(X, y)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_helper(vect, sentence_version):\n",
    "    vec_train = vect.fit_transform(train[sentence_version])\n",
    "    vec_val = vect.transform(val[sentence_version])\n",
    "    vec_test = vect.transform(test[sentence_version])\n",
    "    vec_trainval = vect.transform(trainval[sentence_version])\n",
    "    \n",
    "    vec_train_oversampled = oversample_smote(vec_train, labels[0])\n",
    "    vec_trainval_oversampled = oversample_smote(vec_trainval, labels[3])\n",
    "    return vec_train_oversampled, vec_val, vec_test, vec_trainval_oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_dataset(vect_param):\n",
    "    # bag of words\n",
    "    \n",
    "    # raw\n",
    "    bow = CountVectorizer(**vect_param)\n",
    "    bow_train_oversampled, bow_val, bow_test, bow_trainval_oversampled = vectorize_helper(bow, 'sentence')\n",
    "\n",
    "#     # cleaned\n",
    "#     bow = CountVectorizer(**vect_param)\n",
    "#     bow_train_clean_oversampled, bow_clean_val, bow_clean_test, bow_trainval_clean_oversampled = vectorize_helper(bow, 'cleaned_sentence')\n",
    "    \n",
    "    # tfidf\n",
    "    \n",
    "#     # raw\n",
    "#     tfidf = TfidfVectorizer(**vect_param)\n",
    "#     tfidf_train_oversampled, tfidf_val, tfidf_test, tfidf_trainval_oversampled = vectorize_helper(tfidf, 'sentence')\n",
    "    \n",
    "    # cleaned\n",
    "    tfidf = TfidfVectorizer(**vect_param)    \n",
    "    tfidf_train_clean_oversampled, tfidf_clean_val, tfidf_clean_test, tfidf_trainval_clean_oversampled = vectorize_helper(tfidf, 'cleaned_sentence')\n",
    "\n",
    "    return {'bow_raw': [bow_train_oversampled, bow_val, bow_test, bow_trainval_oversampled], #{'clean': [bow_train_clean_oversampled, bow_clean_val, bow_clean_test, bow_trainval_clean_oversampled],\n",
    "                    #'raw': [bow_train_oversampled, bow_val, bow_test, bow_trainval_oversampled]},\n",
    "            'tfidf_clean': [tfidf_train_clean_oversampled, tfidf_clean_val, tfidf_clean_test, tfidf_trainval_clean_oversampled] #{'clean': [tfidf_train_clean_oversampled, tfidf_clean_val, tfidf_clean_test, tfidf_trainval_clean_oversampled],\n",
    "                     #'raw':[tfidf_train_oversampled, tfidf_val, tfidf_test, tfidf_trainval_oversampled]}\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparam_search(model, model_function, model_grid):\n",
    "    ind = 0 \n",
    "    gridsearch_results = []\n",
    "    for vect_param in tqdm(vect_paramgrid):\n",
    "        dataset = vectorize_dataset(vect_param)\n",
    "        for vect_choice in vect_methods_paramgrid:\n",
    "            vect_type  = vect_choice['type']\n",
    "#             sentence_proc = vect_choice['processing']\n",
    "            train_hp = dataset[vect_type][0][0]\n",
    "            val_hp = dataset[vect_type][1]\n",
    "            test_hp = dataset[vect_type][2]\n",
    "            trainval_hp = dataset[vect_type][3][0]\n",
    "\n",
    "            train_label = dataset[vect_type][0][1]\n",
    "            val_label = labels[1]\n",
    "            test_label = labels[2]\n",
    "            trainval_label = dataset[vect_type][3][1]\n",
    "\n",
    "            for model_param in model_grid:\n",
    "                # fit model on train set\n",
    "                model = model_function(**model_param)\n",
    "                model.fit(train_hp, train_label)\n",
    "                val_pred = model.predict(val_hp)\n",
    "\n",
    "                # scoring\n",
    "                val_metrics = classification_report(val_label, val_pred, output_dict=True)\n",
    "                val_accuracy = val_metrics[\"accuracy\"]\n",
    "                val_f1_weighted = val_metrics[\"weighted avg\"][\"f1-score\"]\n",
    "                val_f1_zero = val_metrics[\"0\"][\"f1-score\"]\n",
    "                val_f1_one = val_metrics[\"1\"][\"f1-score\"]\n",
    "\n",
    "                # fit model on trainval set\n",
    "                model = model_function(**model_param)\n",
    "                model.fit(trainval_hp, trainval_label)\n",
    "                test_pred = model.predict(test_hp)\n",
    "\n",
    "                # scoring\n",
    "                test_metrics = classification_report(test_label, test_pred, output_dict=True)\n",
    "                test_accuracy = test_metrics[\"accuracy\"]\n",
    "                test_f1_weighted = test_metrics[\"weighted avg\"][\"f1-score\"]\n",
    "                test_f1_zero = test_metrics[\"0\"][\"f1-score\"]\n",
    "                test_f1_one = test_metrics[\"1\"][\"f1-score\"]\n",
    "\n",
    "                results = {\"model\": model}\n",
    "                results.update(vect_param)\n",
    "                results.update(vect_choice)\n",
    "                results.update(model_param)\n",
    "                results.update({\"val_f1_weighted\": val_f1_weighted,  \n",
    "                                \"val_f1_zero\": val_f1_zero,\n",
    "                                \"val_f1_one\": val_f1_one,\n",
    "                                \"val_accuracy\": val_accuracy})\n",
    "                results.update({\"test_f1_weighted\": test_f1_weighted, \n",
    "                                \"test_f1_zero\": test_f1_zero,\n",
    "                                \"test_f1_one\": test_f1_one,\n",
    "                                \"test_accuracy\": test_accuracy})\n",
    "                gridsearch_results.append(results)\n",
    "                ind += 1\n",
    "    final_results = pd.DataFrame.from_records(gridsearch_results)\n",
    "    final_results = final_results.sort_values(by=[\"val_f1_one\", \"test_f1_one\"], ascending=False)\n",
    "#    final_results = final_results.sort_values(by=[\"val_f1_weighted\", \"test_f1_weighted\"], ascending=False)\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_lr_results = hyperparam_search(\"log_reg\", LogisticRegression, logreg_paramgrid)\n",
    "final_lr_results.to_csv(\"model_results/bowtfidf/logreg_smote.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_nb_results = hyperparam_search(\"nb\", MultinomialNB, nb_paramgrid)\n",
    "final_nb_results.to_csv(\"model_results/bowtfidf/nb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_svm_results = hyperparam_search(\"svm\", SVC, svm_paramgrid)\n",
    "final_svm_results.to_csv(\"model_results/bowtfidf/svm_smote.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rf_results = hyperparam_search(\"rf\", RandomForestClassifier, rf_paramgrid)\n",
    "final_rf_results.to_csv(\"model_results/bowtfidf/rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dummy_results = hyperparam_search(\"dummy\", DummyClassifier, dummy_paramgrid)\n",
    "final_dummy_results.to_csv(\"model_results/bowtfidf/dummy_ros.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmI8quui7bAh"
   },
   "source": [
    "# Generate Fold Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_k_fold(model_grid, vectorizer_grid, column, data, model_name, vectFunc, model_fn):\n",
    "    \n",
    "    # Generate fold predictions\n",
    "    fold_num = 1\n",
    "    for tf_combi in data:\n",
    "        train = tf_combi[0]\n",
    "        predict_on = tf_combi[1]\n",
    "        predict_label = predict_on.relevance\n",
    "        \n",
    "        # Vectorise Data\n",
    "        for v in vectorizer_grid:\n",
    "            vectorizer = vectFunc(**v)\n",
    "        vec_train = vectorizer.fit_transform(train[column])\n",
    "        vec_predict_on = vectorizer.transform(predict_on[column])\n",
    "        \n",
    "        # Get Labels\n",
    "        train_label = train.relevance\n",
    "        \n",
    "        # Oversample\n",
    "        vec_train_over, train_label_over = oversample_smote(vec_train, train_label)\n",
    "        \n",
    "        # Fit Model\n",
    "        for m in model_grid:\n",
    "            model = model_fn(**m)\n",
    "        model.fit(vec_train_over, train_label_over)\n",
    "        predictions = model.predict_proba(vec_predict_on)\n",
    "        \n",
    "        # Create Dataframe and output\n",
    "        df = pd.DataFrame(data=predictions, columns = [model_name+'_prob_0', model_name+'_prob_1'])\n",
    "        if model_name == 'SVM':\n",
    "            df['relevance'] = predict_label\n",
    "            \n",
    "        if fold_num <=5:\n",
    "            path = DATA_FOLDER + \"fold_predictions/\" + model_name + \"/\" + model_name + '_fold' + str(fold_num) +'.csv'\n",
    "        else:\n",
    "            path = DATA_FOLDER +\"fold_predictions/\" + model_name + \"/\" + model_name + '_test.csv'\n",
    "        \n",
    "        df.to_csv(path, index=False)\n",
    "        \n",
    "        fold_num +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, random_state=4013, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'folds/train_folds_1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-728ccf0b110f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'folds/train_folds_{ind}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'folds/val_folds_{ind}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors)\u001b[0m\n\u001b[1;32m   3168\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3169\u001b[0m         )\n\u001b[0;32m-> 3170\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             f, handles = get_handle(\n\u001b[0m\u001b[1;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'folds/train_folds_1.csv'"
     ]
    }
   ],
   "source": [
    "ind=1\n",
    "for train_index, test_index in kf.split(trainval.reset_index()):\n",
    "    tr = trainval.reset_index().iloc[train_index]\n",
    "    val = trainval.reset_index().iloc[test_index]\n",
    "    tr.to_csv('folds/train_folds_{ind}.csv'.format(ind=ind), index=False)\n",
    "    val.to_csv('folds/val_folds_{ind}.csv'.format(ind=ind), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "DATA_FOLDER = \"../data/\"\n",
    "train1 = pd.read_csv(DATA_FOLDER + 'folds/train_folds_1.csv')\n",
    "train2 = pd.read_csv(DATA_FOLDER + 'folds/train_folds_2.csv')\n",
    "train3 = pd.read_csv(DATA_FOLDER +'folds/train_folds_3.csv')\n",
    "train4 = pd.read_csv(DATA_FOLDER +'folds/train_folds_4.csv')\n",
    "train5 = pd.read_csv(DATA_FOLDER +'folds/train_folds_5.csv')\n",
    "\n",
    "fold1 = pd.read_csv(DATA_FOLDER +'folds/val_folds_1.csv')\n",
    "fold2 = pd.read_csv(DATA_FOLDER +'folds/val_folds_2.csv')\n",
    "fold3 = pd.read_csv(DATA_FOLDER +'folds/val_folds_3.csv')\n",
    "fold4 = pd.read_csv(DATA_FOLDER +'folds/val_folds_4.csv')\n",
    "fold5 = pd.read_csv(DATA_FOLDER +'folds/val_folds_5.csv')\n",
    "\n",
    "train_all = pd.read_csv(DATA_FOLDER +'folds/trainval.csv')\n",
    "testset = pd.read_csv(DATA_FOLDER +'folds/test.csv')\n",
    "\n",
    "# store in suitable data structure\n",
    "data = [(train1, fold1), (train2, fold2),(train3, fold3), (train4, fold4), (train5, fold5), (train_all, testset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model grid that gives highest validtion weighted F1 (Class 1)\n",
    "rf_params = [\n",
    "    {\n",
    "        \"criterion\": [\"entropy\"],\n",
    "        \"min_samples_split\": [5],\n",
    "        \"class_weight\": ['balanced'],\n",
    "        \"max_features\": [\"log2\"],\n",
    "        \"min_samples_leaf\": [2]\n",
    "    }\n",
    "]\n",
    "\n",
    "rf_paramgrid = list(ParameterGrid(rf_params))\n",
    "\n",
    "# Instantiate Vectorizer grid with Params giving highest validation weighted F1 (Class 1)\n",
    "rf_vect_params = {\n",
    "    \"analyzer\": [\"word\"],\n",
    "    \"lowercase\": [True],\n",
    "    \"ngram_range\": [(1,2)],\n",
    "    \"max_df\": [0.25],\n",
    "    \"min_df\": [10]\n",
    "}\n",
    "rf_vect_paramgrid = list(ParameterGrid(rf_vect_params))\n",
    "\n",
    "# Best text processing\n",
    "column = 'cleaned_sentence'\n",
    "\n",
    "# Model Function\n",
    "model_fn = RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_k_fold(rf_paramgrid, rf_vect_paramgrid, column, data, \"RF\", TfidfVectorizer, model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(**rf_vect_paramgrid[0])\n",
    "tfidf_train = tfidf.fit_transform(df1.cleaned_sentence)\n",
    "\n",
    "final_model = RandomForestClassifier(**rf_paramgrid[0])\n",
    "\n",
    "# oversample\n",
    "tfidf_train_over, tfidf_label_over = oversample_smote(tfidf_train, df1.relevance)\n",
    "\n",
    "final_model.fit(tfidf_train_over, tfidf_label_over)\n",
    "\n",
    "vect_pkl_filename = DATA_FOLDER + \"saved_models/model_RF_vectorizer.pkl\"\n",
    "model_pkl_filename = DATA_FOLDER + \"saved_models/model_RF.pkl\"\n",
    "with open(model_pkl_filename, 'wb') as file:\n",
    "    pickle.dump(final_model, file)\n",
    "with open(vect_pkl_filename, 'wb') as file:\n",
    "    pickle.dump(tfidf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model grid that gives highest validtion weighted F1 (Class 1)\n",
    "svm_params = {\n",
    "    \"C\": [0.5],\n",
    "    \"kernel\": [\"sigmoid\"],\n",
    "    \"gamma\": [\"scale\"],\n",
    "    \"class_weight\": ['balanced'],\n",
    "    \"probability\": [True]\n",
    "}\n",
    "\n",
    "svm_paramgrid = list(ParameterGrid(svm_params))\n",
    "\n",
    "# Instantiate Vectorizer grid with Params giving highest validation weighted F1 (Class 1)\n",
    "svm_vect_params = {\n",
    "    \"analyzer\": [\"word\"],\n",
    "    \"lowercase\": [True],\n",
    "    \"ngram_range\": [(1,2)],\n",
    "    \"max_df\": [0.25],\n",
    "    \"min_df\": [1]\n",
    "}\n",
    "svm_vect_paramgrid = list(ParameterGrid(svm_vect_params))\n",
    "\n",
    "#Best text processing\n",
    "column = 'cleaned_sentence'\n",
    "\n",
    "# Model Function\n",
    "model_fn = SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_k_fold(svm_paramgrid, svm_vect_paramgrid, column, data, \"SVM\", TfidfVectorizer, model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(**svm_vect_paramgrid[0])\n",
    "tfidf_train = tfidf.fit_transform(df1.cleaned_sentence)\n",
    "\n",
    "final_model = SVC(**svm_paramgrid[0])\n",
    "\n",
    "# oversample\n",
    "tfidf_train_over, tfidf_label_over = oversample_smote(tfidf_train, df1.relevance)\n",
    "\n",
    "final_model.fit(tfidf_train_over, tfidf_label_over)\n",
    "\n",
    "vect_pkl_filename = DATA_FOLDER + \"saved_models/model_SVM_vectorizer.pkl\"\n",
    "model_pkl_filename = DATA_FOLDER + \"saved_models/model_SVM.pkl\"\n",
    "with open(model_pkl_filename, 'wb') as file:\n",
    "    pickle.dump(final_model, file)\n",
    "with open(vect_pkl_filename, 'wb') as file:\n",
    "    pickle.dump(tfidf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model grid that gives highest validtion weighted F1 (Class 1)\n",
    "nb_params = {\n",
    "    \"alpha\": [0.5]\n",
    "}\n",
    "\n",
    "nb_paramgrid = list(ParameterGrid(nb_params))\n",
    "\n",
    "# Instantiate Vectorizer grid with Params giving highest validation weighted F1 (Class 1)\n",
    "nb_vect_params = {\n",
    "    \"analyzer\": [\"word\"],\n",
    "    \"lowercase\": [True],\n",
    "    \"ngram_range\": [(1,1)],\n",
    "    \"max_df\": [0.25],\n",
    "    \"min_df\": [1]\n",
    "}\n",
    "nb_vect_paramgrid = list(ParameterGrid(nb_vect_params))\n",
    "\n",
    "# Best text processing\n",
    "column = 'sentence'\n",
    "\n",
    "# Model Function\n",
    "model_fn = MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_k_fold(nb_paramgrid, nb_vect_paramgrid, column, data, \"NB\", CountVectorizer, model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(**nb_vect_paramgrid[0])\n",
    "bow_train = bow.fit_transform(df1.cleaned_sentence)\n",
    "\n",
    "final_model = MultinomialNB(**nb_paramgrid[0])\n",
    "\n",
    "# oversample\n",
    "bow_train_over, bow_label_over = oversample_smote(bow_train, df1.relevance)\n",
    "\n",
    "final_model.fit(bow_train_over, bow_label_over)\n",
    "\n",
    "\n",
    "vect_pkl_filename = DATA_FOLDER + \"saved_models/model_NB_vectorizer.pkl\"\n",
    "model_pkl_filename = DATA_FOLDER + \"saved_models/model_NB.pkl\"\n",
    "with open(model_pkl_filename, 'wb') as file:\n",
    "    pickle.dump(final_model, file)\n",
    "with open(vect_pkl_filename, 'wb') as file:\n",
    "    pickle.dump(bow, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model grid that gives highest validtion weighted F1 (Class 1)\n",
    "lr_params = {\n",
    "    \"C\": [1],\n",
    "    'class_weight': ['balanced'],\n",
    "    'penalty':['l2'],\n",
    "    'solver':['lbfgs']\n",
    "}\n",
    "\n",
    "lr_paramgrid = list(ParameterGrid(lr_params))\n",
    "\n",
    "# Instantiate Vectorizer grid with Params giving highest validation weighted F1 (Class 1)\n",
    "lr_vect_params = {\n",
    "    \"analyzer\": [\"word\"],\n",
    "    \"lowercase\": [True],\n",
    "    \"ngram_range\": [(1,1)],\n",
    "    \"max_df\": [0.25],\n",
    "    \"min_df\": [1]\n",
    "}\n",
    "lr_vect_paramgrid = list(ParameterGrid(lr_vect_params))\n",
    "\n",
    "# Best text processing\n",
    "column = 'cleaned_sentence'\n",
    "\n",
    "# Model Function\n",
    "model_fn = LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_k_fold(lr_paramgrid, lr_vect_paramgrid, column, data, \"LR\", TfidfVectorizer, model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(**lr_vect_paramgrid[0])\n",
    "tfidf_train = tfidf.fit_transform(df1.cleaned_sentence)\n",
    "\n",
    "final_model = LogisticRegression(**lr_paramgrid[0])\n",
    "\n",
    "# oversample\n",
    "tfidf_train_over, tfidf_label_over = oversample_smote(tfidf_train, df1.relevance)\n",
    "\n",
    "final_model.fit(tfidf_train_over, tfidf_label_over)\n",
    "\n",
    "vect_pkl_filename = DATA_FOLDER + \"saved_models/model_LR_vectorizer.pkl\"\n",
    "model_pkl_filename = DATA_FOLDER + \"saved_models/model_LR.pkl\"\n",
    "with open(model_pkl_filename, 'wb') as file:\n",
    "    pickle.dump(final_model, file)\n",
    "with open(vect_pkl_filename, 'wb') as file:\n",
    "    pickle.dump(tfidf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
