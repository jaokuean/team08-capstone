{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyPDF2\n",
    "# !pip install gensim\n",
    "# !pip install nltk\n",
    "# !pip install WordCloud\n",
    "# !pip install pdfminer3\n",
    "# !pip install mlflow\n",
    "# !pip3 install spacy\n",
    "# !pip3 install pyLDAvis==2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# pd.options.display.max_columns = 50\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 50)\n",
    "\n",
    "plt.rcParams.update({'figure.figsize':(15,6), 'figure.dpi':60})\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys  \n",
    "# import os\n",
    "# sys.path.append('src') \n",
    "# from edge import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "\n",
    "# PDF text extraction\n",
    "from pdfminer3.layout import LAParams, LTTextBox\n",
    "from pdfminer3.pdfpage import PDFPage\n",
    "from pdfminer3.pdfinterp import PDFResourceManager\n",
    "from pdfminer3.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer3.converter import PDFPageAggregator\n",
    "from pdfminer3.converter import TextConverter\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "# Others\n",
    "import requests\n",
    "import io\n",
    "import string\n",
    "import re\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "import io\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/xinminaw/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xinminaw/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import spacy\n",
    "#spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Details\n",
    "\n",
    "Enter required information about the report.  Report must be in PDF format and downloadable from a URL.\n",
    "\n",
    "1. Company Name\n",
    "2. Company Ticker Symbol\n",
    "3. Year of the Report\n",
    "4. URL of the Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/xlrd/xlsx.py:266: DeprecationWarning: This method will be removed in future versions.  Use 'tree.iter()' or 'list(tree.iter())' instead.\n",
      "  for elem in self.tree.iter() if Element_has_iter else self.tree.getiterator():\n",
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/xlrd/xlsx.py:266: DeprecationWarning: This method will be removed in future versions.  Use 'tree.iter()' or 'list(tree.iter())' instead.\n",
      "  for elem in self.tree.iter() if Element_has_iter else self.tree.getiterator():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Market</th>\n",
       "      <th>Link</th>\n",
       "      <th>Year</th>\n",
       "      <th>Remarks</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BlackRock</td>\n",
       "      <td>Regional</td>\n",
       "      <td>https://www.blackrock.com/corporate/literature...</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Regional</td>\n",
       "      <td>https://www.blackrock.com/corporate/literature...</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Regional</td>\n",
       "      <td>https://www.blackrock.com/corporate/literature...</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Regional</td>\n",
       "      <td>https://www.blackrock.com/corporate/literature...</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Regional</td>\n",
       "      <td>https://www.blackrock.com/corporate/literature...</td>\n",
       "      <td>2018-2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Company    Market                                               Link       Year Remarks Unnamed: 5\n",
       "0  BlackRock  Regional  https://www.blackrock.com/corporate/literature...       2020     NaN        NaN\n",
       "1        NaN  Regional  https://www.blackrock.com/corporate/literature...       2020     NaN        NaN\n",
       "2        NaN  Regional  https://www.blackrock.com/corporate/literature...       2019     NaN        NaN\n",
       "3        NaN  Regional  https://www.blackrock.com/corporate/literature...       2018     NaN        NaN\n",
       "4        NaN  Regional  https://www.blackrock.com/corporate/literature...  2018-2019     NaN        NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_FOLDER = \"../data/\"\n",
    "report_url_excel = pd.ExcelFile(DATA_FOLDER + \"ESG Data Collection.xlsx\")\n",
    "asianbanks_df = report_url_excel.parse('Asian Banks (Group 18)')\n",
    "assetmanagers_df = report_url_excel.parse('Asset managers (Group 7)')\n",
    "insurance_df = report_url_excel.parse('Insurance (Group 20)')\n",
    "pensionfunds_df = report_url_excel.parse('Pension funds (Group 8)')\n",
    "assetmanagers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asianbanks_df.shape[0] + assetmanagers_df.shape[0] + insurance_df.shape[0] + pensionfunds_df.shape[0]\n",
    "76/400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Market</th>\n",
       "      <th>Link</th>\n",
       "      <th>Year</th>\n",
       "      <th>Remarks</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BlackRock</td>\n",
       "      <td>Regional</td>\n",
       "      <td>https://www.blackrock.com/corporate/literature...</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BlackRock</td>\n",
       "      <td>Regional</td>\n",
       "      <td>https://www.blackrock.com/corporate/literature...</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BlackRock</td>\n",
       "      <td>Regional</td>\n",
       "      <td>https://www.blackrock.com/corporate/literature...</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BlackRock</td>\n",
       "      <td>Regional</td>\n",
       "      <td>https://www.blackrock.com/corporate/literature...</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BlackRock</td>\n",
       "      <td>Regional</td>\n",
       "      <td>https://www.blackrock.com/corporate/literature...</td>\n",
       "      <td>2018-2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Company    Market                                               Link       Year Remarks Unnamed: 5\n",
       "0  BlackRock  Regional  https://www.blackrock.com/corporate/literature...       2020     NaN        NaN\n",
       "1  BlackRock  Regional  https://www.blackrock.com/corporate/literature...       2020     NaN        NaN\n",
       "2  BlackRock  Regional  https://www.blackrock.com/corporate/literature...       2019     NaN        NaN\n",
       "3  BlackRock  Regional  https://www.blackrock.com/corporate/literature...       2018     NaN        NaN\n",
       "4  BlackRock  Regional  https://www.blackrock.com/corporate/literature...  2018-2019     NaN        NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assetmanagers_df.Company = assetmanagers_df.Company.fillna(method='ffill')\n",
    "pensionfunds_df.Fund = pensionfunds_df.Fund.fillna(method='ffill')\n",
    "asianbanks_df.Company = asianbanks_df.Company.fillna(method='ffill')\n",
    "insurance_df.Company = insurance_df.Company.fillna(method='ffill')\n",
    "assetmanagers_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Extraction\n",
    "Extract information from the PDF report.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf(file, verbose=False):\n",
    "    \"\"\"\n",
    "    Process raw PDF text to structured and processed PDF text to be worked on in Python.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : textfile\n",
    "        Textfile that contains raw PDF text.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    text : str\n",
    "        processed PDF text if no error is throw\n",
    "\n",
    "    \"\"\"  \n",
    "    \n",
    "    if verbose:\n",
    "        print('Processing {}'.format(file))\n",
    "\n",
    "    try:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        codec = 'utf-8'\n",
    "        laparams = LAParams()\n",
    "\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, codec=codec, laparams=laparams)\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "        \n",
    "        password = \"\"\n",
    "        maxpages = 0\n",
    "        caching = True\n",
    "        pagenos = set()\n",
    "\n",
    "        content = []\n",
    "\n",
    "        for page in PDFPage.get_pages(file,\n",
    "                                      pagenos, \n",
    "                                      maxpages=maxpages,\n",
    "                                      password=password,\n",
    "                                      caching=True,\n",
    "                                      check_extractable=False):\n",
    "\n",
    "            page_interpreter.process_page(page)\n",
    "\n",
    "            content.append(fake_file_handle.getvalue())\n",
    "\n",
    "            fake_file_handle.truncate(0)\n",
    "            fake_file_handle.seek(0)        \n",
    "\n",
    "        text = '##PAGE_BREAK##'.join(content)\n",
    "\n",
    "        # close open handles\n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # close open handles\n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def extract_content(url):\n",
    "    \"\"\"\n",
    "    Downloads PDF text content from a given URL and parse PDF to obtain processed text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        String that contains url to desired PDF\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    text : str\n",
    "        processed PDF text if no error is throw\n",
    "\n",
    "    \"\"\"   \n",
    "    headers={\"User-Agent\":\"Mozilla/5.0\"}\n",
    "\n",
    "    try:\n",
    "        # retrieve PDF binary stream\n",
    "        r = requests.get(url, allow_redirects=True, headers=headers)\n",
    "        \n",
    "        # access pdf content\n",
    "        text = extract_pdf(io.BytesIO(r.content))\n",
    "\n",
    "        # return concatenated content\n",
    "        return text\n",
    "\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "- must be online PDF, cannot have links that require PDF downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# pension funds\n",
    "for index,row in assetmanagers_df.iterrows():\n",
    "    report_company = row[\"Company\"]\n",
    "    report_year = str(row[\"Year\"])\n",
    "    report_url = row[\"Link\"]\n",
    "    report_content = extract_content(report_url)\n",
    "    report = {'company':report_company, 'year':report_year,'url':report_url, 'content':report_content}\n",
    "    with open(DATA_FOLDER + \"sustainability_reports/pension_funds/\" + report_company+report_year+'.json', \"w\") as outfile:  \n",
    "        json.dump(report, outfile)\n",
    "        \n",
    "# asset managers\n",
    "for index,row in assetmanagers_df.iterrows():\n",
    "    report_company = row[\"Company\"]\n",
    "    report_year = str(row[\"Year\"])\n",
    "    report_url = row[\"Link\"]\n",
    "    report_content = extract_content(report_url)\n",
    "    report = {'company':report_company, 'year':report_year,'url':report_url, 'content':report_content}\n",
    "    with open(DATA_FOLDER + \"sustainability_reports/asset_managers/\" + report_company+report_year+'.json', \"w\") as outfile:  \n",
    "        json.dump(report, outfile)\n",
    "\n",
    "# insurance       \n",
    "for index,row in insurance_df.loc[76:,].iterrows():\n",
    "    report_company = row[\"Company\"]\n",
    "    report_year = \"\" if  str(row[\"Year\"]) == np.nan else str(row[\"Year\"])\n",
    "    report_url = row[\"PDF Link\"]\n",
    "    report_content = extract_content(report_url)\n",
    "    report = {'company':report_company, 'year':report_year,'url':report_url, 'content':report_content}\n",
    "    with open(DATA_FOLDER + \"sustainability_reports/insurance/\" + report_company+report_year+'.json', \"w\") as outfile:  \n",
    "        json.dump(report, outfile)\n",
    "\n",
    "# asian banks\n",
    "for index,row in asianbanks_df.iterrows():\n",
    "    report_company = row[\"Company\"]\n",
    "    report_year = \"\" if  str(row[\"Year\"]) == np.nan else str(row[\"Year\"])\n",
    "    report_url = row[\"Link\"]\n",
    "    report_content = extract_content(report_url)\n",
    "    report = {'company':report_company, 'year':report_year,'url':report_url, 'content':report_content}\n",
    "    with open(DATA_FOLDER + \"sustainability_reports/asian_banks/\" + report_company+report_year+'.json', \"w\") as outfile:  \n",
    "        json.dump(report, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "### Extracting content by pages and sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_lines(line_input):\n",
    "    \"\"\"\n",
    "    Helper Function to preprocess and clean sentences from raw PDF text \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    line_input : str\n",
    "        String that contains a sentence to be cleaned\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    line : str\n",
    "        Cleaned sentence\n",
    "\n",
    "    \"\"\"      \n",
    "    # removing header number\n",
    "    line = re.sub(r'^\\s?\\d+(.*)$', r'\\1', line_input)\n",
    "    # removing trailing spaces\n",
    "    line = line.strip()\n",
    "    # words may be split between lines, ensure we link them back together\n",
    "    line = re.sub(r'\\s?-\\s?', '-', line)\n",
    "    # remove space prior to punctuation\n",
    "    line = re.sub(r'\\s?([,:;\\.])', r'\\1', line)\n",
    "    # ESG contains a lot of figures that are not relevant to grammatical structure\n",
    "    line = re.sub(r'\\d{5,}', r' ', line)\n",
    "    # remove emails\n",
    "    line = re.sub(r'\\S*@\\S*\\s?', '', line)\n",
    "    # remove mentions of URLs\n",
    "    line = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', r' ', line)\n",
    "    # remove multiple spaces\n",
    "    line = re.sub(r'\\s+', ' ', line)\n",
    "    # join next line with space\n",
    "    line = re.sub(r' \\n', ' ', line)\n",
    "    line = re.sub(r'.\\n', '. ', line)\n",
    "    line = re.sub(r'\\x0c', ' ', line)\n",
    "    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def remove_non_ascii(text):\n",
    "    \"\"\"\n",
    "    Helper Function to remove non ascii characters from text\n",
    "    \"\"\"\n",
    "    printable = set(string.printable)\n",
    "    return ''.join(filter(lambda x: x in printable, text))\n",
    "\n",
    "def not_header(line):\n",
    "    \"\"\"\n",
    "    Helper Function to remove headers\n",
    "    \"\"\"\n",
    "    return not line.isupper()\n",
    "\n",
    "def extract_pages_sentences(nlp, text):    \n",
    "    \"\"\"\n",
    "    Extracting text from raw PDF text and store them by pages and senteces. Raw text is also cleand by removing junk, URLs, etc.\n",
    "    Consecutive lines are also grouped into paragraphs and spacy is used to parse sentences.\n",
    "    Parameters\n",
    "    ----------\n",
    "    nlp: spacy nlp model\n",
    "        NLP model to parse sentences\n",
    "    text : str\n",
    "        Raw PDF text\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    pages_content : list of str\n",
    "        A list containing text from each page of the PDF report. Page number is the index of list + 1\n",
    "    \n",
    "    pages_sentences : list of list\n",
    "        A list containing lists. Page number is the index of outer list + 1. Inner list contains sentences from each page\n",
    " \n",
    "    \"\"\"  \n",
    "    MIN_WORDS_PER_PAGE = 500\n",
    "    \n",
    "    pages = text.split('##PAGE_BREAK##')\n",
    "    #print('Number of Pages: {}'.format(len(pages)))\n",
    "\n",
    "    lines = []\n",
    "    for i in range(len(pages)):\n",
    "        page_number = i + 1\n",
    "        page = pages[i]\n",
    "        \n",
    "        # remove non ASCII characters\n",
    "        text = remove_non_ascii(page)\n",
    "        \n",
    "        # if len(text.split(' ')) < MIN_WORDS_PER_PAGE:\n",
    "        #     print(f'Skipped Page: {page_number}')\n",
    "        #     continue\n",
    "        \n",
    "        prev = \"\"\n",
    "        for line in text.split('\\n\\n'):\n",
    "            # aggregate consecutive lines where text may be broken down\n",
    "            # only if next line starts with a space or previous does not end with dot.\n",
    "            if(line.startswith(' ') or not prev.endswith('.')):\n",
    "                prev = prev + ' ' + line\n",
    "            else:\n",
    "                # new paragraph\n",
    "                lines.append(prev)\n",
    "                prev = line\n",
    "\n",
    "        # don't forget left-over paragraph\n",
    "        lines.append(prev)\n",
    "        lines.append('##SAME_PAGE##')\n",
    "        \n",
    "    lines = '  '.join(lines).split('##SAME_PAGE##')\n",
    "    \n",
    "    # clean paragraphs from extra space, unwanted characters, urls, etc.\n",
    "    # best effort clean up, consider a more versatile cleaner\n",
    "    \n",
    "    pages_content = []\n",
    "    pages_sentences = []\n",
    "\n",
    "    for line in lines[:-1]: # looping through each page\n",
    "        \n",
    "        line = preprocess_lines(line)       \n",
    "        pages_content.append(str(line).strip())\n",
    "\n",
    "        sentences = []\n",
    "        # split paragraphs into well defined sentences using spacy\n",
    "        for part in list(nlp(line).sents):\n",
    "            sentences.append(str(part).strip())\n",
    "\n",
    "        #sentences += nltk.sent_tokenize(line)\n",
    "            \n",
    "        # Only interested in full sentences and sentences with 10 to 100 words. --> filter out first page/content page\n",
    "        sentences = [s for s in sentences if re.match('^[A-Z][^?!.]*[?.!]$', s) is not None]\n",
    "        sentences = [s.replace('\\n', ' ') for s in sentences]\n",
    "        \n",
    "        pages_sentences.append(sentences)\n",
    "        \n",
    "    return pages_content, pages_sentences #list, list of list where page is index of outer list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing \n",
    "1. Lowercase\n",
    "2. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(report):\n",
    "    \"\"\"\n",
    "    Lemmatize,lowercase and remove stopwords for pages of a report\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    report: list of str\n",
    "        A list containing text from each page of the PDF report. Page number is the index of list + 1\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    report_pages : list of str\n",
    "        A list containing processed text from each page of the PDF report. Page number is the index of list + 1\n",
    "    \n",
    "    \"\"\"  \n",
    "    \n",
    "    report_pages = []\n",
    "\n",
    "    def para_to_sent(para):\n",
    "        \"\"\"\n",
    "        Helper function to split paragraphs into well defined sentences using spacy\n",
    "        \"\"\"\n",
    "        sentences = []\n",
    "        for part in list(nlp(para).sents):\n",
    "            sentences.append(str(part).strip())\n",
    "        return sentences\n",
    "\n",
    "    def remove_stopwords(texts):\n",
    "        \"\"\"\n",
    "        Helper function to remove stopwords from sentence\n",
    "        \"\"\"\n",
    "        return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "    def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "        \"\"\"\n",
    "        Helper function to lemmatize text in sentence\n",
    "        \"\"\"\n",
    "        texts_out = []\n",
    "        doc = nlp(texts) \n",
    "        texts_out.append(\" \".join([token.lemma_ for token in doc]))\n",
    "        return texts_out\n",
    "    \n",
    "    for page in report:\n",
    "\n",
    "        sentences = para_to_sent(page.lower())\n",
    "\n",
    "        # Do lemmatization keeping only noun, adj, vb, adv\n",
    "        page_data = []\n",
    "        for sentence in sentences : \n",
    "            data_lemmatized = lemmatization(sentence, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "            page_data.extend(data_lemmatized)\n",
    "        page_para_lemma = \"\".join(page_data)\n",
    "        \n",
    "        report_pages.append(page_para_lemma)\n",
    "    \n",
    "    return report_pages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# all_reports_pages_sentences_preprocessed = [] # list of dictionaries, each dictionary is 1 report\n",
    "# for report in all_reports_pages_sentences:\n",
    "#     report_pages_preprocessed = preprocessing(report[\"report_pages\"])\n",
    "#     report_sentences_preprocessed = [preprocessing(page) for page in report[\"report_sentences\"]]\n",
    "#     report[\"report_pages_preprocessed\"] = report_pages_preprocessed \n",
    "#     report[\"report_sentences_preprocessed\"] = report_sentences_preprocessed \n",
    "#     all_reports_pages_sentences_preprocessed.append(report)\n",
    "\n",
    "# # pipeline\n",
    "# #report_preprocessed = preprocessing(report_pages) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Report\n",
    "\n",
    "1. If the page contains at least 3 words from relevant_terms_directFilter_lem, keep the page, else filter it out.\n",
    "2. If the page contains any word from (relevant_terms_combinationC_lem AND relevant_terms_combinationA_lem) OR (relevant_terms_combinationC_lem AND  relevant_terms_combinationB_lem), keep the page, else filter it out.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def lemmatization(text_list, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    # lemmatize text in sentence\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for texts in text_list:\n",
    "        texts = texts.lower()\n",
    "        texts_out.append(\" \".join([token.lemma_ for token in nlp(texts)]))\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "relevant_terms_directFilter = set([\"carbon\",\"co2\",\"environment\",\"GHG emissions\",\"Greenhouse Gas\",\"carbon footprint\",\"carbon emissions\",\"Scope 1\",\"Scope 2\",\n",
    "                               \"Scope 3\", \"WACI\",\"Carbon Intensity\",\"carbon pricing\",\"net-zero\",\"metrics and targets\",\"TCFD\",\n",
    "                                \"sustainability goals\",\"decarbonisation\",\"climate\",'energy', 'emission', 'emissions', 'renewable', 'carbon', 'fuel', 'power', \n",
    "                               'green', 'gas', 'green energy', 'sustainable', 'climate', 'sustainability', 'environmental', 'environment', 'GHG', \n",
    "                               'decarbon', 'energy consumption', 'paper consumption','water consumption', 'carbon intensity', 'waste management', 'electricity consumption', \n",
    "                                'cdp', 'global warming', 'business travel','climate solutions', 'decarbonization', 'cvar', 'climate value-at-risk','waste output'])\n",
    "relevant_terms_combinationA = [\"emissions\",\"exposure\",\"carbon related\",\"esg\",\"sustainable\",\"green\",\"climate sensitive\",\"impact investing\", \"investment framework\", 'msci', 'ftse', 'responsible investing', 'responsible investment','transition']\n",
    "relevant_terms_combinationB = [\"portfolio\",\"assets\",\"AUM\",\"investment\",\"financing\",\"ratings\",\"revenue\",\"bond\",\"goal\",\"insurance\", \"equity\", \"swap\", \"option\", \"portfolio holdings\", \"risk management\",'financial products']\n",
    "relevant_terms_combinationC = [\"net zero\",\"carbon footprint\",\"CO2\",\"carbon\",\"oil\",\"coal\", \"gas\", \"fossil fuel\",\"green\"] # compare [CB or CA]  compare with ABC\n",
    "relevant_terms_combination_directFilter_lem = lemmatization(relevant_terms_directFilter)\n",
    "relevant_terms_combinationA_lem = lemmatization(relevant_terms_combinationA)\n",
    "relevant_terms_combinationB_lem = lemmatization(relevant_terms_combinationB)\n",
    "relevant_terms_combinationC_lem = lemmatization(relevant_terms_combinationC)\n",
    "#relevant_terms_wordEmbeddings = [\"\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def filter_report_highLevel(report):\n",
    "    \"\"\"\n",
    "    Page filter to filter report for only relevant pages with decarbonisation related words.\n",
    "    Two types of word filters: direct and indirect. Direct contains words that are directly related to decarbonisation while indirect contains other relevant decarbonisation information.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    report: list of str\n",
    "        A list containing text from each page of the PDF report. Page number is the index of list + 1\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    filtered_report_direct : dict of {int : str}\n",
    "        A dictionary that contains relevant pages obtained using direct filter. The key is the page number and value is the text on the page. \n",
    "    \n",
    "    filtered_report_indirect : dict of {int : str}\n",
    "        A dictionary that contains relevant pages obtained using indirect filter. The key is the page number and value is the text on the page.     \n",
    "    \"\"\"  \n",
    "    filtered_report_direct = {}\n",
    "    filtered_report_indirect = {}\n",
    "    for i in range(len(report)):\n",
    "        page = report[i]\n",
    "        page_number = i + 1\n",
    "        no_words = len(page.split(\" \"))\n",
    "        if sum(map(page.__contains__, relevant_terms_combination_directFilter_lem)) > 2:\n",
    "            filtered_report_direct[page_number] = page\n",
    "        elif (any(map(page.__contains__, relevant_terms_combinationA_lem)) and any(map(page.__contains__, relevant_terms_combinationC_lem))) or (any(map(page.__contains__, relevant_terms_combinationB_lem)) and any(map(page.__contains__, relevant_terms_combinationC_lem))):\n",
    "            filtered_report_indirect[page_number] = page\n",
    "    return filtered_report_direct,filtered_report_indirect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# filter digits\n",
    "def is_number(string): \n",
    "    \"\"\"\n",
    "    Helper function that checks if a string contains numbers\n",
    "    \"\"\"\n",
    "    test_str = string\n",
    "    # next() checking for each element, reaches end, if no element found as digit\n",
    "    res = True if next((chr for chr in test_str if chr.isdigit()), None) else False\n",
    "    return res\n",
    "\n",
    "def is_date(string):\n",
    "    \"\"\"\n",
    "    Helper function that checks if a string contains dates such as \"september\"/\"Monday at 12:01am\"/1999 etc\n",
    "    \"\"\"\n",
    "    if re.match('.*([1-2][0-9]{3})', string) != None:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def filter_report_numbers(filtered_report): \n",
    "    \"\"\"\n",
    "    Page filter to filter the filtered report for pages with numbers that are not dates as to track decarbonisation progress, we need numbers.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filtered_report: dict of {int : str}\n",
    "        A dictionary that contains relevant pages obtained using page filters. The key is the page number and value is the text on the page.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    filtered_report_numbers : dict of {int : str}\n",
    "        A dictionary that contains relevant pages with numbers. The key is the page number and value is the text on the page. \n",
    "    \n",
    "    \"\"\" \n",
    "        \n",
    "    filtered_report_numbers = {}\n",
    "    for page_number,page in filtered_report.items():\n",
    "        # remove all dates from page first\n",
    "        page_no_date = \" \".join([word for word in page.split(\" \") if is_date(word) == False])\n",
    "        # retain pages with numbers\n",
    "        if is_number(page_no_date):\n",
    "            filtered_report_numbers[page_number] = page           \n",
    "    return filtered_report_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def filter_tables(filtered_report):    \n",
    "    \"\"\"\n",
    "    Page filter to filter the filtered report for pages with tables to aid table detection in downstream tasks. This is done by checking if page contains at least 10 numbers + 1 units\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filtered_report: dict of {int : str}\n",
    "        A dictionary that contains relevant pages with numbers. The key is the page number and value is the text on the page.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    filtered_report_numbers : dict of {int : str}\n",
    "        A dictionary that contains relevant pages with tables. The key is the page number and value is the text on the page. \n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    \n",
    "    units = ['tonnes', 'tons', 'kwh', 'kg', 'kilogram', 'kilowatt hour', 'gigajoules', 'gj', 'litre', 'liter', 'co2e', 'tco2e', 'tco2', 'mwh', 'megawatt hour', 'gwh', 'gigawatt hour', '%', 'cubic metres', 'cm3', 'm3', 'per employee','co2']\n",
    "    filtered_report_numbers = {}\n",
    "    for page_number,page in filtered_report.items():\n",
    "        no_numbers = 0\n",
    "        units_flag = 0\n",
    "        for word in page.split(\" \"):\n",
    "            try: \n",
    "                float(word)\n",
    "                if is_date(word) == False:\n",
    "                    no_numbers += 1\n",
    "            except: \n",
    "                if any(char.isdigit() for char in word):\n",
    "                    no_numbers += 1\n",
    "        for unit in units:\n",
    "            if unit in page:\n",
    "                units_flag += 1     \n",
    "        if (no_numbers >= 10) & (units_flag >= 1):\n",
    "            filtered_report_numbers[page_number] = page\n",
    "    return filtered_report_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing all FIs and saving output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    }
   ],
   "source": [
    "folder_names = [\"asset_managers\",\"pension_funds\",\"insurance\",\"asian_banks\"]\n",
    "empty_json = 0\n",
    "for fi in folder_names: \n",
    "    # reload in data to preprocess\n",
    "    all_reports = [file for file in os.listdir(DATA_FOLDER + f\"sustainability_reports/{fi}/\") if file[-4:] == \"json\"]\n",
    "    all_reports_list = []\n",
    "    for file in all_reports:\n",
    "         with open(DATA_FOLDER + f\"sustainability_reports/{fi}/\" + file,) as inputfile:\n",
    "                report = json.load(inputfile)\n",
    "                # filter out empty pages\n",
    "                if report[\"content\"] == \"\":\n",
    "                    empty_json += 1\n",
    "                else : \n",
    "                    all_reports_list.append(report)\n",
    "    \n",
    "    # preprocessing\n",
    "    all_reports_pages_sentences = [] # list of dictionaries, each dictionary is 1 report\n",
    "    for report in all_reports_list:\n",
    "        report_pages, report_sentences = extract_pages_sentences(nlp, report['content'])\n",
    "        report[\"report_pages\"] = report_pages\n",
    "        report[\"report_sentences\"] = report_sentences\n",
    "        all_reports_pages_sentences.append(report)\n",
    "    \n",
    "        \n",
    "    all_reports_pages_sentences_preprocessed = [] # list of dictionaries, each dictionary is 1 report\n",
    "    for report in all_reports_pages_sentences:\n",
    "        report_pages_preprocessed = preprocessing(report[\"report_pages\"])\n",
    "        report_sentences_preprocessed = [preprocessing(page) for page in report[\"report_sentences\"]]\n",
    "        report[\"report_pages_preprocessed\"] = report_pages_preprocessed \n",
    "        report[\"report_sentences_preprocessed\"] = report_sentences_preprocessed \n",
    "        all_reports_pages_sentences_preprocessed.append(report)\n",
    "     \n",
    "    all_reports_pages_sentences_preprocessed_filtered = []\n",
    "    for report in all_reports_pages_sentences_preprocessed:\n",
    "        # filter by both words and numbers\n",
    "        # pages\n",
    "        filtered_report_direct_highLevel,filtered_report_indirect_highLevel = filter_report_highLevel(report[\"report_pages_preprocessed\"])\n",
    "        filtered_report_pages_direct_numbers = filter_report_numbers(filtered_report_direct_highLevel)\n",
    "        filtered_report_pages_indirect_numbers = filter_report_numbers(filtered_report_indirect_highLevel)\n",
    "        report[\"filtered_report_pages_direct\"] = filtered_report_pages_direct_numbers\n",
    "        report[\"filtered_report_pages_indirect\"] = filtered_report_pages_indirect_numbers\n",
    "        index_direct = [page_no-1 for page_no in filtered_report_pages_direct_numbers.keys()] \n",
    "        index_indirect = [page_no-1 for page_no in filtered_report_pages_indirect_numbers.keys()] \n",
    "\n",
    "        # sentences\n",
    "        filtered_report_sentences_direct_numbers = {}\n",
    "        for page in filtered_report_pages_direct_numbers.keys():\n",
    "            filtered_report_sentences_direct_numbers[page] = report[\"report_sentences_preprocessed\"][page-1]\n",
    "        filtered_report_sentences_indirect_numbers = {}\n",
    "        for page in filtered_report_pages_indirect_numbers.keys():\n",
    "            filtered_report_sentences_indirect_numbers[page] = report[\"report_sentences_preprocessed\"][page-1]\n",
    "        report[\"filtered_report_sentences_direct\"] = filtered_report_sentences_direct_numbers\n",
    "        report[\"filtered_report_sentences_indirect\"] = filtered_report_sentences_indirect_numbers\n",
    "\n",
    "        # tables\n",
    "        report[\"filtered_report_tables_direct\"] = filter_tables(filtered_report_pages_direct_numbers)\n",
    "        report[\"filtered_report_tables_indirect\"] = filter_tables(filtered_report_pages_indirect_numbers)\n",
    "        all_reports_pages_sentences_preprocessed_filtered.append(report)\n",
    "        \n",
    "    with open(DATA_FOLDER + \"preprocessed_sustainability_reports/\" + f'all_{fi}_preprocessed_vFINAL.json', \"w\") as outfile:  \n",
    "        json.dump(all_reports_pages_sentences_preprocessed_filtered, outfile)\n",
    "\n",
    "print(empty_json)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading In Pre-processed Data\n",
    "-- Skip all previous sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinminaw/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_FOLDER + \"preprocessed_sustainability_reports/\" + 'all_pensionfunds_preprocessed.json',) as inputfile:  \n",
    "    all_pensionfunds_preprocessed = json.load(inputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
