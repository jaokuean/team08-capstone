{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### New url\n",
    "\n",
    "User can upload file from local computer or from online. Currently new pdfs will be stored as 1 json  in a folder called in \"../data/sustainability_reports_new\". \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/xinminaw/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xinminaw/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys  \n",
    "import os\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "\n",
    "# PDF text extraction\n",
    "from pdfminer3.layout import LAParams, LTTextBox\n",
    "from pdfminer3.pdfpage import PDFPage\n",
    "from pdfminer3.pdfinterp import PDFResourceManager\n",
    "from pdfminer3.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer3.converter import PDFPageAggregator\n",
    "from pdfminer3.converter import TextConverter\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "# Others\n",
    "import requests\n",
    "import io\n",
    "import string\n",
    "import re\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "import io\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import spacy\n",
    "#spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner'])\n",
    "\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(stop_words)\n",
    "\n",
    "DATA_FOLDER = \"../data/\"\n",
    "########################################## DATA COLLECTION & PREPROCSSING ##########################\n",
    "\n",
    "# Text extraction from pdf\n",
    "def extract_pdf(file, verbose=False):\n",
    "\n",
    "    \n",
    "    if verbose:\n",
    "        print('Processing {}'.format(file))\n",
    "\n",
    "    try:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        codec = 'utf-8'\n",
    "        laparams = LAParams()\n",
    "\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, codec=codec, laparams=laparams)\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "        \n",
    "        password = \"\"\n",
    "        maxpages = 0\n",
    "        caching = True\n",
    "        pagenos = set()\n",
    "\n",
    "        content = []\n",
    "\n",
    "        for page in PDFPage.get_pages(file,\n",
    "                                      pagenos, \n",
    "                                      maxpages=maxpages,\n",
    "                                      password=password,\n",
    "                                      caching=True,\n",
    "                                      check_extractable=False):\n",
    "\n",
    "            page_interpreter.process_page(page)\n",
    "\n",
    "            content.append(fake_file_handle.getvalue())\n",
    "\n",
    "            fake_file_handle.truncate(0)\n",
    "            fake_file_handle.seek(0)        \n",
    "\n",
    "        text = '##PAGE_BREAK##'.join(content)\n",
    "\n",
    "        # close open handles\n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # close open handles\n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "\n",
    "        return \"\"\n",
    "    \n",
    "# Text extraction from url\n",
    "def extract_content(url):\n",
    "    \"\"\"\n",
    "    A simple user define function that, given a url, download PDF text content\n",
    "    Parse PDF and return plain text version\n",
    "    \"\"\"\n",
    "    headers={\"User-Agent\":\"Mozilla/5.0\"}\n",
    "\n",
    "    try:\n",
    "        # retrieve PDF binary stream\n",
    "        r = requests.get(url, allow_redirects=True, headers=headers)\n",
    "        \n",
    "        # access pdf content\n",
    "        text = extract_pdf(io.BytesIO(r.content))\n",
    "\n",
    "        # return concatenated content\n",
    "        return text\n",
    "\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "    \n",
    "# nlp preprocessing\n",
    "def preprocess_lines(line_input):\n",
    "    \n",
    "    # removing header number\n",
    "    line = re.sub(r'^\\s?\\d+(.*)$', r'\\1', line_input)\n",
    "    # removing trailing spaces\n",
    "    line = line.strip()\n",
    "    # words may be split between lines, ensure we link them back together\n",
    "    line = re.sub(r'\\s?-\\s?', '-', line)\n",
    "    # remove space prior to punctuation\n",
    "    line = re.sub(r'\\s?([,:;\\.])', r'\\1', line)\n",
    "    # ESG contains a lot of figures that are not relevant to grammatical structure\n",
    "    line = re.sub(r'\\d{5,}', r' ', line)\n",
    "    # remove emails\n",
    "    line = re.sub(r'\\S*@\\S*\\s?', '', line)\n",
    "    # remove mentions of URLs\n",
    "    line = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', r' ', line)\n",
    "    # remove multiple spaces\n",
    "    line = re.sub(r'\\s+', ' ', line)\n",
    "    # join next line with space\n",
    "    line = re.sub(r' \\n', ' ', line)\n",
    "    line = re.sub(r'.\\n', '. ', line)\n",
    "    line = re.sub(r'\\x0c', ' ', line)\n",
    "    \n",
    "    return line\n",
    "        \n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    printable = set(string.printable)\n",
    "    return ''.join(filter(lambda x: x in printable, text))\n",
    "\n",
    "def not_header(line):\n",
    "    # as we're consolidating broken lines into paragraphs, we want to make sure not to include headers\n",
    "    return not line.isupper()\n",
    "\n",
    "def extract_pages_sentences(nlp, text):\n",
    "    \"\"\"\n",
    "    Extracting ESG statements from raw text by removing junk, URLs, etc.\n",
    "    We group consecutive lines into paragraphs and use spacy to parse sentences.\n",
    "    \"\"\"\n",
    "    MIN_WORDS_PER_PAGE = 500\n",
    "    \n",
    "    pages = text.split('##PAGE_BREAK##')\n",
    "    #print('Number of Pages: {}'.format(len(pages)))\n",
    "\n",
    "    lines = []\n",
    "    for i in range(len(pages)):\n",
    "        page_number = i + 1\n",
    "        page = pages[i]\n",
    "        \n",
    "        # remove non ASCII characters\n",
    "        text = remove_non_ascii(page)\n",
    "        \n",
    "        # if len(text.split(' ')) < MIN_WORDS_PER_PAGE:\n",
    "        #     print(f'Skipped Page: {page_number}')\n",
    "        #     continue\n",
    "        \n",
    "        prev = \"\"\n",
    "        for line in text.split('\\n\\n'):\n",
    "            # aggregate consecutive lines where text may be broken down\n",
    "            # only if next line starts with a space or previous does not end with dot.\n",
    "            if(line.startswith(' ') or not prev.endswith('.')):\n",
    "                prev = prev + ' ' + line\n",
    "            else:\n",
    "                # new paragraph\n",
    "                lines.append(prev)\n",
    "                prev = line\n",
    "\n",
    "        # don't forget left-over paragraph\n",
    "        lines.append(prev)\n",
    "        lines.append('##SAME_PAGE##')\n",
    "        \n",
    "    lines = '  '.join(lines).split('##SAME_PAGE##')\n",
    "    \n",
    "    # clean paragraphs from extra space, unwanted characters, urls, etc.\n",
    "    # best effort clean up, consider a more versatile cleaner\n",
    "    \n",
    "    pages_content = []\n",
    "    pages_sentences = []\n",
    "\n",
    "    for line in lines[:-1]: # looping through each page\n",
    "        \n",
    "        line = preprocess_lines(line)       \n",
    "        pages_content.append(str(line).strip())\n",
    "\n",
    "        sentences = []\n",
    "        # split paragraphs into well defined sentences using spacy\n",
    "        for part in list(nlp(line).sents):\n",
    "            sentences.append(str(part).strip())\n",
    "\n",
    "        #sentences += nltk.sent_tokenize(line)\n",
    "            \n",
    "        # Only interested in full sentences and sentences with 10 to 100 words. --> filter out first page/content page\n",
    "        sentences = [s for s in sentences if re.match('^[A-Z][^?!.]*[?.!]$', s) is not None]\n",
    "        sentences = [s.replace('\\n', ' ') for s in sentences]\n",
    "#       sentences = [s for s in sentences if (len(s.split(' ')) > 10) & (len(s.split(' ')) < 100)]\n",
    "        \n",
    "        pages_sentences.append(sentences)\n",
    "        \n",
    "    return pages_content, pages_sentences #list, list of list where page is index of outer list\n",
    "\n",
    "\n",
    "# lowercase & lemmatisation\n",
    "def preprocessing(report):\n",
    "    report_pages = []\n",
    "\n",
    "    def para_to_sent(para):\n",
    "        sentences = []\n",
    "        # split paragraphs into well defined sentences using spacy\n",
    "        for part in list(nlp(para).sents):\n",
    "            sentences.append(str(part).strip())\n",
    "        return sentences\n",
    "\n",
    "    # Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "    def remove_stopwords(texts):\n",
    "        return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "    def make_bigrams(texts):\n",
    "        return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "    def make_trigrams(texts):\n",
    "        return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "    def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "        texts_out = []\n",
    "        doc = nlp(texts) \n",
    "        texts_out.append(\" \".join([token.lemma_ for token in doc]))\n",
    "        return texts_out\n",
    "    \n",
    "    for page in report:\n",
    "\n",
    "        sentences = para_to_sent(page.lower())\n",
    "\n",
    "        # # Build the bigram and trigram models\n",
    "        # bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "        # trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "        # # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "        # bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "        # trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "        # # Remove Stop Words\n",
    "        # data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "        # # Form Bigrams\n",
    "        # data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "        # Do lemmatization keeping only noun, adj, vb, adv\n",
    "        page_data = []\n",
    "        for sentence in sentences : \n",
    "            data_lemmatized = lemmatization(sentence, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "            page_data.extend(data_lemmatized)\n",
    "        page_para_lemma = \"\".join(page_data)\n",
    "        \n",
    "        report_pages.append(page_para_lemma)\n",
    "    \n",
    "    return report_pages\n",
    "\n",
    "\n",
    "\n",
    "# filtering report apges\n",
    "def lemmatization(text_list, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for texts in text_list:\n",
    "        texts = texts.lower()\n",
    "        texts_out.append(\" \".join([token.lemma_ for token in nlp(texts)]))\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "def filter_report_highLevel(report):\n",
    "    \n",
    "    relevant_terms_directFilter = set([\"carbon\",\"co2\",\"environment\",\"GHG emissions\",\"Greenhouse Gas\",\"carbon footprint\",\"carbon emissions\",\"Scope 1\",\"Scope 2\",\n",
    "                               \"Scope 3\", \"WACI\",\"Carbon Intensity\",\"carbon pricing\",\"net-zero\",\"metrics and targets\",\"TCFD\",\n",
    "                                \"sustainability goals\",\"decarbonisation\",\"climate\",'energy', 'emission', 'emissions', 'renewable', 'carbon', 'fuel', 'power', \n",
    "                               'green', 'gas', 'green energy', 'sustainable', 'climate', 'sustainability', 'environmental', 'environment', 'GHG', \n",
    "                               'decarbon', 'energy consumption', 'paper consumption','water consumption', 'carbon intensity', 'waste management', 'electricity consumption', \n",
    "                                'cdp', 'global warming', 'business travel','climate solutions', 'decarbonization', 'cvar', 'climate value-at-risk','waste output'])\n",
    "    relevant_terms_combinationA = [\"emissions\",\"exposure\",\"carbon related\",\"esg\",\"sustainable\",\"green\",\"climate sensitive\",\"impact investing\", \"investment framework\", 'msci', 'ftse', 'responsible investing', 'responsible investment','transition']\n",
    "    relevant_terms_combinationB = [\"portfolio\",\"assets\",\"AUM\",\"investment\",\"financing\",\"ratings\",\"revenue\",\"bond\",\"goal\",\"insurance\", \"equity\", \"swap\", \"option\", \"portfolio holdings\", \"risk management\",'financial products']\n",
    "    relevant_terms_combinationC = [\"net zero\",\"carbon footprint\",\"CO2\",\"carbon\",\"oil\",\"coal\", \"gas\", \"fossil fuel\",\"green\"] # compare [CB or CA]  compare with ABC\n",
    "    relevant_terms_combination_directFilter_lem = lemmatization(relevant_terms_directFilter)\n",
    "    relevant_terms_combinationA_lem = lemmatization(relevant_terms_combinationA)\n",
    "    relevant_terms_combinationB_lem = lemmatization(relevant_terms_combinationB)\n",
    "    relevant_terms_combinationC_lem = lemmatization(relevant_terms_combinationC)\n",
    "    \n",
    "    \n",
    "    filtered_report_direct = {}\n",
    "    filtered_report_indirect = {}\n",
    "    for i in range(len(report)):\n",
    "        page = report[i]\n",
    "        page_number = i + 1\n",
    "        no_words = len(page.split(\" \"))\n",
    "        if sum(map(page.__contains__, relevant_terms_combination_directFilter_lem)) > 2:\n",
    "            filtered_report_direct[page_number] = page\n",
    "        elif (any(map(page.__contains__, relevant_terms_combinationA_lem)) and any(map(page.__contains__, relevant_terms_combinationC_lem))) or (any(map(page.__contains__, relevant_terms_combinationB_lem)) and any(map(page.__contains__, relevant_terms_combinationC_lem))):\n",
    "            filtered_report_indirect[page_number] = page\n",
    "    return filtered_report_direct,filtered_report_indirect\n",
    "\n",
    "\n",
    "# filter digits\n",
    "def is_number(string): \n",
    "    test_str = string\n",
    "    # next() checking for each element, reaches end, if no element found as digit\n",
    "    res = True if next((chr for chr in test_str if chr.isdigit()), None) else False\n",
    "    return res\n",
    "\n",
    "#filter date --> \"september\"/\"Monday at 12:01am\"/1999 etc\n",
    "from dateutil.parser import parse\n",
    "def is_date(string):\n",
    "    if re.match('.*([1-2][0-9]{3})', string) != None:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def filter_report_numbers(filtered_report):\n",
    "    filtered_report_numbers = {}\n",
    "    for page_number,page in filtered_report.items():\n",
    "        # remove all dates from page first\n",
    "        page_no_date = \" \".join([word for word in page.split(\" \") if is_date(word) == False])\n",
    "        # retain pages with numbers\n",
    "        if is_number(page_no_date):\n",
    "            filtered_report_numbers[page_number] = page           \n",
    "    return filtered_report_numbers\n",
    "\n",
    "\n",
    "\n",
    "def filter_tables(filtered_report): # if page contains at least 10 numbers + 1 units\n",
    "    units = ['tonnes', 'tons', 'kwh', 'kg', 'kilogram', 'kilowatt hour', 'gigajoules', 'gj', 'litre', 'liter', \n",
    "              'co2e', 'tco2e', 'tco2', 'mwh', 'megawatt hour', 'gwh', 'gigawatt hour', '%', 'cubic metres', \n",
    "              'cm3', 'm3', 'per employee','co2']\n",
    "    filtered_report_numbers = {}\n",
    "    for page_number,page in filtered_report.items():\n",
    "        no_numbers = 0\n",
    "        units_flag = 0\n",
    "        for word in page.split(\" \"):\n",
    "            try: \n",
    "                float(word)\n",
    "                if is_date(word) == False:\n",
    "                    no_numbers += 1\n",
    "            except: \n",
    "                if any(char.isdigit() for char in word):\n",
    "                    no_numbers += 1\n",
    "        for unit in units:\n",
    "            if unit in page:\n",
    "                units_flag += 1     \n",
    "        if (no_numbers >= 10) & (units_flag >= 1):\n",
    "            filtered_report_numbers[page_number] = page\n",
    "    return filtered_report_numbers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new pdf is saved in a new json file & also appended to existing file\n",
    "# report_url can be either from internet : url downloaded=False OR local : path to pdf downloaded=True\n",
    "\n",
    "def upload_pdf(report_url,report_company,report_year,downloaded=False):\n",
    "    if downloaded == True:\n",
    "        with open(report_url,\"rb\") as inputfile:\n",
    "            report_content = extract_pdf(inputfile)\n",
    "    else:\n",
    "        report_content = extract_content(report_url)\n",
    "        \n",
    "    report = {'company':report_company, 'year':report_year,'url':report_url, 'content':report_content}\n",
    "    \n",
    "    if report[\"content\"] == \"\":\n",
    "        print(\"Unable to get PDF\")\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        report_pages, report_sentences = extract_pages_sentences(nlp, report['content'])\n",
    "        report[\"report_pages\"] = report_pages\n",
    "        report[\"report_sentences\"] = report_sentences\n",
    "        report_pages_preprocessed = preprocessing(report[\"report_pages\"])\n",
    "        report_sentences_preprocessed = [preprocessing(page) for page in report[\"report_sentences\"]]\n",
    "        report[\"report_pages_preprocessed\"] = report_pages_preprocessed \n",
    "        report[\"report_sentences_preprocessed\"] = report_sentences_preprocessed \n",
    "        \n",
    "        # pages\n",
    "        filtered_report_direct_highLevel,filtered_report_indirect_highLevel = filter_report_highLevel(report[\"report_pages_preprocessed\"])\n",
    "        filtered_report_pages_direct_numbers = filter_report_numbers(filtered_report_direct_highLevel)\n",
    "        filtered_report_pages_indirect_numbers = filter_report_numbers(filtered_report_indirect_highLevel)\n",
    "        report[\"filtered_report_pages_direct\"] = filtered_report_pages_direct_numbers\n",
    "        report[\"filtered_report_pages_indirect\"] = filtered_report_pages_indirect_numbers\n",
    "        index_direct = [page_no-1 for page_no in filtered_report_pages_direct_numbers.keys()] \n",
    "        index_indirect = [page_no-1 for page_no in filtered_report_pages_indirect_numbers.keys()] \n",
    "\n",
    "        # sentences\n",
    "        filtered_report_sentences_direct_numbers = {}\n",
    "        for page in filtered_report_pages_direct_numbers.keys():\n",
    "            filtered_report_sentences_direct_numbers[page] = report[\"report_sentences_preprocessed\"][page-1]\n",
    "        filtered_report_sentences_indirect_numbers = {}\n",
    "        for page in filtered_report_pages_indirect_numbers.keys():\n",
    "            filtered_report_sentences_indirect_numbers[page] = report[\"report_sentences_preprocessed\"][page-1]\n",
    "        report[\"filtered_report_sentences_direct\"] = filtered_report_sentences_direct_numbers\n",
    "        report[\"filtered_report_sentences_indirect\"] = filtered_report_sentences_indirect_numbers\n",
    "\n",
    "        # tables\n",
    "        report[\"filtered_report_tables_direct\"] = filter_tables(filtered_report_pages_direct_numbers)\n",
    "        report[\"filtered_report_tables_indirect\"] = filter_tables(filtered_report_pages_indirect_numbers)\n",
    "        \n",
    "        file_path = DATA_FOLDER + \"sustainability_reports/new/\" + report_company + report_year+'.json'\n",
    "        \n",
    "        with open(file_path, \"w\") as outfile:  \n",
    "            json.dump(report, outfile)\n",
    "        \n",
    "        return file_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_url = \"https://www.cppinvestments.com/wp-content/uploads/2019/10/CPPIB_SI_Report_ENG.pdf\"\n",
    "report_company = \"Canada Pension\"\n",
    "report_year = \"2017\"\n",
    "file_path = upload_pdf(report_url,report_company,report_year,downloaded=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/sustainability_reports/new/Canada Pension2017.json'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
