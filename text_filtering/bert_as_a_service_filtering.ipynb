{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4AbYWwPirPV"
   },
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Xbp-Xa1hcbq",
    "outputId": "ac80a225-f89b-4c39-b659-9bbe47878913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "from scipy import spatial\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "z7VqWBsChVNa"
   },
   "outputs": [],
   "source": [
    "data = \"/content/gdrive/MyDrive/Capstone/Data/pipeline_reports\"\n",
    "all_files = os.listdir(data)\n",
    "all_json = {}\n",
    "for file in all_files:\n",
    "    if \"vfinal\" in file:\n",
    "        with open(data + \"/\" + file,) as inputfile:\n",
    "            all_json[file] = json.load(inputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubJ3RgLpJRXr"
   },
   "outputs": [],
   "source": [
    "#test\n",
    "all_json['all_asian_banks_preprocessed.json'][0][\"filtered_report_pages_direct\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS3BOOVXiu4M"
   },
   "source": [
    "## Preprocess Data for BERT Embeddings\n",
    "1. remove numbers --> SCRAPE BECAUSE THE WHOLE POINT IS TO BE ABLE TO CAPTURE NUMBERS???????\n",
    "2. remove punctuations except %,$,&\n",
    "- as numbers & punctations are encoded differently\n",
    "- upper/lower casing dont affect embeddings\n",
    "\n",
    "Json Fields Used : \n",
    "1. use *filtered_report_sentences_direct* & *filtered_report_sentences_indirect* keys to get page numbers of relevant pages and get the unprocesse sentences from *report_sentences*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fIenLl1bfjCd"
   },
   "outputs": [],
   "source": [
    "def remove_numbers(string):\n",
    "    return ''.join(i for i in string if not i.isdigit())\n",
    "\n",
    "def remove_punc(s):\n",
    "    import string\n",
    "    exclude = string.punctuation\n",
    "    final_punc = ''.join(list(i for i in exclude if i not in ['%', '$', '&']))\n",
    "    s = ''.join(ch for ch in s if ch not in list(final_punc))\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HY0Dtou3h43q"
   },
   "outputs": [],
   "source": [
    "bert_json = {}\n",
    "for json_name,json in all_json.items():\n",
    "    fi_list = []\n",
    "    for fi in json:\n",
    "        fi_dict = {}\n",
    "        fi_direct_dict = {}\n",
    "        for page in fi[\"filtered_report_pages_direct\"].keys():\n",
    "            page = int(page)\n",
    "#           fi_direct_dict[page] = list(map(lambda x : remove_punc(x) ,list(map(lambda x : remove_numbers(x) ,fi[\"report_sentences\"][page-1]))))\n",
    "            fi_direct_dict[page] = list(map(lambda x : remove_punc(x) ,fi[\"report_sentences\"][page-1]))\n",
    "        fi_indirect_dict = {}\n",
    "        for page in fi[\"filtered_report_pages_indirect\"].keys():\n",
    "            page = int(page)\n",
    " #          fi_indirect_dict[page] = list(map(lambda x : remove_punc(x) ,list(map(lambda x : remove_numbers(x) ,fi[\"report_sentences\"][page-1]))))\n",
    "            fi_indirect_dict[page] =  list(map(lambda x : remove_punc(x) ,fi[\"report_sentences\"][page-1]))      \n",
    "        fi_dict[\"company\"] = fi[\"company\"] #identifier\n",
    "        fi_dict[\"year\"] = fi[\"year\"] #identifier\n",
    "        fi_dict[\"filtered_report_pages_direct_bert\"]  = fi_direct_dict\n",
    "        fi_dict[\"filtered_report_pages_indirect_bert\"]  = fi_indirect_dict\n",
    "        fi_list.append(fi_dict)\n",
    "    bert_json[json_name] = fi_list\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMILMqNvrb04"
   },
   "source": [
    "### BERT embeddings\n",
    "\n",
    "Compare text similarity:\n",
    "- BERT embeddings + Cosine Similarity does the best\n",
    "\n",
    "\n",
    "Reference : https://medium.com/@adriensieg/text-similarities-da019229c894\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vt-cwriBrKcU",
    "outputId": "862e119f-e9c7-4670-caad-3365c38f4ebb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n",
      "Collecting bert-serving-client\n",
      "  Downloading bert_serving_client-1.10.0-py2.py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: pyzmq>=17.1.0 in /usr/local/lib/python3.7/dist-packages (from bert-serving-client) (22.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bert-serving-client) (1.19.5)\n",
      "Installing collected packages: bert-serving-client\n",
      "Successfully installed bert-serving-client-1.10.0\n",
      "Collecting bert-serving-server[http]\n",
      "  Downloading bert_serving_server-1.10.0-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 216 kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bert-serving-server[http]) (1.19.5)\n",
      "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from bert-serving-server[http]) (1.1.0)\n",
      "Requirement already satisfied: pyzmq>=17.1.0 in /usr/local/lib/python3.7/dist-packages (from bert-serving-server[http]) (22.3.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from bert-serving-server[http]) (1.15.0)\n",
      "Collecting GPUtil>=1.3.0\n",
      "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
      "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from bert-serving-server[http]) (1.1.4)\n",
      "Collecting flask-cors\n",
      "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
      "Collecting flask-compress\n",
      "  Downloading Flask_Compress-1.10.1-py3-none-any.whl (7.9 kB)\n",
      "Collecting flask-json\n",
      "  Downloading Flask_JSON-0.3.4-py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: bert-serving-client in /usr/local/lib/python3.7/dist-packages (from bert-serving-server[http]) (1.10.0)\n",
      "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->bert-serving-server[http]) (1.0.1)\n",
      "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->bert-serving-server[http]) (7.1.2)\n",
      "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->bert-serving-server[http]) (1.1.0)\n",
      "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->bert-serving-server[http]) (2.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->bert-serving-server[http]) (2.0.1)\n",
      "Collecting brotli\n",
      "  Downloading Brotli-1.0.9-cp37-cp37m-manylinux1_x86_64.whl (357 kB)\n",
      "\u001b[K     |████████████████████████████████| 357 kB 11.3 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: GPUtil\n",
      "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=7f5e7c8b704528512868ec15b1ab5ad79d266aa0b4fdd46f1b603e28ab92ba7c\n",
      "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
      "Successfully built GPUtil\n",
      "Installing collected packages: GPUtil, brotli, flask-json, flask-cors, flask-compress, bert-serving-server\n",
      "Successfully installed GPUtil-1.4.0 bert-serving-server-1.10.0 brotli-1.0.9 flask-compress-1.10.1 flask-cors-3.0.10 flask-json-0.3.4\n",
      "--2021-11-01 19:03:50--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.193.128, 173.194.194.128, 173.194.195.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.193.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 407727028 (389M) [application/zip]\n",
      "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
      "\n",
      "uncased_L-12_H-768_ 100%[===================>] 388.84M   169MB/s    in 2.3s    \n",
      "\n",
      "2021-11-01 19:03:52 (169 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
      "\n",
      "Archive:  uncased_L-12_H-768_A-12.zip\n",
      "   creating: uncased_L-12_H-768_A-12/\n",
      "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
      "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
      "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
      "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
      "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n",
      "gdrive\t  sample_data\t\t   uncased_L-12_H-768_A-12.zip\n",
      "out.file  uncased_L-12_H-768_A-12\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "\n",
    "!pip install bert-serving-client\n",
    "!pip install -U bert-serving-server[http]\n",
    "\n",
    "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "!unzip uncased_L-12_H-768_A-12.zip\n",
    "!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 > out.file 2>&1 &\n",
    "\n",
    "\n",
    "!ls  # you should see uncased_something_.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ynej_mzAtd6a",
    "outputId": "9ba75c1a-a710-4bab-8016-e84b4b58c023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.10375775 -0.41058773  0.6212927  ... -0.29355988  0.26888955\n",
      "   0.14812963]\n",
      " [ 0.76116025 -0.06709035  0.07512389 ... -0.04068742  0.14368583\n",
      "   0.19906425]\n",
      " [-0.06862521  0.08689015  0.13055946 ... -0.22566105  0.22352234\n",
      "  -0.00201502]\n",
      " ...\n",
      " [-0.15848124 -0.2721892  -0.12754104 ... -0.069576   -0.19471477\n",
      "   0.15317261]\n",
      " [-0.02237912 -0.6119282  -0.27465814 ... -0.12987773 -0.4983658\n",
      "  -0.05472057]\n",
      " [ 0.10988103 -0.53783214  0.6870812  ... -0.6287739   0.19253941\n",
      "  -0.21154976]]\n"
     ]
    }
   ],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient(check_length=False)\n",
    "\n",
    "relevant_sentences = ['In 2019, Citi financed $74 million of subordinate lien bonds that were certified green, given the projects environmental aspects.', \n",
    "             'In addition, our cogeneration plant, fueled by natural gas, will produce heat and electricity on-site, reducing the building\\'s carbon footprint by 34 percent.',\n",
    "             'These efforts reduced energy consumption by more than 2,100 metric tons (mt) of carbon dioxide equivalents (CO2e) during the one-year challenge.',\n",
    "             'The companies in our equity portfolio emitted around 133 tonnes of CO2 -equivalents for every million US dollars of revenue.', \n",
    "             'The equity portfolio’s carbon intensity was 9 percent below that of the benchmark index.',\n",
    "             'A total of 106 companies that produce certain types of weapon, tobacco or coal, or use coal for power production, are currently excluded from the fund', \n",
    "             'For public and private assets, excluding cash and non-equity derivatives as they were not reported in 2019, our year-overyear portfolio weighted average carbon intensity was reduced by approximately 23%.',\n",
    "             'Having met these targets, we have set new, more ambitious ones: to reduce the Fund’s emissions intensity by 40% and fossil fuel reserves by 80% by 2025.',\n",
    "             'The carbon footprint of the non-listed companies was 0.6 tCO₂e per million SEK invested',\n",
    "             'Energy consumption and carbon emissions per unit area were 149 kWh/ m² and 0.037 tCO₂e/m², which means a reduction of 9 per cent and 12 per cent, respectively.',\n",
    "             'The carbon intensity (CO2 equivalent tons per million yen of sales) of GPIF’s equity and corporate bond portfolio decreased by 15.3%, from 2.29 tons to 1.94 tons, in the space of a year.'\n",
    "             'Based on our percentage holdings in each company, the total emissions of the equity portfolio were 108 million tonnes of CO2 - equivalents in 2019.',\n",
    "             'The carbon footprint of the companies in our equity portfolio',\n",
    "             'The companies in our equity portfolio emitted around 156 tonnes of CO2 -equivalents for every million US dollars (USD) of revenue.'\n",
    "             'The carbon intensity of the companies in the equity portfolio and the benchmark index decreased by 16 and 17 percent respectively from 2018 to 2019.',\n",
    "             'We are focused on supporting the goal of net zero greenhouse gas emissions by 2050, in line with global efforts to limit warming to 1.5°C. ',\n",
    "             'quantitative target for ESG-themed investments and finance of ¥700 billion ',\n",
    "             'Commit to reduce investment carbon footprint by',\n",
    "             'esg investing', 'green bonds', 'Green Investment target', 'Achieve 100% renewable electricity by 2025'\n",
    "             ]\n",
    "relevant_sentences_embeddings = bc.encode(relevant_sentences)\n",
    "\n",
    "print(relevant_sentences_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJuixdO_N0zl"
   },
   "outputs": [],
   "source": [
    "def cosine_distance(s1,s2):\n",
    "    return 1 - spatial.distance.cosine(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXeNZzh6tl_l"
   },
   "outputs": [],
   "source": [
    "all_cosine_similarities_ = [] # all combinations of cosine similarity\n",
    "all_filtered_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEB-0Vc2qgzo"
   },
   "outputs": [],
   "source": [
    "all_relevant_sentences = []\n",
    "for json_name,json in bert_json.items():\n",
    "    fi_list = []\n",
    "    for fi_index in range(len(json))[:10]: #change\n",
    "        fi = json[fi_index]\n",
    "        page_relevant_sentences = {}\n",
    "        page_relevant_sentences_original = {}\n",
    "        for page_number, page in fi[\"filtered_report_pages_direct_bert\"].items():\n",
    "            relevant_sentences = []\n",
    "            relevant_sentences_original = []\n",
    "            for sentence_index in range(len(page)):\n",
    "                original_sentence = all_json[json_name][fi_index][\"report_sentences\"][page_number-1][sentence_index]\n",
    "                all_filtered_sentences.append(original_sentence)\n",
    "                sentence = page[sentence_index]\n",
    "                sentence_encoding = bc.encode([sentence])[0]\n",
    "                for relevant_sentence in relevant_sentences_embeddings:\n",
    "                    cosine_similarity = cosine_distance(sentence_encoding,relevant_sentence)\n",
    "                    all_cosine_similarities.append(cosine_similarity)\n",
    "                    if cosine_similarity >= 0.8 : # tentative\n",
    "                        relevant_sentences.append([sentence,cosine_similarity])\n",
    "                        relevant_sentences_original.append(original_sentence)\n",
    "                        all_relevant_sentences.append(original_sentence)\n",
    "                        break\n",
    "            if len(relevant_sentences) != 0 :\n",
    "                page_relevant_sentences[page_number] = relevant_sentences\n",
    "                page_relevant_sentences_original[page_number] = relevant_sentences_original\n",
    "        bert_json[json_name][fi_index][\"bert_relevant_sentences_direct\"] = page_relevant_sentences\n",
    "        bert_json[json_name][fi_index][\"bert_relevant_sentences_direct_original\"] = page_relevant_sentences_original\n",
    "\n",
    "\n",
    "        page_relevant_sentences_indirect = {}\n",
    "        page_relevant_sentences_indirect_original = {}\n",
    "        for page_number, page in fi[\"filtered_report_pages_indirect_bert\"].items():\n",
    "            relevant_sentences = []\n",
    "            relevant_sentences_original = []\n",
    "            for sentence_index in range(len(page)):\n",
    "                original_sentence = all_json[json_name][fi_index][\"report_sentences\"][page_number-1][sentence_index]\n",
    "                all_filtered_sentences.append(original_sentence)\n",
    "                sentence = page[sentence_index]\n",
    "                sentence_encoding = bc.encode([sentence])[0]\n",
    "                for relevant_sentence in relevant_sentences_embeddings:\n",
    "                    cosine_similarity = cosine_distance(sentence_encoding,relevant_sentence)\n",
    "                    all_cosine_similarities.append(cosine_similarity)\n",
    "                    if cosine_similarity >= 0.7357 : # tentative maybe must b smilar to most terms?\n",
    "                        relevant_sentences.append([sentence,cosine_similarity])\n",
    "                        relevant_sentences_original.append(original_sentence)\n",
    "                        all_relevant_sentences.append(original_sentence)\n",
    "                        break\n",
    "            if len(relevant_sentences) != 0 :\n",
    "                page_relevant_sentences_indirect[page_number] = relevant_sentences\n",
    "                page_relevant_sentences_indirect_original[page_number] = relevant_sentences_original\n",
    "        bert_json[json_name][fi_index][\"bert_relevant_sentences_indirect\"] = page_relevant_sentences_indirect\n",
    "        bert_json[json_name][fi_index][\"bert_relevant_sentences_indirect_original\"] = page_relevant_sentences_indirect_original\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 549
    },
    "id": "t0AkjdoAq0gx",
    "outputId": "d1db2d5f-11c9-4b21-c9fe-a8a6103596c0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cos_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>249264.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.681896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.079136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.206599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.641717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.693079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.735703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cos_similarity\n",
       "count   249264.000000\n",
       "mean         0.681896\n",
       "std          0.079136\n",
       "min          0.206599\n",
       "25%          0.641717\n",
       "50%          0.693079\n",
       "75%          0.735703\n",
       "max          1.000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to determine threshold\n",
    "sns.histplot(all_cosine_similarities)\n",
    "pd.DataFrame(all_cosine_similarities,columns=[\"cos_similarity\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UQDNeNhvFwG",
    "outputId": "0c89dbee-c5ec-46ab-a684-f2d0ff23f7f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18465\n",
      "8470\n"
     ]
    }
   ],
   "source": [
    "print(len(all_filtered_sentences))\n",
    "print(len(all_relevant_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlvc_lae23Vw"
   },
   "source": [
    "### Splitting Data for labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cupog-ckyYG6",
    "outputId": "e1548519-658a-4849-ba62-6046b1e895c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6776, 1)\n",
      "(5082, 1)\n"
     ]
    }
   ],
   "source": [
    "# all_filtered_sentences_df = pd.DataFrame(all_filtered_sentences,columns=[\"filtered_sentences\"])\n",
    "# n = all_filtered_sentences_df.shape[0]//5\n",
    "# split_1 = all_filtered_sentences_df.sample(n=n,random_state=200) #random state is a seed value\n",
    "# test = all_filtered_sentences_df.drop(split_1.index)\n",
    "# split_2 = test.sample(n=n,random_state=200)\n",
    "# test = test.drop(split_2.index)\n",
    "# split_3 = test.sample(n=n,random_state=200) \n",
    "# test = test.drop(split_3.index)\n",
    "# split_4 = test.sample(n=n,random_state=200) \n",
    "# test = test.drop(split_4.index)\n",
    "# split_5 = test\n",
    " \n",
    "data = \"/content/gdrive/MyDrive/Capstone/Data/to_label\"\n",
    "all_relevant_sentences_df = pd.DataFrame(all_relevant_sentences,columns=[\"relevant_sentences\"])\n",
    "n = all_relevant_sentences_df.shape[0]//5\n",
    "split_1 = all_relevant_sentences_df.sample(n=n,random_state=200)\n",
    "split_1.to_csv(data + \"/split_1.csv\")\n",
    "test = all_relevant_sentences_df.drop(split_1.index)\n",
    "print(test.shape)\n",
    "split_2 = test.sample(n=n,random_state=200)\n",
    "split_2.to_csv(data + \"/split_2.csv\")\n",
    "test = test.drop(split_2.index)\n",
    "print(test.shape)\n",
    "split_3 = test.sample(n=n,random_state=200)\n",
    "split_3.to_csv(data + \"/split_3.csv\")\n",
    "test = test.drop(split_3.index)\n",
    "split_4 = test.sample(n=n,random_state=200)\n",
    "split_4.to_csv(data + \"/split_4.csv\")\n",
    "test = test.drop(split_4.index)\n",
    "split_5 = test\n",
    "split_5.to_csv(data + \"/split_5.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VSezqHGdJqz"
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBO195fMhtsF"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from scipy import spatial\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy\n",
    "\n",
    "\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BQamHPqgfhGJ"
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def remove_punc(s):\n",
    "    import string\n",
    "    exclude = string.punctuation\n",
    "    final_punc = ''.join(list(i for i in exclude if i not in ['%', '$', '&']))\n",
    "    s = ''.join(ch for ch in s if ch not in list(final_punc))\n",
    "    return s\n",
    "\n",
    "def cosine_distance(s1,s2):\n",
    "    return 1 - spatial.distance.cosine(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgB2qopIeO9S"
   },
   "outputs": [],
   "source": [
    "# bert as a service\n",
    "%tensorflow_version 1.x\n",
    "\n",
    "!pip install bert-serving-client\n",
    "!pip install -U bert-serving-server[http]\n",
    "\n",
    "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "!unzip uncased_L-12_H-768_A-12.zip\n",
    "!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 > out.file 2>&1 &\n",
    "\n",
    "\n",
    "!ls  # you should see uncased_something_.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgSC1qbPYs4A"
   },
   "outputs": [],
   "source": [
    "# from bert_serving.client import BertClient\n",
    "# import numpy\n",
    "# bc = BertClient(check_length=False)\n",
    "\n",
    "def create_bert_embeddings(jsonfile):\n",
    "  if jsonfile[\"bert_relevant_sentences_direct_original\"] != {}:\n",
    "    embeddings_dict = {}\n",
    "    for page,sentences in jsonfile[\"bert_relevant_sentences_direct_original\"].items():\n",
    "      embeddings_dict[page] = []\n",
    "      for sentence in sentences:\n",
    "        sentence_encoding = list(bc.encode([sentence])[0])       \n",
    "        embeddings_dict[page].append(list(map(lambda x: numpy.float64(x),sentence_encoding)))\n",
    "    jsonfile[\"bert_relevant_sentences_direct_original_embeddings\"] = embeddings_dict\n",
    "  else:\n",
    "    jsonfile[\"bert_relevant_sentences_direct_original_embeddings\"] = {}\n",
    "\n",
    "  if jsonfile[\"bert_relevant_sentences_indirect_original\"] != {}:\n",
    "    embeddings_dict = {}\n",
    "    for page,sentences in jsonfile[\"bert_relevant_sentences_indirect_original\"].items():\n",
    "      embeddings_dict[page] = []\n",
    "      for sentence in sentences:\n",
    "        sentence_encoding = list(bc.encode([sentence])[0])       \n",
    "        embeddings_dict[page].append(list(map(lambda x: numpy.float64(x),sentence_encoding)))\n",
    "    jsonfile[\"bert_relevant_sentences_indirect_original_embeddings\"] = embeddings_dict\n",
    "  else:\n",
    "    jsonfile[\"bert_relevant_sentences_indirect_original_embeddings\"] = {}\n",
    "  return jsonfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "IXExAH7Gy0lw"
   },
   "outputs": [],
   "source": [
    "# to process 1 json\n",
    "def bert_filtering(file_path):\n",
    "    with open(file_path,) as inputfile:\n",
    "        json_file = json.load(inputfile)\n",
    "        json_file = [json_file]\n",
    "    fi_list = []\n",
    "    #step 1\n",
    "    for fi in json_file:\n",
    "        fi_dict = {}\n",
    "        fi_direct_dict = {}\n",
    "        for page in fi[\"filtered_report_pages_direct\"].keys():\n",
    "            page = int(page)\n",
    "            fi_direct_dict[page] = list(map(lambda x : remove_punc(x) ,fi[\"report_sentences\"][page-1]))\n",
    "        fi_indirect_dict = {}\n",
    "        for page in fi[\"filtered_report_pages_indirect\"].keys():\n",
    "            page = int(page)\n",
    "            fi_indirect_dict[page] =  list(map(lambda x : remove_punc(x) ,fi[\"report_sentences\"][page-1]))      \n",
    "        fi_dict[\"company\"] = fi[\"company\"] #identifier\n",
    "        fi_dict[\"year\"] = fi[\"year\"] #identifier\n",
    "        fi_dict[\"url\"] = fi[\"url\"]\n",
    "        fi_dict[\"filtered_report_pages_direct_bert\"]  = fi_direct_dict\n",
    "        fi_dict[\"filtered_report_pages_indirect_bert\"]  = fi_indirect_dict\n",
    "        fi_list.append(fi_dict)\n",
    "        \n",
    "    # insttantiate BERT\n",
    "    from bert_serving.client import BertClient\n",
    "    bc = BertClient(check_length=False)\n",
    "\n",
    "    relevant_sentences = ['In 2019, Citi financed $74 million of subordinate lien bonds that were certified green, given the projects environmental aspects.', \n",
    "                'In addition, our cogeneration plant, fueled by natural gas, will produce heat and electricity on-site, reducing the building\\'s carbon footprint by 34 percent.',\n",
    "                'These efforts reduced energy consumption by more than 2,100 metric tons (mt) of carbon dioxide equivalents (CO2e) during the one-year challenge.',\n",
    "                'The companies in our equity portfolio emitted around 133 tonnes of CO2 -equivalents for every million US dollars of revenue.', \n",
    "                'The equity portfolio’s carbon intensity was 9 percent below that of the benchmark index.',\n",
    "                'A total of 106 companies that produce certain types of weapon, tobacco or coal, or use coal for power production, are currently excluded from the fund', \n",
    "                'For public and private assets, excluding cash and non-equity derivatives as they were not reported in 2019, our year-overyear portfolio weighted average carbon intensity was reduced by approximately 23%.',\n",
    "                'Having met these targets, we have set new, more ambitious ones: to reduce the Fund’s emissions intensity by 40% and fossil fuel reserves by 80% by 2025.',\n",
    "                'The carbon footprint of the non-listed companies was 0.6 tCO₂e per million SEK invested',\n",
    "                'Energy consumption and carbon emissions per unit area were 149 kWh/ m² and 0.037 tCO₂e/m², which means a reduction of 9 per cent and 12 per cent, respectively.',\n",
    "                'The carbon intensity (CO2 equivalent tons per million yen of sales) of GPIF’s equity and corporate bond portfolio decreased by 15.3%, from 2.29 tons to 1.94 tons, in the space of a year.'\n",
    "                'Based on our percentage holdings in each company, the total emissions of the equity portfolio were 108 million tonnes of CO2 - equivalents in 2019.',\n",
    "                'The carbon footprint of the companies in our equity portfolio',\n",
    "                'The companies in our equity portfolio emitted around 156 tonnes of CO2 -equivalents for every million US dollars (USD) of revenue.'\n",
    "                'The carbon intensity of the companies in the equity portfolio and the benchmark index decreased by 16 and 17 percent respectively from 2018 to 2019.',\n",
    "                'We are focused on supporting the goal of net zero greenhouse gas emissions by 2050, in line with global efforts to limit warming to 1.5°C. ',\n",
    "                'quantitative target for ESG-themed investments and finance of ¥700 billion ',\n",
    "                'Commit to reduce investment carbon footprint by',\n",
    "                'esg investing', 'green bonds', 'Green Investment target', 'Achieve 100% renewable electricity by 2025']\n",
    "    relevant_sentences_embeddings = bc.encode(relevant_sentences)\n",
    "\n",
    "    #step 2\n",
    "    json_list = fi_list\n",
    "    for fi_index in range(len(json_list)): #remove\n",
    "        fi = json_list[fi_index]\n",
    "        page_relevant_sentences = {}\n",
    "        page_relevant_sentences_original = {}\n",
    "        for page_number, page in fi[\"filtered_report_pages_direct_bert\"].items():\n",
    "            relevant_sentences = []\n",
    "            relevant_sentences_original = []\n",
    "            for sentence_index in range(len(page)):\n",
    "                original_sentence = json_file[fi_index][\"report_sentences\"][page_number-1][sentence_index]\n",
    "                sentence = page[sentence_index]\n",
    "                sentence_encoding = bc.encode([sentence])[0]\n",
    "                for relevant_sentence in relevant_sentences_embeddings:\n",
    "                    cosine_similarity = cosine_distance(sentence_encoding,relevant_sentence)\n",
    "                    if cosine_similarity >= 0.8 : # tentative\n",
    "                        relevant_sentences.append(sentence)\n",
    "                        relevant_sentences_original.append(original_sentence)\n",
    "                        break\n",
    "            if len(relevant_sentences) != 0 :\n",
    "                page_relevant_sentences[page_number] = relevant_sentences\n",
    "                page_relevant_sentences_original[page_number] = relevant_sentences_original\n",
    "        #json_list[fi_index][\"bert_relevant_sentences_direct\"] = page_relevant_sentences\n",
    "                #print(len(relevant_sentences))\n",
    "        json_list[fi_index][\"bert_relevant_sentences_direct_original\"] = page_relevant_sentences_original # sentences used to train\n",
    "\n",
    "\n",
    "        page_relevant_sentences_indirect = {}\n",
    "        page_relevant_sentences_indirect_original = {}\n",
    "        for page_number, page in fi[\"filtered_report_pages_indirect_bert\"].items():\n",
    "            relevant_sentences = []\n",
    "            relevant_sentences_original = []\n",
    "            for sentence_index in range(len(page)):\n",
    "                original_sentence = json_file[fi_index][\"report_sentences\"][page_number-1][sentence_index]\n",
    "                sentence = page[sentence_index]\n",
    "                sentence_encoding = bc.encode([sentence])[0]\n",
    "                for relevant_sentence in relevant_sentences_embeddings:\n",
    "                    cosine_similarity = cosine_distance(sentence_encoding,relevant_sentence)\n",
    "                    if cosine_similarity >= 0.7357 : # tentative maybe must b smilar to most terms?\n",
    "                        relevant_sentences.append(sentence)\n",
    "                        relevant_sentences_original.append(original_sentence)\n",
    "                        break\n",
    "            if len(relevant_sentences) != 0 :\n",
    "                page_relevant_sentences_indirect[page_number] = relevant_sentences\n",
    "                page_relevant_sentences_indirect_original[page_number] = relevant_sentences_original\n",
    "                #print(len(relevant_sentences))\n",
    "        #json_list[fi_index][\"bert_relevant_sentences_indirect\"] = page_relevant_sentences_indirect\n",
    "        json_list[fi_index][\"bert_relevant_sentences_indirect_original\"] = page_relevant_sentences_indirect_original # sentences used to train\n",
    "        \n",
    "\n",
    "    #step 3 create embeddings\n",
    "    final_json_embeddings = create_bert_embeddings(json_list[0])\n",
    "    output_path = file_path[:-5] + \"_BERT_embeddings.json\"\n",
    "    \n",
    "\n",
    "    with open(output_path, \"w\") as outfile:  \n",
    "        json.dump(final_json_embeddings, outfile)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ZsM3CPGVdEMF"
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"/content/gdrive/MyDrive/Capstone/Data/pipeline_reports/\"\n",
    "all_files = os.listdir(DATA_FOLDER)\n",
    "for file in all_files:\n",
    "    file_path = DATA_FOLDER + file\n",
    "    # output_name = DATA_FOLDER + file[:-5] + \"_BERT_embeddings.json\"\n",
    "    output_path = bert_filtering(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGQZ1ENAa4dU"
   },
   "source": [
    "Create embeddings json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "fvFzpveqgMb_"
   },
   "outputs": [],
   "source": [
    "# from bert_serving.client import BertClient\n",
    "# import numpy\n",
    "# bc = BertClient(check_length=False)\n",
    "\n",
    "def create_bert_embeddings(jsonfile):\n",
    "  if jsonfile[\"bert_relevant_sentences_direct_original\"] != {}:\n",
    "    embeddings_dict = {}\n",
    "    for page,sentences in jsonfile[\"bert_relevant_sentences_direct_original\"].items():\n",
    "      embeddings_dict[page] = []\n",
    "      for sentence in sentences:\n",
    "        sentence_encoding = list(bc.encode([sentence])[0])       \n",
    "        embeddings_dict[page].append(list(map(lambda x: numpy.float64(x),sentence_encoding)))\n",
    "    jsonfile[\"bert_relevant_sentences_direct_original_embeddings\"] = embeddings_dict\n",
    "  else:\n",
    "    jsonfile[\"bert_relevant_sentences_direct_original_embeddings\"] = {}\n",
    "\n",
    "  if jsonfile[\"bert_relevant_sentences_indirect_original\"] != {}:\n",
    "    embeddings_dict = {}\n",
    "    for page,sentences in jsonfile[\"bert_relevant_sentences_indirect_original\"].items():\n",
    "      embeddings_dict[page] = []\n",
    "      for sentence in sentences:\n",
    "        sentence_encoding = list(bc.encode([sentence])[0])       \n",
    "        embeddings_dict[page].append(list(map(lambda x: numpy.float64(x),sentence_encoding)))\n",
    "    jsonfile[\"bert_relevant_sentences_indirect_original_embeddings\"] = embeddings_dict\n",
    "  else:\n",
    "    jsonfile[\"bert_relevant_sentences_indirect_original_embeddings\"] = {}\n",
    "  return jsonfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "pHuYcS07gMcP"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "file_path = \"/content/gdrive/MyDrive/Capstone/Data/pipeline_reports/Canada Pension2017_BERT.json\"\n",
    "with open(file_path,) as inputfile:\n",
    "    json_file = json.load(inputfile)\n",
    "\n",
    "json_file_embeddings = []\n",
    "for fi in json_file:\n",
    "  json_file_embeddings.append(create_bert_embeddings(fi))\n",
    "\n",
    "output_name = file_path = DATA_FOLDER + \"all_vfinal_BERT_embeddings.json\"\n",
    "with open(output_name, \"w\") as outfile:  \n",
    "    json.dump(json_file_embeddings, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uE_PY2mfpzzH",
    "outputId": "81da44ac-7400-4c6e-9762-7d81327c3fd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['company', 'year', 'filtered_report_pages_direct_bert', 'filtered_report_pages_indirect_bert', 'bert_relevant_sentences_direct_original', 'bert_relevant_sentences_indirect_original'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_json[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DH_JOt38xtAr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": " bert_as_a_service_filtering",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
