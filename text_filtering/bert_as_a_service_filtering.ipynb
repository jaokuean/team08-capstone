{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"../data/preprocessed_sustainability_reports\"\n",
    "all_files = os.listdir(data)\n",
    "all_json = {}\n",
    "for file in all_files:\n",
    "    if \"vfinal\" in file:\n",
    "        with open(data + \"/\" + file,) as inputfile:\n",
    "            all_json[file] = json.load(inputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data for BERT Embeddings\n",
    "1. remove numbers --> SCRAPE BECAUSE THE WHOLE POINT IS TO BE ABLE TO CAPTURE NUMBERS\n",
    "2. remove punctuations except %,$,&\n",
    "- as numbers & punctations are encoded differently\n",
    "- upper/lower casing dont affect embeddings\n",
    "\n",
    "Json Fields Used : \n",
    "1. use *filtered_report_sentences_direct* & *filtered_report_sentences_indirect* keys to get page numbers of relevant pages and get the unprocesse sentences from *report_sentences*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(string):\n",
    "    return ''.join(i for i in string if not i.isdigit())\n",
    "\n",
    "def remove_punc(s):\n",
    "    import string\n",
    "    exclude = string.punctuation\n",
    "    final_punc = ''.join(list(i for i in exclude if i not in ['%', '$', '&']))\n",
    "    s = ''.join(ch for ch in s if ch not in list(final_punc))\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_json = {}\n",
    "for json_name,json in all_json.items():\n",
    "    fi_list = []\n",
    "    for fi in json:\n",
    "        fi_dict = {}\n",
    "        fi_direct_dict = {}\n",
    "        for page in fi[\"filtered_report_pages_direct\"].keys():\n",
    "            page = int(page)\n",
    "#           fi_direct_dict[page] = list(map(lambda x : remove_punc(x) ,list(map(lambda x : remove_numbers(x) ,fi[\"report_sentences\"][page-1]))))\n",
    "            fi_direct_dict[page] = list(map(lambda x : remove_punc(x) ,fi[\"report_sentences\"][page-1]))\n",
    "        fi_indirect_dict = {}\n",
    "        for page in fi[\"filtered_report_pages_indirect\"].keys():\n",
    "            page = int(page)\n",
    " #          fi_indirect_dict[page] = list(map(lambda x : remove_punc(x) ,list(map(lambda x : remove_numbers(x) ,fi[\"report_sentences\"][page-1]))))\n",
    "            fi_indirect_dict[page] =  list(map(lambda x : remove_punc(x) ,fi[\"report_sentences\"][page-1]))      \n",
    "        fi_dict[\"company\"] = fi[\"company\"] #identifier\n",
    "        fi_dict[\"year\"] = fi[\"year\"] #identifier\n",
    "        fi_dict[\"filtered_report_pages_direct_bert\"]  = fi_direct_dict\n",
    "        fi_dict[\"filtered_report_pages_indirect_bert\"]  = fi_indirect_dict\n",
    "        fi_list.append(fi_dict)\n",
    "    bert_json[json_name] = fi_list\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT embeddings\n",
    "\n",
    "Compare text similarity:\n",
    "- BERT embeddings + Cosine Similarity does the best\n",
    "\n",
    "\n",
    "Reference : https://medium.com/@adriensieg/text-similarities-da019229c894\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "\n",
    "!pip install bert-serving-client\n",
    "!pip install -U bert-serving-server[http]\n",
    "\n",
    "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "!unzip uncased_L-12_H-768_A-12.zip\n",
    "!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 > out.file 2>&1 &\n",
    "\n",
    "\n",
    "!ls  # you should see uncased_something_.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient(check_length=False)\n",
    "\n",
    "relevant_sentences = ['In 2019, Citi financed $74 million of subordinate lien bonds that were certified green, given the projects environmental aspects.', \n",
    "             'In addition, our cogeneration plant, fueled by natural gas, will produce heat and electricity on-site, reducing the building\\'s carbon footprint by 34 percent.',\n",
    "             'These efforts reduced energy consumption by more than 2,100 metric tons (mt) of carbon dioxide equivalents (CO2e) during the one-year challenge.',\n",
    "             'The companies in our equity portfolio emitted around 133 tonnes of CO2 -equivalents for every million US dollars of revenue.', \n",
    "             'The equity portfolio’s carbon intensity was 9 percent below that of the benchmark index.',\n",
    "             'A total of 106 companies that produce certain types of weapon, tobacco or coal, or use coal for power production, are currently excluded from the fund', \n",
    "             'For public and private assets, excluding cash and non-equity derivatives as they were not reported in 2019, our year-overyear portfolio weighted average carbon intensity was reduced by approximately 23%.',\n",
    "             'Having met these targets, we have set new, more ambitious ones: to reduce the Fund’s emissions intensity by 40% and fossil fuel reserves by 80% by 2025.',\n",
    "             'The carbon footprint of the non-listed companies was 0.6 tCO₂e per million SEK invested',\n",
    "             'Energy consumption and carbon emissions per unit area were 149 kWh/ m² and 0.037 tCO₂e/m², which means a reduction of 9 per cent and 12 per cent, respectively.',\n",
    "             'The carbon intensity (CO2 equivalent tons per million yen of sales) of GPIF’s equity and corporate bond portfolio decreased by 15.3%, from 2.29 tons to 1.94 tons, in the space of a year.'\n",
    "             'Based on our percentage holdings in each company, the total emissions of the equity portfolio were 108 million tonnes of CO2 - equivalents in 2019.',\n",
    "             'The carbon footprint of the companies in our equity portfolio',\n",
    "             'The companies in our equity portfolio emitted around 156 tonnes of CO2 -equivalents for every million US dollars (USD) of revenue.'\n",
    "             'The carbon intensity of the companies in the equity portfolio and the benchmark index decreased by 16 and 17 percent respectively from 2018 to 2019.',\n",
    "             'We are focused on supporting the goal of net zero greenhouse gas emissions by 2050, in line with global efforts to limit warming to 1.5°C. ',\n",
    "             'quantitative target for ESG-themed investments and finance of ¥700 billion ',\n",
    "             'Commit to reduce investment carbon footprint by',\n",
    "             'esg investing', 'green bonds', 'Green Investment target', 'Achieve 100% renewable electricity by 2025'\n",
    "             ]\n",
    "relevant_sentences_embeddings = bc.encode(relevant_sentences)\n",
    "\n",
    "print(relevant_sentences_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(s1,s2):\n",
    "    return 1 - spatial.distance.cosine(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_relevant_sentences = []\n",
    "for json_name,json in bert_json.items():\n",
    "    fi_list = []\n",
    "    for fi_index in range(len(json))[:10]: #change\n",
    "        fi = json[fi_index]\n",
    "        page_relevant_sentences = {}\n",
    "        page_relevant_sentences_original = {}\n",
    "        for page_number, page in fi[\"filtered_report_pages_direct_bert\"].items():\n",
    "            relevant_sentences = []\n",
    "            relevant_sentences_original = []\n",
    "            for sentence_index in range(len(page)):\n",
    "                original_sentence = all_json[json_name][fi_index][\"report_sentences\"][page_number-1][sentence_index]\n",
    "                all_filtered_sentences.append(original_sentence)\n",
    "                sentence = page[sentence_index]\n",
    "                sentence_encoding = bc.encode([sentence])[0]\n",
    "                for relevant_sentence in relevant_sentences_embeddings:\n",
    "                    cosine_similarity = cosine_distance(sentence_encoding,relevant_sentence)\n",
    "                    all_cosine_similarities.append(cosine_similarity)\n",
    "                    if cosine_similarity >= 0.8 : # tentative\n",
    "                        relevant_sentences.append([sentence,cosine_similarity])\n",
    "                        relevant_sentences_original.append(original_sentence)\n",
    "                        all_relevant_sentences.append(original_sentence)\n",
    "                        break\n",
    "            if len(relevant_sentences) != 0.7357 :\n",
    "                page_relevant_sentences[page_number] = relevant_sentences\n",
    "                page_relevant_sentences_original[page_number] = relevant_sentences_original\n",
    "        bert_json[json_name][fi_index][\"bert_relevant_sentences_direct\"] = page_relevant_sentences\n",
    "        bert_json[json_name][fi_index][\"bert_relevant_sentences_direct_original\"] = page_relevant_sentences_original\n",
    "\n",
    "\n",
    "        page_relevant_sentences_indirect = {}\n",
    "        page_relevant_sentences_indirect_original = {}\n",
    "        for page_number, page in fi[\"filtered_report_pages_indirect_bert\"].items():\n",
    "            relevant_sentences = []\n",
    "            relevant_sentences_original = []\n",
    "            for sentence_index in range(len(page)):\n",
    "                original_sentence = all_json[json_name][fi_index][\"report_sentences\"][page_number-1][sentence_index]\n",
    "                all_filtered_sentences.append(original_sentence)\n",
    "                sentence = page[sentence_index]\n",
    "                sentence_encoding = bc.encode([sentence])[0]\n",
    "                for relevant_sentence in relevant_sentences_embeddings:\n",
    "                    cosine_similarity = cosine_distance(sentence_encoding,relevant_sentence)\n",
    "                    all_cosine_similarities.append(cosine_similarity)\n",
    "                    if cosine_similarity >= 0.7357 : # tentative maybe must b smilar to most terms?\n",
    "                        relevant_sentences.append([sentence,cosine_similarity])\n",
    "                        relevant_sentences_original.append(original_sentence)\n",
    "                        all_relevant_sentences.append(original_sentence)\n",
    "                        break\n",
    "            if len(relevant_sentences) != 0 :\n",
    "                page_relevant_sentences_indirect[page_number] = relevant_sentences\n",
    "                page_relevant_sentences_indirect_original[page_number] = relevant_sentences_original\n",
    "        bert_json[json_name][fi_index][\"bert_relevant_sentences_indirect\"] = page_relevant_sentences_indirect\n",
    "        bert_json[json_name][fi_index][\"bert_relevant_sentences_indirect_original\"] = page_relevant_sentences_indirect_original\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to determine threshold\n",
    "sns.histplot(all_cosine_similarities)\n",
    "pd.DataFrame(all_cosine_similarities,columns=[\"cos_similarity\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_filtered_sentences))\n",
    "print(len(all_relevant_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data for labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_filtered_sentences_df = pd.DataFrame(all_filtered_sentences,columns=[\"filtered_sentences\"])\n",
    "# n = all_filtered_sentences_df.shape[0]//5\n",
    "# split_1 = all_filtered_sentences_df.sample(n=n,random_state=200) #random state is a seed value\n",
    "# test = all_filtered_sentences_df.drop(split_1.index)\n",
    "# split_2 = test.sample(n=n,random_state=200)\n",
    "# test = test.drop(split_2.index)\n",
    "# split_3 = test.sample(n=n,random_state=200) \n",
    "# test = test.drop(split_3.index)\n",
    "# split_4 = test.sample(n=n,random_state=200) \n",
    "# test = test.drop(split_4.index)\n",
    "# split_5 = test\n",
    " \n",
    "data = \"../data/to_label\"\n",
    "all_relevant_sentences_df = pd.DataFrame(all_relevant_sentences,columns=[\"relevant_sentences\"])\n",
    "n = all_relevant_sentences_df.shape[0]//5\n",
    "split_1 = all_relevant_sentences_df.sample(n=n,random_state=200)\n",
    "split_1.to_csv(data + \"/split_1.csv\")\n",
    "test = all_relevant_sentences_df.drop(split_1.index)\n",
    "print(test.shape)\n",
    "split_2 = test.sample(n=n,random_state=200)\n",
    "split_2.to_csv(data + \"/split_2.csv\")\n",
    "test = test.drop(split_2.index)\n",
    "print(test.shape)\n",
    "split_3 = test.sample(n=n,random_state=200)\n",
    "split_3.to_csv(data + \"/split_3.csv\")\n",
    "test = test.drop(split_3.index)\n",
    "split_4 = test.sample(n=n,random_state=200)\n",
    "split_4.to_csv(data + \"/split_4.csv\")\n",
    "test = test.drop(split_4.index)\n",
    "split_5 = test\n",
    "split_5.to_csv(data + \"/split_5.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
