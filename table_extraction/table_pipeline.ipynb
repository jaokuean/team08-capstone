{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ce5989a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mpixel_mean\u001b[0m\n",
      "  \u001b[35mpixel_std\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PyPDF2 import PdfFileReader\n",
    "import requests, io\n",
    "import gzip\n",
    "import requests\n",
    "import pdf2image\n",
    "import cv2\n",
    "import camelot\n",
    "import pandas as pd\n",
    "\n",
    "# import Multi-Type-TD-TSR\n",
    "import torch, torchvision\n",
    "import pytesseract\n",
    "import detectron2\n",
    "import Multi_Type_TD_TSR.google_colab.deskew as deskew\n",
    "import Multi_Type_TD_TSR.google_colab.table_detection as table_detection\n",
    "import Multi_Type_TD_TSR.google_colab.table_xml as txml\n",
    "import Multi_Type_TD_TSR.google_colab.table_ocr as tocr\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import itertools\n",
    "import random\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "# import common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "setup_logger()\n",
    "\n",
    "#create detectron config\n",
    "cfg = get_cfg()\n",
    "\n",
    "#set yaml\n",
    "cfg.merge_from_file('All_X152.yaml')\n",
    "\n",
    "#set model weights\n",
    "cfg.MODEL.WEIGHTS = 'model_final.pth' # Set path model .pth\n",
    "\n",
    "predictor = DefaultPredictor(cfg) \n",
    "\n",
    "# list of keywords\n",
    "ESG_DICTIONARY = ['ghg', 'scope 1', \n",
    "                'scope 2', 'scope 3', 'energy', \n",
    "                'paper', 'green bonds', 'renewable energy',\n",
    "                'water', 'carbon intensity', \n",
    "                'carbon emissions', 'waste', \n",
    "                'electricity', \n",
    "                'weighted average carbon intensity', 'WACI']\n",
    "\n",
    "\n",
    "# list of units\n",
    "ESG_UNITS = ['tonnes', 'tons', 'kWh', 'kilogram', 'kilowatt hour', \n",
    "           'gigajoules', 'GJ', 'litre', 'liter', 'CO2e', 'tCO', 't CO', 'MWh', \n",
    "           'megawatt hour', 'GWh', 'gigawatt hour',\n",
    "           'cubic metres', 'cm3', 'm3', 'per employee', 'ream', 'quire', 'sheet', 'bundle', 'bale']\n",
    "\n",
    "def isoverlap(r1, r2):\n",
    "    \"\"\"\n",
    "    Check if two extracted table images have overlapping coordinates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    r1 : list of int\n",
    "        [x, y, w, h] coordinates of 1st rectangle.\n",
    "    r2 : list of int\n",
    "        [x, y, w, h] coordinates of 2nd rectangle.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    bool\n",
    "        True if the coordinates overlap else False.\n",
    "    \"\"\"\n",
    "    y1 = r1[1]\n",
    "    x1 = r1[0]\n",
    "    h1 = r1[3]\n",
    "    w1 = r1[2]\n",
    "    \n",
    "    y2 = r2[1]\n",
    "    x2 = r2[0]\n",
    "    h2 = r2[3]\n",
    "    w2 = r2[2]\n",
    "    \n",
    "    if ((x1+w1)<x2 or (x2+w2)<x1 or (y1+h1)<y2 or (y2+h2)<y1):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def check_coord_overlap(table_coords):\n",
    "    \"\"\"\n",
    "    Returns new list of non-overlapping table coordinates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table_coords : list of list of int\n",
    "        List of [x, y, w, h] coordinates of rectangles (rectangular boundaries of table images).\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    new_table_coords : list of list of int\n",
    "        List of [x, y, w, h] coordinates of non-overlapping rectanglular table images.\n",
    "    \"\"\"\n",
    "    new_table_coords = []\n",
    "    \n",
    "    for i in range (len(table_coords)):\n",
    "        flag = True\n",
    "        if i == 0:\n",
    "            new_table_coords.append(table_coords[i])\n",
    "        else:\n",
    "            for j in range (len(new_table_coords)):\n",
    "                if isoverlap(table_coords[i], new_table_coords[j]):\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag:\n",
    "                new_table_coords.append(table_coords[i])\n",
    "    \n",
    "    return new_table_coords\n",
    "\n",
    "def get_pdf_dimension(pdf_url):\n",
    "    \"\"\"\n",
    "    Retrieve dimensions of PDF page.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pdf_url : str\n",
    "        URL of PDF report.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    height : float\n",
    "        Height of PDF page.\n",
    "    width : float\n",
    "        Width of PDF page.\n",
    "    \"\"\"\n",
    "    response = requests.get(pdf_url)\n",
    "    with io.BytesIO(response.content) as open_pdf_file:\n",
    "        pdf = PdfFileReader(open_pdf_file, strict=False)\n",
    "        height = pdf.getPage(0).mediaBox.getHeight()\n",
    "        width = pdf.getPage(0).mediaBox.getWidth()\n",
    "    return height, width\n",
    "\n",
    "def get_image_and_dimension(pdf_url):\n",
    "    \"\"\"\n",
    "    Convert PDF to PIL image and get dimensions of image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pdf_url : str\n",
    "        URL of PDF report.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    height : float\n",
    "        Height of PDF page.\n",
    "    width : float\n",
    "        Width of PDF page.\n",
    "    images : list of objects\n",
    "        List of PIL images converted from all pages in PDF report.\n",
    "    \"\"\"\n",
    "    response = requests.get(pdf_url, stream=True, timeout=30)\n",
    "    # pdf = gzip.open(response.raw)\n",
    "    # images = pdf2image.convert_from_bytes(pdf.read())\n",
    "    images = pdf2image.convert_from_bytes(response.content, size=1000)\n",
    "    pg_1_img = images[0] # type=PIL.PpmImagePlugin.PpmImageFile\n",
    "    width, height = pg_1_img.size\n",
    "    return height, width, images\n",
    "\n",
    "def get_scaling_factor(pdf_height, pdf_width, img_height, img_width):\n",
    "    \"\"\"\n",
    "    Get scaling factor of extracted table images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pdf_height : float\n",
    "        Height of PDF report.\n",
    "    pdf_width : float\n",
    "        Width of PDF report.\n",
    "    img_height : float\n",
    "        Height of extracted image.\n",
    "    img_width : float\n",
    "        Width of extracted image.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    scaling_factor_height : float\n",
    "        Scaling factor for height.\n",
    "    scaling_factor_width : float\n",
    "        Scaling factor for width.\n",
    "    \"\"\"\n",
    "    scaling_factor_height = img_height/pdf_height\n",
    "    scaling_factor_width = img_width/pdf_width\n",
    "    return scaling_factor_height, scaling_factor_width\n",
    "\n",
    "def convert_PIL_cv2(pdf_pil_img):\n",
    "    \"\"\"\n",
    "    Convert PIL images to CV images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pdf_pil_img : list of objects\n",
    "        List of PIL images.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    pdf_cv2_img : list of objects\n",
    "        List of CV images.\n",
    "    \"\"\"    \n",
    "    pdf_cv2_img = []\n",
    "    for pil_img in pdf_pil_img:\n",
    "        cv2_img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "        pdf_cv2_img.append(cv2_img)\n",
    "    return pdf_cv2_img\n",
    "\n",
    "def isfloat(value):\n",
    "    \"\"\"\n",
    "    Check if value is numerical.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    value : any\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    bool\n",
    "        True if value is of type float else False.\n",
    "    \"\"\"    \n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def num_there(s):\n",
    "    \"\"\"\n",
    "    Check if any character is a digit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s : str\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    bool\n",
    "        True if character is digit else False.\n",
    "    \"\"\"  \n",
    "    return any(i.isdigit() for i in s)\n",
    "\n",
    "# list of units to be used for filtering - note: this is different from the ones under Table Detection section! \n",
    "UNITS = ['tonnes', 'tons', 'kWh', 'kilogram', 'kilowatt hour', \n",
    "           'gigajoules', 'GJ', 'litre', 'liter', 'CO2e', 'tCO', 't CO', 'MWh', \n",
    "           'megawatt hour', 'GWh', 'gigawatt hour',\n",
    "           'cubic metres', 'cm3', 'm3', 'per employee', 'ream', 'quire', 'sheet', 'bundle', 'bale', '%', 't']\n",
    "\n",
    "def clean_tbl(tbl):\n",
    "    \"\"\"\n",
    "    Perform cleaning of dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tbl : df\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    tbl : df or None\n",
    "        Cleaned df if df can be cleaned else None.\n",
    "    final_words : list of str or None\n",
    "        List of ESG keywords if keyword found in dataframe else None.\n",
    "    \"\"\"  \n",
    "    # replace subscript and newline \n",
    "    tbl = tbl.replace(r'(<s>).*(</s>)','',regex=True)\n",
    "    tbl = tbl.replace(r'\\n','',regex=True)\n",
    "    # convert 1st row to header \n",
    "    header_df = tbl.iloc[0] #grab the first row for the header\n",
    "    tbl = tbl[1:]\n",
    "    tbl.columns = header_df \n",
    "    # remove comma in numeric values \n",
    "    tbl = tbl.apply(lambda x: x.str.replace(',',''))\n",
    "    # remove brackets surrounding numeric metrics \n",
    "    tbl = tbl.replace(r\"\\((\\d+)\\)\", r\"\\1\", regex=True)\n",
    "    # loop through each cell and check if they are float/num or they are metrics with units \n",
    "    for row in range(tbl.shape[0]):\n",
    "        for col in range(1, tbl.shape[1]):\n",
    "            value = tbl.iloc[row, col]\n",
    "            if len(value.split()) > 3:\n",
    "                tbl.iloc[row,col] = np.nan\n",
    "            elif isfloat(value) or (any(substring in value for substring in UNITS) and num_there(value)):\n",
    "                continue \n",
    "            else:\n",
    "                tbl.iloc[row,col] = np.nan\n",
    "    # drop columns with > 80% NaN\n",
    "    tbl = tbl.loc[:, tbl.isnull().mean() < .8]\n",
    "    # drop rows with any NaN\n",
    "    tbl = tbl.dropna()\n",
    "    if (tbl.shape[1] == 1) or (tbl.shape[0] == 0): # if there's only 1 col left or 0 row left \n",
    "        return None, None \n",
    "    page_kw = ['page', 'Page', 'PAGE']\n",
    "    for s in page_kw:\n",
    "        if any(s in h for h in tbl.columns):\n",
    "            return None, None \n",
    "    first_column = tbl.iloc[:, 0] # get first column of tbl \n",
    "    num_of_nan = first_column.isnull().sum(axis = 0)\n",
    "    # large proportion of nan cells in 1st column\n",
    "    if num_of_nan/len(first_column) > 0.8:\n",
    "        return None, None\n",
    "    # no headers \n",
    "    headers =tbl.columns\n",
    "    if not(any(h for h in headers)):\n",
    "        return None, None \n",
    "    # list of words in df for relevance \n",
    "    words = pd.unique(tbl.values.ravel())\n",
    "    words = pd.unique([word for line in words for word in line.split()])\n",
    "    final_words = []\n",
    "    for s in ESG_DICTIONARY:\n",
    "        if any(s in word.lower() for word in words):\n",
    "            final_words.append(s) \n",
    "    for s in ESG_DICTIONARY:\n",
    "        if any(s in word.lower() for word in tbl.columns):\n",
    "            final_words.append(s)\n",
    "    final_words = list(set(final_words))\n",
    "    return tbl, final_words \n",
    "\n",
    "def extract_tbl_from_page(page_idx, pdf_cv2_img, scaling_factor_height, scaling_factor_width, img_height, url, path):\n",
    "    \"\"\"\n",
    "    Extract text-based and image-based table from individual page of PDF report.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    page_idx : int\n",
    "        Page index of PDF report\n",
    "    pdf_cv2_img : list of objects\n",
    "        List of CV images converted from PDF pages.\n",
    "    scaling_factor_height : float\n",
    "        Scaling factor for height.\n",
    "    scaling_factor_width : float\n",
    "        Scaling factor for width.\n",
    "    img_height : float\n",
    "        Original image height.\n",
    "    url : str\n",
    "        URL of PDF report.\n",
    "    path : str\n",
    "        Path directory where extracted table images are saved.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    tbl_lst : list of df\n",
    "        List of cleaned dataframes.\n",
    "    tbl_keywords_lst : list of list of str\n",
    "        List of keywords lists identified from each extracted dataframe in a PDF page.\n",
    "    img_keywords_lst : list of list of str\n",
    "        List of keywords lists identified from each extracted table image in a PDF page.\n",
    "    lst_imgs: list of str\n",
    "        List of image paths for every detected table image in a PDF page.\n",
    "    \"\"\"  \n",
    "    table_list, table_coords = table_detection.make_prediction(pdf_cv2_img[page_idx], predictor)\n",
    "    tbl_lst = []\n",
    "    tbl_keywords_lst = []\n",
    "    img_keywords_lst = []\n",
    "    lst_imgs = []\n",
    "    \n",
    "    table_coords = check_coord_overlap(table_coords)\n",
    "    \n",
    "    count = -1\n",
    "    # ---- start Table extraction ---- /recount from every page \n",
    "    i = 0 \n",
    "    # ---- end Table extraction ----\n",
    "    for table_coord in table_coords:\n",
    "        count += 1\n",
    "        try: \n",
    "            x1 = table_coord[0]\n",
    "            y1 = table_coord[1]\n",
    "            x2 = table_coord[2] + table_coord[0]\n",
    "            y2 = table_coord[3] + table_coord[1]\n",
    "            \n",
    "            # extract detected table by coordinates\n",
    "            page_img = pdf_cv2_img[page_idx]\n",
    "            page_img = page_img[y1:y2, x1:x2]\n",
    "            scale_percent = 150 # percent of original size\n",
    "            width = int(page_img.shape[1] * scale_percent / 100)\n",
    "            height = int(page_img.shape[0] * scale_percent / 100)\n",
    "            dim = (width, height)\n",
    "            # resize image\n",
    "            resized = cv2.resize(page_img, dim)\n",
    "            #cv2_imshow(resized)\n",
    "            text = pytesseract.image_to_string(resized)\n",
    "            \n",
    "            # if any keyword found in text in table, save image into output folder\n",
    "            if any(keyword in text.lower() for keyword in ESG_DICTIONARY) and any(unit in text for unit in ESG_UNITS):\n",
    "                print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "                print(\"RELEVANT IMAGE FOUND!, tbl_num is {0}\".format(count))\n",
    "                print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "                \n",
    "                # cv2 and pdf used 2 different coordinate systems, thus y1 and y2 need to be modified\n",
    "                y1 = img_height - y1\n",
    "                y2 = img_height - y2\n",
    "                \n",
    "                # scale by scaling factors\n",
    "                scaled_x1 = x1/scaling_factor_width\n",
    "                scaled_x2 = x2/scaling_factor_width\n",
    "                scaled_y1 = y1/scaling_factor_height\n",
    "                scaled_y2 = y2/scaling_factor_height\n",
    "                edge_tol = scaled_x2 - scaled_x1\n",
    "                coords = f\"{scaled_x1}, {scaled_y1}, {scaled_x2}, {scaled_y2}\"\n",
    "                \n",
    "                tbls_scaled = camelot.read_pdf(url, flavor='stream', edge_tol=edge_tol, pages=str(page_idx+1), flag_size=True, table_areas=[coords], split_text=True) # split_text=True\n",
    "                tbl_cleaned, ESG_keywords = clean_tbl(tbls_scaled[0].df)\n",
    "                # there is a valid table \n",
    "                \n",
    "                if tbl_cleaned is None:\n",
    "                    continue\n",
    "                else:\n",
    "                    if len(ESG_keywords) > 0:\n",
    "                        print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "                        print(\"CLEANED TABLE OBTAINED!, tbl_num is {0}\".format(count))\n",
    "                        print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "                        tbl_lst.append(tbl_cleaned)\n",
    "                        tbl_keywords_lst.append(ESG_keywords)\n",
    "                        img_keywords_lst.append(ESG_keywords)\n",
    "                        # Table detection portion saving \n",
    "                        # --- start ---\n",
    "                        output_path = f\"{path}/PAGE{str(page_idx+1)}_IMAGE{str(i)}.jpg\"\n",
    "                        cv2.imwrite(output_path, page_img)\n",
    "                        i += 1\n",
    "                        lst_imgs.append(output_path)\n",
    "                        # --- end ---\n",
    "                    else:\n",
    "                        print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "                        print(\"SAVING ONLY IMAGE!, img_num is {0}\".format(count))\n",
    "                        print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "                        # --- start ---\n",
    "                        output_path = f\"{path}/PAGE{str(page_idx+1)}_IMAGE{str(i)}.jpg\"\n",
    "                        cv2.imwrite(output_path, page_img)\n",
    "                        i += 1\n",
    "                        lst_imgs.append(output_path)\n",
    "                        # --- end ---\n",
    "                        kw_lst = []\n",
    "                        for t in ESG_DICTIONARY:\n",
    "                            if t in text.lower():\n",
    "                                kw_lst.append(t)\n",
    "                        img_keywords_lst.append(kw_lst)\n",
    "                        \n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('error on page {0}'.format(page_idx+1))\n",
    "            continue \n",
    "    \n",
    "    return tbl_lst, tbl_keywords_lst, img_keywords_lst, lst_imgs\n",
    "\n",
    "def extract_tbl_from_pdf(pdf_url, pages_to_look_for, path):\n",
    "    \"\"\"\n",
    "    Create dictionaries in the form of {page no.: output}.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pdf_url : str\n",
    "        URL of PDF report.\n",
    "    pages_to_look_for : list of str\n",
    "        List of relevant page numbers of PDF report.\n",
    "    path : str\n",
    "        Path directory where extracted table images are saved.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    pdf_dict : dict of {str : list of df}\n",
    "        Dictionary of {page: list of cleaned dataframes}.\n",
    "    tbl_keywords_dict : dict of {str : list of list}\n",
    "        Dictionary of {page: list of keywords lists identified from each extracted dataframe}.\n",
    "    img_keywords_dict : dict of {str : list of list}\n",
    "        Dictionary of {page: list of keywords lists identified from each extracted table image}.\n",
    "    image_path_obj: dict of {str : list of str}\n",
    "        Dictionary of {page: list of image paths for every detected table image}.\n",
    "    \"\"\"  \n",
    "    pdf_dict = {}\n",
    "    tbl_keywords_dict = {}\n",
    "    img_keywords_dict = {}\n",
    "    image_path_obj = {}\n",
    "    \n",
    "    # consistent to all pages in a pdf_url \n",
    "    print('getting pdf & img height, width...')\n",
    "    pdf_height, pdf_width = get_pdf_dimension(pdf_url) # original pdf height,width \n",
    "    img_height, img_width, pdf_pil_img = get_image_and_dimension(pdf_url) # original image height,width \n",
    "    scaling_factor_height, scaling_factor_width = get_scaling_factor(pdf_height, pdf_width, img_height, img_width)\n",
    "    print('retrieved pdf & img height, width')\n",
    "    \n",
    "    # convert PIL images to CV images\n",
    "    print('converting all pages to CV2 img...')\n",
    "    pdf_cv2_img = convert_PIL_cv2(pdf_pil_img)\n",
    "    print ('converted all pages to CV2 img')\n",
    "    \n",
    "    # pdf_cv2_img = images of all pages in a pdf \n",
    "    print('looping through each page...')\n",
    "    for page_no in pages_to_look_for:\n",
    "        print('retrieving from page {0}'.format(page_no))\n",
    "        page_idx = int(page_no) - 1\n",
    "        tbl_lst, tbl_keywords_lst, img_keywords_lst, img_lst= extract_tbl_from_page(page_idx, pdf_cv2_img, scaling_factor_height, scaling_factor_width, img_height, pdf_url, path)\n",
    "        pdf_dict[page_no] = tbl_lst\n",
    "        tbl_keywords_dict[page_no] = tbl_keywords_lst\n",
    "        image_path_obj[page_no] = img_lst\n",
    "        img_keywords_dict[page_no] = img_keywords_lst\n",
    "        \n",
    "    \n",
    "    return pdf_dict, tbl_keywords_dict, img_keywords_dict, image_path_obj\n",
    "\n",
    "def table_pipeline(report):\n",
    "    \"\"\"\n",
    "    Main table pipeline function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    report : dict of {str : str or dict}\n",
    "        Dictionary of a company's report details and preprocessed text.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    pickle : dict of {str : str or dict}\n",
    "        Dictionary containing a company's name, year, PDF URL and cleaned dataframes.\n",
    "    report : dict of {str : str or dict}\n",
    "        Dictionary containing a company's report details, preprocessed text and table pipeline output.\n",
    "    \"\"\"          \n",
    "    # basic information\n",
    "    company = report['company']\n",
    "    year = report['year']\n",
    "    pdf_url = report['url']\n",
    "\n",
    "    # create dictionary for report \n",
    "    pickle = {}\n",
    "    pickle['company'] = company\n",
    "    pickle['year'] = year\n",
    "    pickle['url'] = pdf_url\n",
    "\n",
    "    path = 'data/dashboard_data/table_images/' + company + '_' + year\n",
    "    os.mkdir(path)\n",
    "    print(f\"Detection for Report: {company}_{year}\")\n",
    "\n",
    "    # relevant pages for ESG info extraction\n",
    "    pages_to_look_for = []\n",
    "    for page in report['filtered_report_tables_direct']:\n",
    "        pages_to_look_for.append(page)\n",
    "\n",
    "    for page in report['filtered_report_tables_indirect']:\n",
    "        pages_to_look_for.append(page)\n",
    "\n",
    "    try:\n",
    "        print('calling extract_tbl_from_pdf')\n",
    "        pickle['tbl_pages'], report['table_keywords'], report['table_image_keywords'], report['table_images']= extract_tbl_from_pdf(pdf_url, pages_to_look_for, path) # returns a dict  \n",
    "        print('Successful for {0}, {1}'.format(company, year))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error occurred in table_extraction/ Request failed\")\n",
    "        pickle['tbl_pages'] = []\n",
    "        report['table_keywords'] = 'nan'\n",
    "        report['table_image_keywords'] = 'nan'\n",
    "        report['table_images'] = 'nan'\n",
    "\n",
    "    return pickle, report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
