{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping articles\n",
    "\n",
    "Resources : \n",
    "1. https://medium.com/analytics-vidhya/googlenews-api-live-news-from-google-news-using-python-b50272f0a8f0\n",
    "2. http://theautomatic.net/2020/08/05/how-to-scrape-news-articles-with-python/\n",
    "\n",
    "Note : Google may recognize the program as automated robots and block the IP, using cloud server and fetching data with high frequency will get higher chance to be blocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/xinminaw/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from GoogleNews import GoogleNews\n",
    "from newspaper import Article\n",
    "from newspaper import Config\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "#config will allow us to access the specified url for which we are #not authorized. Sometimes we may get 403 client error while parsing #the link to download the article.\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'\n",
    "config = Config()\n",
    "config.browser_user_agent = user_agent\n",
    "googlenews=GoogleNews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_google_news(searchterm):\n",
    "    \n",
    "    googlenews.search(searchterm) \n",
    "    all_articles = pd.DataFrame()\n",
    "    articles_count = 0\n",
    "\n",
    "    # lopp through all pages\n",
    "    for i in range(1,100):\n",
    "        print(f\"Page {i} of Google Search\")\n",
    "        # allow for time interval between multiple scraping\n",
    "        for j in range(3):\n",
    "            try:\n",
    "                random_sleep_link = random.uniform(5, 10) \n",
    "                time.sleep(random_sleep_link)\n",
    "                googlenews.getpage(i)  # each page is 1 request\n",
    "            except:\n",
    "                random_sleep_except = random.uniform(240,360)\n",
    "                print(\"Encountered an error! I'll pause for\"+str(random_sleep_except/60) + \" minutes and try again \\n\")\n",
    "                time.sleep(random_sleep_except) #sleep the script for x seconds and....#\n",
    "                continue #...start the loop again from the beginning#\n",
    "            else:\n",
    "                break\n",
    "\n",
    "\n",
    "        result = googlenews.result()\n",
    "        df = pd.DataFrame(result)\n",
    "\n",
    "        list=[]\n",
    "\n",
    "        for ind in df.index:\n",
    "            try: \n",
    "                dict={}\n",
    "                article = Article(df['link'][ind],config=config)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article.nlp()\n",
    "                dict['Date']=df['date'][ind]\n",
    "                dict['Media']=df['media'][ind]\n",
    "                dict['Title']=article.title\n",
    "                dict['Article']=article.text\n",
    "                dict['Summary']=article.summary\n",
    "                list.append(dict)\n",
    "                \n",
    "                articles_count += 1\n",
    "                print(f\"Total Articles so far = {articles_count}\")\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        news_df =pd.DataFrame(list)\n",
    "        all_articles =  all_articles.append(news_df,ignore_index=True)\n",
    "    \n",
    "    print(f\"Total number of scraped articles for {searchterm} = {all_articles.shape[0]}\")\n",
    "    all_articles.to_excel(f\"../data/articles/{searchterm}_articles.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the below cell\n",
    "replace \"index\" with the numbers 0:jeramine, 1:sean, 2:aifen, 3:jk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Search Term : Microsoft carbon-negative\n",
      "Page 1 of Google Search\n",
      "Total Articles so far = 1\n",
      "Total Articles so far = 2\n",
      "Total Articles so far = 3\n",
      "Total Articles so far = 4\n",
      "Total Articles so far = 5\n",
      "Total Articles so far = 6\n",
      "Total Articles so far = 7\n",
      "Total Articles so far = 8\n",
      "Total Articles so far = 9\n",
      "Total Articles so far = 10\n",
      "Total Articles so far = 11\n",
      "Total Articles so far = 12\n",
      "Total Articles so far = 13\n",
      "Total Articles so far = 14\n",
      "Total Articles so far = 15\n",
      "Total Articles so far = 16\n",
      "Total Articles so far = 17\n",
      "Total Articles so far = 18\n",
      "Total number of scraped articles for Microsoft carbon-negative = 18\n"
     ]
    }
   ],
   "source": [
    "decarbonisation_terms = [\"decarbonisation\",\"carbon-negative\",\"net zero\",\"zero carbon\", \"reduce greenhouse gas\",\"reduce carbon footprint\"]\n",
    "companies = [\"Coca-Cola\",\"Home Depot\",\"Paypal\",\"NextEra Energy\",\"Microsoft\"]\n",
    "all_articles = pd.DataFrame()\n",
    "\n",
    "for term in decarbonisation_terms[1:2]:\n",
    "    search_term = companies[index] + \" \" + term  # replace index here with corresponding numbers\n",
    "    print(f\"Current Search Term : {search_term}\")\n",
    "    articles = scrape_google_news(search_term) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
