{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from preprocessing import * \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../data/all_labelled_17Oct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns = ['index', 'sentence', 'relevance', 'carbon_class']\n",
    "df1['cleaned_sentence'] = df1['sentence'].apply(clean_sentence)\n",
    "df1 = df1[df1['carbon_class'].notnull()]\n",
    "df1 = df1.astype({'carbon_class':int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = \\\n",
    "              np.split(df1.sample(frac=1, random_state=4103), \n",
    "                       [int(.6*len(df1)), int(.8*len(df1))])\n",
    "trainval =pd.concat([train, val])# Split labelled set\n",
    "labels = [train.carbon_class, val.carbon_class, test.carbon_class, trainval.carbon_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Best Models & Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_MODEL = LogisticRegression\n",
    "BEST_LR = list(ParameterGrid({'C':[0.1], 'class_weight':['balanced'], 'penalty':['l2'], 'solver': ['lbfgs']}))\n",
    "LR_VECT = TfidfVectorizer\n",
    "BEST_LR_VECT = list(ParameterGrid({'analyzer':['word'], 'lowercase':[True], 'max_df':[0.25], 'min_df':[1], 'ngram_range':[(1,1)]}))\n",
    "BEST_LR_SENTENCE = 'cleaned_sentence'\n",
    "LR = [LR_MODEL, BEST_LR, LR_VECT, BEST_LR_VECT, BEST_LR_SENTENCE]\n",
    "\n",
    "CB_MODEL = CatBoostClassifier\n",
    "BEST_CB = list(ParameterGrid({'depth':[1], 'iterations':[50], 'learning_rate':[1], 'verbose':[False]}))\n",
    "CB_VECT = TfidfVectorizer\n",
    "BEST_CB_VECT = list(ParameterGrid({'analyzer':['word'], 'lowercase':[True], 'max_df':[0.25], 'min_df':[1], 'ngram_range':[(1,3)]}))\n",
    "BEST_CB_SENTENCE = 'cleaned_sentence'\n",
    "CB = [CB_MODEL, BEST_CB, CB_VECT, BEST_CB_VECT, BEST_CB_SENTENCE]\n",
    "\n",
    "SVM_MODEL = SVC\n",
    "BEST_SVM_HARD = list(ParameterGrid({'C':[0.1], 'class_weight':['balanced'], 'gamma':['scale'], 'kernel': ['sigmoid']}))\n",
    "BEST_SVM_SOFT = list(ParameterGrid({'C':[0.1], 'class_weight':['balanced'], 'gamma':['scale'], 'kernel': ['sigmoid'],'probability':[True]}))\n",
    "SVM_VECT = TfidfVectorizer\n",
    "BEST_SVM_VECT = list(ParameterGrid({'analyzer':['word'], 'lowercase':[True], 'max_df':[0.25], 'min_df':[1], 'ngram_range':[(1,1)]}))\n",
    "BEST_SVM_SENTENCE = 'sentence'\n",
    "SVM_HARD = [SVM_MODEL, BEST_SVM_HARD, SVM_VECT, BEST_SVM_VECT, BEST_SVM_SENTENCE]\n",
    "SVM_SOFT = [SVM_MODEL, BEST_SVM_SOFT, SVM_VECT, BEST_SVM_VECT, BEST_SVM_SENTENCE]\n",
    "\n",
    "NB_MODEL = MultinomialNB\n",
    "BEST_NB = list(ParameterGrid({'alpha':[1]}))\n",
    "NB_VECT = TfidfVectorizer\n",
    "BEST_NB_VECT = list(ParameterGrid({'analyzer':['word'], 'lowercase':[True], 'max_df':[0.25], 'min_df':[1], 'ngram_range':[(1,3)]}))\n",
    "BEST_NB_SENTENCE = 'cleaned_sentence'\n",
    "NB = [NB_MODEL, BEST_NB, NB_VECT, BEST_NB_VECT, BEST_NB_SENTENCE]\n",
    "\n",
    "RF_MODEL = RandomForestClassifier\n",
    "BEST_RF = list(ParameterGrid({'class_weight':['balanced'], 'criterion':['entropy'], \\\n",
    "                       'min_samples_leaf':[1], 'min_samples_split':[5], 'max_features':['log2']}))\n",
    "RF_VECT = CountVectorizer\n",
    "BEST_RF_VECT = list(ParameterGrid({'analyzer':['word'], 'lowercase':[True], 'max_df':[1], 'min_df':[1], 'ngram_range':[(1,1)]}))\n",
    "BEST_RF_SENTENCE = 'cleaned_sentence'\n",
    "RF = [RF_MODEL, BEST_RF, RF_VECT,BEST_RF_VECT, BEST_RF_SENTENCE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_hard = [LR, CB, SVM_HARD, NB, RF]\n",
    "ensemble_soft = [LR, CB, SVM_SOFT, NB, RF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_smote(X,y):\n",
    "    smote = SMOTE(random_state = 4103)\n",
    "    X, y = smote.fit_resample(X, y)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_helper(vect, sentence_version):\n",
    "    vec_train = vect.fit_transform(train[sentence_version])\n",
    "    vec_val = vect.transform(val[sentence_version])\n",
    "    vec_test = vect.transform(test[sentence_version])\n",
    "    vec_trainval = vect.transform(trainval[sentence_version])\n",
    "    \n",
    "    vec_train_oversampled = oversample_smote(vec_train, labels[0])\n",
    "    vec_trainval_oversampled = oversample_smote(vec_trainval, labels[3])\n",
    "    return vec_train_oversampled, vec_val, vec_test, vec_trainval_oversampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tfidf Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_zero = [\"emissions\",\"footprint\",\"ghg\", \"coal\"]\n",
    "class_one = [\"energy\",\"renewable\",\"electricity\",\"power\", \"solar\", \"kwh\"]\n",
    "class_two = [\"waste\",\"paper\", \"office\",\"recycled\",\"environmental\"]\n",
    "class_three = [\"sustainable\",\"investment\",\"investments\",\"bonds\", \"portfolio\", \"finance\"]\n",
    "\n",
    "# class_zero = [\"emissions\",\"footprint\",\"ghg\"]\n",
    "# class_one = [\"energy\",\"renewable\",\"electricity\",\"power\", \"solar\"]\n",
    "# class_two = [\"waste\",\"recycled\",\"environmental\"]\n",
    "# class_three = [\"susstainable\",\"investment\",\"investments\",\"bonds\",\"finance\"]\n",
    "\n",
    "def carbon_class_filter(row):\n",
    "    sentence = row[\"sentence\"]\n",
    "    if any(map(sentence.__contains__, class_zero)):\n",
    "        return 0\n",
    "    elif any(map(sentence.__contains__, class_one)):\n",
    "        return 1\n",
    "    elif any(map(sentence.__contains__, class_two)):\n",
    "        return 2\n",
    "    elif any(map(sentence.__contains__, class_three)):\n",
    "        return 3\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cc = df1[df1.relevance==1]\n",
    "df_all_cc['heu'] = df_all_cc.apply(carbon_class_filter, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classification_report(df_all_cc.carbon_class, df_all_cc.heu, output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions for val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['heu'] = test.apply(carbon_class_filter, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ensemble_pred_hard(ensemble):\n",
    "    df_test = []\n",
    "    for model_fn, model_param, vect_fn, vect_param, sentence_ver in ensemble:\n",
    "        for vp in vect_param:\n",
    "            vect = vect_fn(**vp)\n",
    "        train_os, val, test, trainval_os = vectorize_helper(vect, sentence_ver)\n",
    "        train_label = train_os[1]\n",
    "        val_label = labels[1]\n",
    "        test_label = labels[2]\n",
    "        trainval_label = trainval_os[1]\n",
    "        \n",
    "        # test pred\n",
    "        for mp in model_param:\n",
    "            model_tv = model_fn(**mp)\n",
    "        model_tv.fit(trainval_os[0], trainval_label)\n",
    "        test_pred = model_tv.predict(test)\n",
    "        df_test.append(test_pred)        \n",
    "    test_pred = pd.DataFrame(df_test).T  \n",
    "    test_pred.columns = ['lr', 'cb', 'svm', 'nb', 'rf']\n",
    "    test_pred['cb'] = test_pred['cb'].apply(lambda x: int(x[0]))\n",
    "    return test_pred  \n",
    "\n",
    "def get_majority_pred_hard(df):\n",
    "    final_pred = []\n",
    "    for j in df.iterrows():\n",
    "        lst = list(i for i in j[1])\n",
    "        pred = max(set(lst), key=lst.count)\n",
    "        final_pred.append(pred)\n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hard_df = generate_ensemble_pred_hard(ensemble_hard)\n",
    "hard_df['heu'] = list(test.heu) \n",
    "hard_pred = get_majority_pred_hard(hard_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classification_report(labels[2], hard_pred, output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ensemble_pred_soft(ensemble):\n",
    "    df_test = []\n",
    "    for model_fn, model_param, vect_fn, vect_param, sentence_ver in ensemble:\n",
    "        for vp in vect_param:\n",
    "            vect = vect_fn(**vp)\n",
    "        train_os, val, test, trainval_os = vectorize_helper(vect, sentence_ver)\n",
    "        train_label = train_os[1]\n",
    "        val_label = labels[1]\n",
    "        test_label = labels[2]\n",
    "        trainval_label = trainval_os[1]\n",
    "        \n",
    "        # test pred\n",
    "        for mp in model_param:\n",
    "            model_tv = model_fn(**mp)\n",
    "        model_tv.fit(trainval_os[0], trainval_label)\n",
    "        test_pred = model_tv.predict_proba(test)\n",
    "        df_test.append(test_pred)  \n",
    "    test_pred = pd.concat([pd.DataFrame(df_test[i]) for i in range(0,5)], axis=1)\n",
    "    cols = ['lr_0', 'lr_1', 'lr_2', 'lr_3', 'lr_4', \n",
    "            'cb_0', 'cb_1', 'cb_2', 'cb_3', 'cb_4',\n",
    "            'svm_0', 'svm_1', 'svm_2', 'svm_3', 'svm_4',\n",
    "            'nb_0', 'nb_1', 'nb_2', 'nb_3', 'nb_4',\n",
    "            'rf_0', 'rf_1', 'rf_2', 'rf_3', 'rf_4',\n",
    "           ]\n",
    "    test_pred.columns = cols\n",
    "    return test_pred\n",
    "\n",
    "def sum_probs(df, heu_preds):\n",
    "    df['0_total'] = df['lr_0'] + df['cb_0'] + df['svm_0'] + df['nb_0'] + df['rf_0']\n",
    "    df['1_total'] = df['lr_1'] + df['cb_1'] + df['svm_1'] + df['nb_1'] + df['rf_1']\n",
    "    df['2_total'] = df['lr_2'] + df['cb_2'] + df['svm_2'] + df['nb_2'] + df['rf_2']\n",
    "    df['3_total'] = df['lr_3'] + df['cb_3'] + df['svm_3'] + df['nb_3'] + df['rf_3']\n",
    "    df['4_total'] = df['lr_4'] + df['cb_4'] + df['svm_4'] + df['nb_4'] + df['rf_4']\n",
    "    probs = df[['0_total','1_total','2_total','3_total','4_total']]\n",
    "    for i in range(len(heu_preds)):\n",
    "        pred = heu_preds[i]\n",
    "        to_increase = '{pred}_total'.format(pred=pred)\n",
    "        probs.at[i,to_increase] += 1\n",
    "    return probs\n",
    "\n",
    "def get_majority_pred_soft(df):\n",
    "    final_pred = []\n",
    "    for i in df.iterrows():\n",
    "        lst = [j for j in i[1]]   \n",
    "        max_value = max(lst)\n",
    "        soft_pred = lst.index(max_value)\n",
    "        final_pred.append(soft_pred)\n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_df = generate_ensemble_pred_soft(ensemble_soft)\n",
    "soft_sum_probs = sum_probs(soft_df, list(test.heu))\n",
    "all_pred_soft = get_majority_pred_soft(soft_sum_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(labels[2], all_pred_soft, output_dict=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
