{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from preprocessing import * \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from bert_serving.client import BertClient\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1 = pd.read_csv('../data/labelled_data/all_labelled_17Oct.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def remove_punc(s):\n",
    "    import string\n",
    "    exclude = string.punctuation\n",
    "    final_punc = ''.join(list(i for i in exclude if i not in ['$', '&', '%']))\n",
    "    s = ''.join(ch for ch in s if ch not in list(final_punc))\n",
    "    return s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1.columns = ['index', 'sentence', 'relevance', 'carbon_class']\n",
    "df1['cleaned_sentence'] = df1['sentence'].apply(clean_sentence)\n",
    "df1['sentence_no_punc'] = df1['sentence'].map(remove_punc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train, val, test = \\\n",
    "              np.split(df1.sample(frac=1, random_state=4103), \n",
    "                       [int(.6*len(df1)), int(.8*len(df1))])\n",
    "trainval =pd.concat([train, val])# Split labelled set\n",
    "labels = [train.relevance, val.relevance, test.relevance, trainval.relevance]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialise Best Models & Vectorizers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "LR_MODEL = LogisticRegression\n",
    "BEST_LR = list(ParameterGrid({\n",
    "    \"C\": [1],\n",
    "    'class_weight': ['balanced'],\n",
    "    'penalty':['l2'],\n",
    "    'solver':['lbfgs']\n",
    "}))\n",
    "LR_VECT = TfidfVectorizer\n",
    "BEST_LR_VECT = list(ParameterGrid({\n",
    "    \"analyzer\": [\"word\"],\n",
    "    \"lowercase\": [True],\n",
    "    \"ngram_range\": [(1,1)],\n",
    "    \"max_df\": [0.25],\n",
    "    \"min_df\": [1]\n",
    "}))\n",
    "BEST_LR_SENTENCE = 'cleaned_sentence'\n",
    "LR = [LR_MODEL, BEST_LR, LR_VECT, BEST_LR_VECT, BEST_LR_SENTENCE]\n",
    "\n",
    "SVM_MODEL = SVC\n",
    "BEST_SVM_HARD = list(ParameterGrid({\n",
    "    \"C\": [0.5],\n",
    "    \"kernel\": [\"sigmoid\"],\n",
    "    \"gamma\": [\"scale\"],\n",
    "    \"class_weight\": ['balanced']\n",
    "}))\n",
    "BEST_SVM_SOFT = list(ParameterGrid({\n",
    "    \"C\": [0.5],\n",
    "    \"kernel\": [\"sigmoid\"],\n",
    "    \"gamma\": [\"scale\"],\n",
    "    \"class_weight\": ['balanced'],\n",
    "    \"probability\": [True]\n",
    "}))\n",
    "SVM_VECT = TfidfVectorizer\n",
    "BEST_SVM_VECT = list(ParameterGrid({\n",
    "    \"analyzer\": [\"word\"],\n",
    "    \"lowercase\": [True],\n",
    "    \"ngram_range\": [(1,2)],\n",
    "    \"max_df\": [0.25],\n",
    "    \"min_df\": [1]\n",
    "}))\n",
    "BEST_SVM_SENTENCE = 'cleaned_sentence'\n",
    "SVM_HARD = [SVM_MODEL, BEST_SVM_HARD, SVM_VECT, BEST_SVM_VECT, BEST_SVM_SENTENCE]\n",
    "SVM_SOFT = [SVM_MODEL, BEST_SVM_SOFT, SVM_VECT, BEST_SVM_VECT, BEST_SVM_SENTENCE]\n",
    "\n",
    "NB_MODEL = MultinomialNB\n",
    "BEST_NB = list(ParameterGrid({'alpha':[0.5]}))\n",
    "NB_VECT = CountVectorizer\n",
    "BEST_NB_VECT = list(ParameterGrid({\n",
    "    \"analyzer\": [\"word\"],\n",
    "    \"lowercase\": [True],\n",
    "    \"ngram_range\": [(1,1)],\n",
    "    \"max_df\": [0.25],\n",
    "    \"min_df\": [1]\n",
    "}))\n",
    "BEST_NB_SENTENCE = 'sentence'\n",
    "NB = [NB_MODEL, BEST_NB, NB_VECT, BEST_NB_VECT, BEST_NB_SENTENCE]\n",
    "\n",
    "RF_MODEL = RandomForestClassifier\n",
    "BEST_RF = list(ParameterGrid({\n",
    "        \"criterion\": [\"entropy\"],\n",
    "        \"min_samples_split\": [5],\n",
    "        \"class_weight\": ['balanced'],\n",
    "        \"max_features\": [\"log2\"],\n",
    "        \"min_samples_leaf\": [2]\n",
    "    }))\n",
    "RF_VECT = TfidfVectorizer\n",
    "BEST_RF_VECT = list(ParameterGrid({\n",
    "    \"analyzer\": [\"word\"],\n",
    "    \"lowercase\": [True],\n",
    "    \"ngram_range\": [(1,2)],\n",
    "    \"max_df\": [0.25],\n",
    "    \"min_df\": [10]\n",
    "}))\n",
    "BEST_RF_SENTENCE = 'cleaned_sentence'\n",
    "RF = [RF_MODEL, BEST_RF, RF_VECT,BEST_RF_VECT, BEST_RF_SENTENCE]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ensemble_hard = [LR, SVM_HARD, NB, RF]\n",
    "ensemble_soft = [LR, SVM_SOFT, NB, RF]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def oversample_smote(X,y):\n",
    "    smote = SMOTE(random_state = 4103)\n",
    "    X, y = smote.fit_resample(X, y)\n",
    "    return X,y\n",
    "\n",
    "def vectorize_helper(vect, sentence_version):\n",
    "    vec_train = vect.fit_transform(train[sentence_version])\n",
    "    vec_val = vect.transform(val[sentence_version])\n",
    "    vec_test = vect.transform(test[sentence_version])\n",
    "    vec_trainval = vect.transform(trainval[sentence_version])\n",
    "    \n",
    "    vec_train_oversampled = oversample_smote(vec_train, labels[0])\n",
    "    vec_trainval_oversampled = oversample_smote(vec_trainval, labels[3])\n",
    "    return vec_train_oversampled, vec_val, vec_test, vec_trainval_oversampled"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# BERT\n",
    "LR_BERT = LogisticRegression\n",
    "BEST_LR_BERT = list(ParameterGrid({\n",
    "    \"C\": [0.1],\n",
    "    'class_weight': ['balanced'],\n",
    "    'penalty':['l2'],\n",
    "    'solver':['lbfgs']\n",
    "}))\n",
    "LR_BERT_VECT = BertClient(check_length=False)\n",
    "bert_train = LR_BERT_VECT.encode(list(train.sentence_no_punc))\n",
    "bert_val = LR_BERT_VECT.encode(list(val.sentence_no_punc))\n",
    "\n",
    "#oversample\n",
    "bert_train_smote,bert_train_smote_y = oversample_smote(bert_train, train.relevance)\n",
    "\n",
    "LR_BERT_MODEL= LogisticRegression(**BEST_LR_BERT[0])\n",
    "LR_BERT_MODEL.fit(bert_train_smote, bert_train_smote_y)\n",
    "LR_BERT_VAL_PRED = LR_BERT_MODEL.predict(bert_val)\n",
    "LR_BERT_VAL_PRED_SOFT = LR_BERT_MODEL.predict_proba(bert_val)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hard Voting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generate_ensemble_pred_hard(ensemble):\n",
    "    df_val = [LR_BERT_VAL_PRED]\n",
    "    for model_fn, model_param, vect_fn, vect_param, sentence_ver in ensemble:\n",
    "        for vp in vect_param:\n",
    "            vect = vect_fn(**vp)\n",
    "        train_os, val, test, trainval_os = vectorize_helper(vect, sentence_ver)\n",
    "        train_label = train_os[1]\n",
    "        val_label = labels[1]\n",
    "        test_label = labels[2]\n",
    "        trainval_label = trainval_os[1]\n",
    "        \n",
    "        # val pred\n",
    "        for mp in model_param:\n",
    "            model_tv = model_fn(**mp)\n",
    "        model_tv.fit(train_os[0], train_label)\n",
    "        val_pred = model_tv.predict(val)\n",
    "        df_val.append(val_pred)        \n",
    "    val_pred = pd.DataFrame(df_val).T  \n",
    "    val_pred.columns = ['lr_bert', 'lr', 'svm', 'nb', 'rf']\n",
    "    return val_pred  \n",
    "\n",
    "def get_majority_pred_hard(df):\n",
    "    final_pred = []\n",
    "    for j in df.iterrows():\n",
    "        lst = list(i for i in j[1])\n",
    "        pred = max(set(lst), key=lst.count)\n",
    "        final_pred.append(pred)\n",
    "    return final_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hard_df = generate_ensemble_pred_hard(ensemble_hard)\n",
    "hard_pred = get_majority_pred_hard(hard_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "classification_report(labels[1], hard_pred, output_dict=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Soft Voting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generate_ensemble_pred_soft(ensemble):\n",
    "    df_val = [LR_BERT_VAL_PRED_SOFT]\n",
    "    for model_fn, model_param, vect_fn, vect_param, sentence_ver in ensemble:\n",
    "        for vp in vect_param:\n",
    "            vect = vect_fn(**vp)\n",
    "        train_os, val, test, trainval_os = vectorize_helper(vect, sentence_ver)\n",
    "        train_label = train_os[1]\n",
    "        val_label = labels[1]\n",
    "        test_label = labels[2]\n",
    "        trainval_label = trainval_os[1]\n",
    "        \n",
    "        # val pred\n",
    "        for mp in model_param:\n",
    "            model_tv = model_fn(**mp)\n",
    "        model_tv.fit(train_os[0], train_label)\n",
    "        val_pred = model_tv.predict_proba(val)\n",
    "        df_val.append(val_pred)  \n",
    "    val_pred = pd.concat([pd.DataFrame(df_val[i]) for i in range(0,5)], axis=1)\n",
    "    cols = ['bert_lr_0', 'bert_lr_1',\n",
    "            'lr_0', 'lr_1', \n",
    "            'svm_0', 'svm_1',\n",
    "            'nb_0', 'nb_1', \n",
    "            'rf_0', 'rf_1'\n",
    "           ]\n",
    "    val_pred.columns = cols\n",
    "    return val_pred\n",
    "\n",
    "def sum_probs(df):\n",
    "    df['0_total'] = df['bert_lr_0'] + df['lr_0'] + df['svm_0'] + df['nb_0'] + df['rf_0']\n",
    "    df['1_total'] = df['bert_lr_1'] + df['lr_1'] + df['svm_1'] + df['nb_1'] + df['rf_1']\n",
    "    probs = df[['0_total','1_total']]\n",
    "    return probs\n",
    "\n",
    "def get_majority_pred_soft(df):\n",
    "    final_pred = []\n",
    "    for i in df.iterrows():\n",
    "        lst = [j for j in i[1]]   \n",
    "        max_value = max(lst)\n",
    "        soft_pred = lst.index(max_value)\n",
    "        final_pred.append(soft_pred)\n",
    "    return final_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "soft_df = generate_ensemble_pred_soft(ensemble_soft)\n",
    "soft_sum_probs = sum_probs(soft_df)\n",
    "all_pred_soft = get_majority_pred_soft(soft_sum_probs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "classification_report(labels[1], all_pred_soft, output_dict=True)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('testenv': conda)"
  },
  "interpreter": {
   "hash": "9f1984dae840b49724e479621db683dc20905a8f43a10def079a271928f3c8cd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}