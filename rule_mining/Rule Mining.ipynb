{
 "cells": [
  {
   "cell_type": "markdown",


   "source": [
    "This notebook serves to extract information from parsed PDF text. The steps are as follows.\n",
    "\n",
    "1. Filter sentences with numbers, new lines in them (if aim is to extract number + key metrics)\n",
    "2. Remove stop words, punctuations, year etc\n",
    "3. Apply part of speech tagging - generate some rules that will allow for extraction of number and metrics. To remember that negative words means that a minus needs to be added in front of the number\n",
    "4. Output for each source -> The metrics, and value"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",


   "source": [
    "# Import Packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "from utils import *"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",

   "source": [
    "# Rule mining"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",

   "source": [
    "test.txt from compiled sentences from word embeddings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df = pd.read_csv('../data/mining/text_ext_labelled_final.csv')\n",
    "df = df.astype(str)\n",
    "df = df.fillna(\"\")\n",
    "test_set_preproc_no_lemma, test_set_preproc_lemma  = [], []\n",
    "for row in df.iterrows():\n",
    "    line, labelled = row[1][0], [i for i in row[1][1:]]\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    # append no lemma version\n",
    "    test_set_preproc_no_lemma.append([line.lower(), preprocess(line.lower(), False), labelled])\n",
    "    #lemmatize\n",
    "    #append lemma version\n",
    "    test_set_preproc_lemma.append([line.lower(), preprocess(line.lower(), True), labelled])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",

   "source": [
    "def process_list_nb(test_set_preproc):\n",
    "    tags = []\n",
    "    for i,j,k in test_set_preproc: \n",
    "        tk_org, pos_org, tag_org = pos_extraction(i)\n",
    "        tk, pos, tag = pos_extraction(j)\n",
    "        tags.append([i, j, tk_org, pos, tag, k]) \n",
    "    return tags"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tags_lemma = process_list_nb(test_set_preproc_lemma)\n",
    "tags_no_lemma = process_list_nb(test_set_preproc_no_lemma)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",

   "source": [
    "# Method"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",

   "source": [
    "1. Store a list, keeping the number and extracted portion\n",
    "> Alternative is to store a set and disregard the number in case multiple numbers in one sentence and we dedup the extracted sentences\n",
    "2. Check left and right until we hit a verb not of finer pos tag 'VBP' or 'VB'. If in between this checking there are no NOUNS, continue checking after this VERB (exclude noun directly before and after). \n",
    "3. Get the indices of the tag we need to slice to retrieve from the tokens_list (must use this method since pos now has punctuations & stopwords)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",

   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_text(tags, verb_exclude): \n",
    "    tokens, pos_list, tag_list = tags[2], tags[3], tags[4]\n",
    "    results = []\n",
    "    for i in range(len(pos_list)):\n",
    "        pos = pos_list[i]\n",
    "        tag = tag_list[i]\n",
    "        tok = tokens[i]\n",
    "        if pos == 'NUM' and line_has_digits(tokens[i].text): #million recognised as a NUM\n",
    "            j,k = extract_text_numbers(pos_list, tag_list, verb_exclude, i)\n",
    "            results.append([tokens[i].text, generate_extracted_text(tokens, pos_list, j,k)])\n",
    "        if (pos == 'DET' and tag == 'PDT') or tok.text in special_ext_tokens:\n",
    "            j,k = extract_text_quant_words(pos_list, tag_list, i)\n",
    "            results.append([tokens[i].text, generate_extracted_text(tokens, pos_list, j,k)])\n",
    "    return results    \n",
    "\n",
    "def extract_text_numbers(pos_list, tag_list, verb_exclude, i):\n",
    "    j = max(i-1,0)\n",
    "    k = min(i+1, len(pos_list)-1)    \n",
    "    noun_flag_left, noun_flag_right = False, False\n",
    "    if j != 0:\n",
    "        while pos_list[j] != 'VERB' or noun_flag_left == False or tag_list[j] in verb_exclude:\n",
    "            if pos_list[j] == 'NOUN':\n",
    "                noun_flag_left = True\n",
    "            j -= 1\n",
    "            if j == 0:\n",
    "                break\n",
    "    if k != len(pos_list)-1:\n",
    "        while pos_list[k] != 'VERB' or noun_flag_right == False or tag_list[k] in verb_exclude:\n",
    "            if pos_list[k] == 'NOUN':\n",
    "                noun_flag_right = True\n",
    "            k += 1\n",
    "            if k == len(pos_list)-1:\n",
    "                break\n",
    "    return j,k\n",
    "\n",
    "def extract_text_quant_words(pos_list, tag_list, i):\n",
    "    j = max(i-1,0)\n",
    "    k = min(i+1, len(pos_list)-1)    \n",
    "    adj_flag_left, adj_flag_right = False, False\n",
    "    if j != 0:\n",
    "        while pos_list[j] != 'NOUN' or adj_flag_left == False:\n",
    "            if pos_list[j] == 'ADJ':\n",
    "                adj_flag_left = True\n",
    "            j -= 1\n",
    "            if j == 0:\n",
    "                break\n",
    "    if k != len(pos_list)-1:\n",
    "        while pos_list[k] != 'NOUN' or adj_flag_right== False:\n",
    "            if pos_list[k] == 'ADJ':\n",
    "                adj_flag_right = True\n",
    "            k += 1\n",
    "            if k == len(pos_list)-1:\n",
    "                break\n",
    "    return j,k\n",
    "    \n",
    "def generate_extracted_text(tokens, pos_list, j, k): #need to write if not simply joining will give extra spaces\n",
    "    extracted_text = ''\n",
    "    for tk in range(j,k):\n",
    "        if pos_list[tk] != 'PUNCT' and pos_list[tk] != 'PART':\n",
    "            extracted_text += ' '\n",
    "        extracted_text += tokens[tk].text\n",
    "    return extracted_text.strip()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def scoring_extraction(extracted_text, labelled_text, v_list):\n",
    "    index = len(extracted_text)\n",
    "    for i in range(index):\n",
    "        v_list[0] += len(labelled_text[i].split(\" \"))\n",
    "        v_list[1] += abs(len(extracted_text[i][1].split(\" \"))- len(labelled_text[i].split(\" \")))\n",
    "    return v_list\n",
    "\n",
    "def print_accuracy(v):\n",
    "    matched = round(100*(1-(v[1]/v[0])),2)\n",
    "    print(\"The percentage of words accurately extracted is \"+ str(matched)+ \"%\")\n",
    "\n",
    "def extract_and_score_text(tags):\n",
    "    l = []\n",
    "    v0, v1, v2, v3 = [0,0], [0,0], [0,0], [0,0]\n",
    "    for info in tags:\n",
    "        result_noexclusion = extract_text(info, [])\n",
    "        result_verbfg_exclusion1 = extract_text(info, ['VBP']) # best\n",
    "        result_verbfg_exclusion2 = extract_text(info, ['VB'])\n",
    "        result_verbfg_exclusion3 = extract_text(info, ['VBP', 'VB'])\n",
    "        v0 = scoring_extraction(result_noexclusion, info[5], v0)\n",
    "        v1 = scoring_extraction(result_verbfg_exclusion1, info[5], v1)\n",
    "        v2 = scoring_extraction(result_verbfg_exclusion2, info[5], v2)\n",
    "        v3 = scoring_extraction(result_verbfg_exclusion3, info[5], v3)\n",
    "        l.append([info[0], result_noexclusion, result_verbfg_exclusion1, result_verbfg_exclusion2, result_verbfg_exclusion3])\n",
    "    print_accuracy(v0)\n",
    "    print_accuracy(v1)\n",
    "    print_accuracy(v2)\n",
    "    print_accuracy(v3)\n",
    "    return l"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "source": [
    "print(\"print running lemma version\")\n",
    "df_lemma = extract_and_score_text(tags_lemma)\n",
    "print(\"print running non-lemma version\")\n",
    "df_no_lemma = extract_and_score_text(tags_no_lemma)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "source": [
    "pd.DataFrame(df_lemma).to_csv('test_final.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "008e83f9660cc2c6b74ec749cb321b12384d3e1ed2527a0588d1b73db62f72fc"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}