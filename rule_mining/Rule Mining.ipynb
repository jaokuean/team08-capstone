{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee7b3682",
   "metadata": {},
   "source": [
    "This notebook serves to extract information from parsed PDF text. The steps are as follows.\n",
    "\n",
    "1. Filter sentences with numbers, new lines in them (if aim is to extract number + key metrics)\n",
    "2. Remove stop words, punctuations, year etc\n",
    "3. Apply part of speech tagging - generate some rules that will allow for extraction of number and metrics. To remember that negative words means that a minus needs to be added in front of the number\n",
    "4. Output for each source -> The metrics, and value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eae53e",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf6c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import re\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5b5c8a",
   "metadata": {},
   "source": [
    "Retrieve stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cce383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "stopwords_to_keep = set(['above', 'below', 'up', 'down', 'over', 'under'])\n",
    "final_stopwords = nltk_stopwords - stopwords_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af3e563",
   "metadata": {},
   "source": [
    "Retrieve punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb3439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_punc = string.punctuation\n",
    "final_punc = ''.join(list(i for i in string_punc if i not in ['%', '$', '&']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fbaaed",
   "metadata": {},
   "source": [
    "# Import Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf94c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../data/citi.json',) #can change to os directory method later to process all text\n",
    "data = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9310d095",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5b1fe",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc5ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this filters out those lines with possible metrics\n",
    "def lines_with_digits(data): #filter out data using length first!!!see \n",
    "    filtered = []\n",
    "    for i in data['content']: \n",
    "        if 'carbon' not in i.lower() and 'co2' not in i.lower() and 'renewable' not in i.lower() and 'green' not in i.lower(): \n",
    "            # We can filter manually here, or we can do labelling (binary logreg should be high accuracy). \n",
    "            # Though I think modelling is overkill, filtering manually is more feasbile\n",
    "            continue\n",
    "        line = remove_year(i)\n",
    "        for j in line:\n",
    "            if j.isdigit(): # this step filters 25% of the data\n",
    "                filtered.append(i)\n",
    "                break\n",
    "    return filtered\n",
    "\n",
    "# preproc for pos-tagging\n",
    "def preprocess(text, stopwords, punc):\n",
    "    removed = remove_year(text)\n",
    "    removed = remove_stopwords(removed, stopwords)\n",
    "    removed = remove_punctuations(removed, punc)\n",
    "    return removed \n",
    "\n",
    "# this removes years from the string (prevent inteference with pos extraction)\n",
    "def remove_year(text):\n",
    "    removed = re.sub(r'[1-2][0-9]{3}', '', text) # remove year between 1000 and 2999\n",
    "    return removed\n",
    "\n",
    "# this remove stop words specific to use case\n",
    "def remove_stopwords(text, stopwords):\n",
    "    words = word_tokenize(text)\n",
    "    keep = []\n",
    "    for i in words:\n",
    "        if i not in stopwords:\n",
    "            keep.append(i)\n",
    "    return ' '.join(keep)\n",
    "    \n",
    "# this removes punctuations \n",
    "def remove_punctuations(text, punc):\n",
    "    subbed = text.translate(str.maketrans('', '', punc))\n",
    "    #remove extra white space\n",
    "    formatted_string = re.sub('\\s{2,}', ' ', subbed)\n",
    "    return formatted_string\n",
    "\n",
    "# this extracts out the pos tag for each token in thhe cleaned words\n",
    "nlp = en_core_web_sm.load()\n",
    "def pos_extraction(text):\n",
    "    tokens, pos, tag = [], [], []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        tokens.append(token)\n",
    "        pos.append(token.pos_)\n",
    "        tag.append(token.tag_)\n",
    "    return tokens,pos,tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34491dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_text = lines_with_digits(data)\n",
    "processed = [preprocess(i, final_stopwords, final_punc) for i in relevant_text]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
