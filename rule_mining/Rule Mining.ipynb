{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43cd7b95",
   "metadata": {},
   "source": [
    "This notebook serves to extract information from parsed PDF text. The steps are as follows.\n",
    "\n",
    "1. Filter sentences with numbers, new lines in them (if aim is to extract number + key metrics)\n",
    "2. Remove stop words, punctuations, year etc\n",
    "3. Apply part of speech tagging - generate some rules that will allow for extraction of number and metrics. To remember that negative words means that a minus needs to be added in front of the number\n",
    "4. Output for each source -> The metrics, and value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eb82c8",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec79369a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde0c95b",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacd9374",
   "metadata": {},
   "source": [
    "# Rule mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9171ad",
   "metadata": {},
   "source": [
    "test.txt from compiled sentences from word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9b2fbf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/mining/text_ext_labelled_final.csv')\n",
    "df = df.astype(str)\n",
    "df = df.fillna(\"\")\n",
    "test_set_preproc_no_lemma, test_set_preproc_lemma  = [], []\n",
    "for row in df.iterrows():\n",
    "    line, labelled = row[1][0], [i for i in row[1][1:]]\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    # append no lemma version\n",
    "    test_set_preproc_no_lemma.append([line.lower(), preprocess(line.lower(), False), labelled])\n",
    "    #lemmatize\n",
    "    #append lemma version\n",
    "    test_set_preproc_lemma.append([line.lower(), preprocess(line.lower(), True), labelled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4daaf90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_lemma = process_list(test_set_preproc_lemma)\n",
    "tags_no_lemma = process_list(test_set_preproc_no_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "501d6ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impact -> PROPN,NNP\n",
      "Investment -> PROPN,NNP\n",
      "Fund -> PROPN,NNP\n",
      "Climate -> PROPN,NNP\n",
      "& -> CCONJ,CC\n",
      "Biodiversity -> PROPN,NNP\n",
      "size -> NOUN,NN\n",
      "double -> ADJ,JJ\n",
      "to -> ADP,IN\n",
      "us$ -> SYM,$\n",
      "350 -> NUM,CD\n",
      "million -> NUM,CD\n"
     ]
    }
   ],
   "source": [
    "a,b,c = pos_extraction('Impact Investment Fund Climate & Biodiversity size double to us$ 350 million')\n",
    "pos_printer(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b58fa7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by -> ADP,IN\n",
      "2025 -> NUM,CD\n",
      ", -> PUNCT,,\n",
      "AP7 -> PROPN,NNP\n",
      "will -> VERB,MD\n",
      "have -> AUX,VB\n",
      "double -> DET,PDT\n",
      "the -> DET,DT\n",
      "proportion -> NOUN,NN\n",
      "of -> ADP,IN\n",
      "green -> ADJ,JJ\n",
      "investment -> NOUN,NN\n",
      "compare -> VERB,VBP\n",
      "with -> ADP,IN\n",
      "2020 -> NUM,CD\n",
      ". -> PUNCT,.\n"
     ]
    }
   ],
   "source": [
    "a,b,c = pos_extraction('by 2025 , AP7 will have double the proportion of green investment compare with 2020 .')\n",
    "pos_printer(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "562b87e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consistent -> ADJ,JJ\n",
      "with -> ADP,IN\n",
      "BlackRocks -> PROPN,NNP\n",
      "goal -> NOUN,NN\n",
      "to -> PART,TO\n",
      "double -> ADJ,JJ\n",
      "offering -> NOUN,NN\n",
      "of -> ADP,IN\n",
      "sustainable -> ADJ,JJ\n",
      "etf -> NOUN,NN\n",
      "( -> PUNCT,-LRB-\n",
      "to -> ADP,IN\n",
      "150 -> NUM,CD\n",
      ") -> PUNCT,-RRB-\n",
      ", -> PUNCT,,\n",
      "iShares -> PROPN,NNP\n",
      "launch -> VERB,VBP\n",
      "over -> ADP,IN\n",
      "45 -> NUM,CD\n",
      "new -> ADJ,JJ\n",
      "sustainable -> ADJ,JJ\n",
      "etf -> NOUN,NN\n",
      "across -> ADP,IN\n",
      "the -> DET,DT\n",
      "US -> PROPN,NNP\n",
      ", -> PUNCT,,\n",
      "Europe -> PROPN,NNP\n",
      ", -> PUNCT,,\n",
      "and -> CCONJ,CC\n",
      "Canada -> PROPN,NNP\n",
      "in -> ADP,IN\n",
      "2020 -> NUM,CD\n",
      ". -> PUNCT,.\n"
     ]
    }
   ],
   "source": [
    "a,b,c = pos_extraction('consistent with BlackRocks goal to double offering of sustainable etf ( to 150 ) , iShares launch over 45 new sustainable etf across the US , Europe , and Canada in 2020 .')\n",
    "pos_printer(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "072267ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-PRON- -> PUNCT,:\n",
      "client -> NOUN,NN\n",
      "from -> ADP,IN\n",
      "around -> ADP,IN\n",
      "the -> DET,DT\n",
      "world -> NOUN,NN\n",
      "recently -> ADV,RB\n",
      "report -> VERB,VBP\n",
      "plan -> NOUN,NN\n",
      "to -> PART,TO\n",
      "double -> VERB,VB\n",
      "-PRON- -> PUNCT,HYPH\n",
      "sustainable -> ADJ,JJ\n",
      "asset -> NOUN,NN\n",
      "over -> ADP,IN\n",
      "the -> DET,DT\n",
      "next -> ADJ,JJ\n",
      "five -> NUM,CD\n",
      "year -> NOUN,NN\n",
      "with -> ADP,IN\n",
      "climate -> NOUN,NN\n",
      "change -> NOUN,NN\n",
      "highlight -> NOUN,NN\n",
      "as -> SCONJ,IN\n",
      "the -> DET,DT\n",
      "most -> ADV,RBS\n",
      "prominent -> ADJ,JJ\n",
      "sustainability -> NOUN,NN\n",
      "issue -> NOUN,NN\n",
      ". -> PUNCT,.\n"
     ]
    }
   ],
   "source": [
    "a,b,c = pos_extraction('-PRON- client from around the world recently report plan to double -PRON- sustainable asset over the next five year with climate change highlight as the most prominent sustainability issue .')\n",
    "pos_printer(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7625c3e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sustainability -> PROPN,NNP\n",
      "target -> PROPN,NNP\n",
      "show -> PROPN,NNP\n",
      "result -> VERB,VBP\n",
      "ten -> NUM,CD\n",
      "year -> NOUN,NN\n",
      "with -> ADP,IN\n",
      "the -> DET,DT\n",
      "AP -> PROPN,NNP\n",
      "fund -> NOUN,NN\n",
      "Council -> PROPN,NNP\n",
      "on -> ADP,IN\n",
      "Ethics -> PROPN,NNP\n",
      "AP3 -> PROPN,NNP\n",
      "be -> AUX,VB\n",
      "well -> ADV,RB\n",
      "on -> ADP,IN\n",
      "-PRON- -> PUNCT,:\n",
      "way -> NOUN,NN\n",
      "to -> PART,TO\n",
      "achieve -> VERB,VB\n",
      "the -> DET,DT\n",
      "four -> NUM,CD\n",
      "sustaina -> NOUN,NN\n",
      "- -> PUNCT,HYPH\n",
      "bility -> NOUN,NN\n",
      "target -> NOUN,NN\n",
      "set -> VERB,VBN\n",
      "for -> ADP,IN\n",
      "2018 -> NUM,CD\n",
      "where -> ADV,WRB\n",
      "investment -> NOUN,NN\n",
      "in -> ADP,IN\n",
      "green -> ADJ,JJ\n",
      "bond -> NOUN,NN\n",
      "have -> AUX,VBP\n",
      "triple -> ADJ,JJ\n",
      "since -> SCONJ,IN\n",
      "2014 -> NUM,CD\n",
      ". -> PUNCT,.\n"
     ]
    }
   ],
   "source": [
    "a,b,c = pos_extraction('sustainability target show result ten year with the AP fund Council on Ethics AP3 be well on -PRON- way to achieve the four sustaina - bility target set for 2018 where investment in green bond have triple since 2014 .')\n",
    "pos_printer(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "50c62143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the -> DET,DT\n",
      "carbon -> NOUN,NN\n",
      "footprint -> NOUN,NN\n",
      "of -> ADP,IN\n",
      "the -> DET,DT\n",
      "company -> NOUN,NN\n",
      "in -> ADP,IN\n",
      "-PRON- -> PUNCT,-LRB-\n",
      "equity -> NOUN,NN\n",
      "portfolio -> NOUN,NN\n",
      "be -> AUX,VB\n",
      "at -> ADP,IN\n",
      "about -> ADP,IN\n",
      "the -> DET,DT\n",
      "same -> ADJ,JJ\n",
      "level -> NOUN,NN\n",
      "as -> SCONJ,IN\n",
      "in -> ADP,IN\n",
      "2018 -> NUM,CD\n",
      ". -> PUNCT,.\n"
     ]
    }
   ],
   "source": [
    "a,b,c = pos_extraction('the carbon footprint of the company in -PRON- equity portfolio be at about the same level as in 2018 .')\n",
    "pos_printer(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3a179c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the carbon footprint of the company in -PRON- equity portfolio be at about the same level as in 2018 .']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatization(['the carbon footprint of the companies in our equity portfolio was at about the same level as in 2018.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ab65a1",
   "metadata": {},
   "source": [
    "# Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f3405",
   "metadata": {},
   "source": [
    "1. Store a list, keeping the number and extracted portion\n",
    "> Alternative is to store a set and disregard the number in case multiple numbers in one sentence and we dedup the extracted sentences\n",
    "2. Check left and right until we hit a verb not of finer pos tag 'VBP' or 'VB'. If in between this checking there are no NOUNS, continue checking after this VERB (exclude noun directly before and after). \n",
    "3. Get the indices of the tag we need to slice to retrieve from the tokens_list (must use this method since pos now has punctuations & stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d4f071aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_ext_tokens = ['double', 'doubled', 'triple', 'tripled', 'half', 'quarter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0944d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(tags, verb_exclude): \n",
    "    tokens, pos_list, tag_list = tags[2], tags[3], tags[4]\n",
    "    results = []\n",
    "    for i in range(len(pos_list)):\n",
    "        pos = pos_list[i]\n",
    "        tag = tag_list[i]\n",
    "        tok = tokens[i]\n",
    "        if pos == 'NUM' and line_has_digits(tokens[i].text): #million recognised as a NUM\n",
    "            j,k = extract_text_numbers(pos_list, tag_list, verb_exclude, i)\n",
    "            results.append([tokens[i].text, generate_extracted_text(tokens, pos_list, j,k)])\n",
    "        if (pos == 'DET' and tag == 'PDT') or tok.text in special_ext_tokens:\n",
    "            j,k = extract_text_quant_words(pos_list, tag_list, i)\n",
    "            results.append([tokens[i].text, generate_extracted_text(tokens, pos_list, j,k)])\n",
    "    return results    \n",
    "\n",
    "def extract_text_numbers(pos_list, tag_list, verb_exclude, i):\n",
    "    j = max(i-1,0)\n",
    "    k = min(i+1, len(pos_list)-1)    \n",
    "    noun_flag_left, noun_flag_right = False, False\n",
    "    if j != 0:\n",
    "        while pos_list[j] != 'VERB' or noun_flag_left == False or tag_list[j] in verb_exclude:\n",
    "            if pos_list[j] == 'NOUN':\n",
    "                noun_flag_left = True\n",
    "            j -= 1\n",
    "            if j == 0:\n",
    "                break\n",
    "    if k != len(pos_list)-1:\n",
    "        while pos_list[k] != 'VERB' or noun_flag_right == False or tag_list[k] in verb_exclude:\n",
    "            if pos_list[k] == 'NOUN':\n",
    "                noun_flag_right = True\n",
    "            k += 1\n",
    "            if k == len(pos_list)-1:\n",
    "                break\n",
    "    return j,k\n",
    "\n",
    "def extract_text_quant_words(pos_list, tag_list, i):\n",
    "    j = max(i-1,0)\n",
    "    k = min(i+1, len(pos_list)-1)    \n",
    "    adj_flag_left, adj_flag_right = False, False\n",
    "    if j != 0:\n",
    "        while pos_list[j] != 'NOUN' or adj_flag_left == False:\n",
    "            if pos_list[j] == 'ADJ':\n",
    "                adj_flag_left = True\n",
    "            j -= 1\n",
    "            if j == 0:\n",
    "                break\n",
    "    if k != len(pos_list)-1:\n",
    "        while pos_list[k] != 'NOUN' or adj_flag_right== False:\n",
    "            if pos_list[k] == 'ADJ':\n",
    "                adj_flag_right = True\n",
    "            k += 1\n",
    "            if k == len(pos_list)-1:\n",
    "                break\n",
    "    return j,k\n",
    "    \n",
    "def generate_extracted_text(tokens, pos_list, j, k): #need to write if not simply joining will give extra spaces\n",
    "    extracted_text = ''\n",
    "    for tk in range(j,k):\n",
    "        if pos_list[tk] != 'PUNCT' and pos_list[tk] != 'PART':\n",
    "            extracted_text += ' '\n",
    "        extracted_text += tokens[tk].text\n",
    "    return extracted_text.strip()\n",
    "\n",
    "def scoring_extraction(extracted_text, labelled_text, v_list):\n",
    "    index = len(extracted_text)\n",
    "    for i in range(index):\n",
    "        v_list[0] += len(labelled_text[i].split(\" \"))\n",
    "        v_list[1] += abs(len(extracted_text[i][1].split(\" \"))- len(labelled_text[i].split(\" \")))\n",
    "    return v_list\n",
    "\n",
    "def print_accuracy(v):\n",
    "    matched = round(100*(1-(v[1]/v[0])),2)\n",
    "    print(\"The percentage of words accurately extracted is \"+ str(matched)+ \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "83b2a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_score_text(tags):\n",
    "    l = []\n",
    "    v0, v1, v2, v3 = [0,0], [0,0], [0,0], [0,0]\n",
    "    for info in tags:\n",
    "        result_noexclusion = extract_text(info, [])\n",
    "        result_verbfg_exclusion1 = extract_text(info, ['VBP']) # best\n",
    "        result_verbfg_exclusion2 = extract_text(info, ['VB'])\n",
    "        result_verbfg_exclusion3 = extract_text(info, ['VBP', 'VB'])\n",
    "        v0 = scoring_extraction(result_noexclusion, info[5], v0)\n",
    "        v1 = scoring_extraction(result_verbfg_exclusion1, info[5], v1)\n",
    "        v2 = scoring_extraction(result_verbfg_exclusion2, info[5], v2)\n",
    "        v3 = scoring_extraction(result_verbfg_exclusion3, info[5], v3)\n",
    "        l.append([info[0], result_noexclusion, result_verbfg_exclusion1, result_verbfg_exclusion2, result_verbfg_exclusion3])\n",
    "    print_accuracy(v0)\n",
    "    print_accuracy(v1)\n",
    "    print_accuracy(v2)\n",
    "    print_accuracy(v3)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "33c0c75f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print running lemma version\n",
      "The percentage of words accurately extracted is 72.58%\n",
      "The percentage of words accurately extracted is 77.06%\n",
      "The percentage of words accurately extracted is 65.41%\n",
      "The percentage of words accurately extracted is 67.11%\n",
      "print running non-lemma version\n",
      "The percentage of words accurately extracted is 72.85%\n",
      "The percentage of words accurately extracted is 74.28%\n",
      "The percentage of words accurately extracted is 73.03%\n",
      "The percentage of words accurately extracted is 74.46%\n"
     ]
    }
   ],
   "source": [
    "print(\"print running lemma version\")\n",
    "df_lemma = extract_and_score_text(tags_lemma)\n",
    "print(\"print running non-lemma version\")\n",
    "df_no_lemma = extract_and_score_text(tags_no_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c511bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_lemma).to_csv('test_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec15590",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89688cc",
   "metadata": {},
   "source": [
    "# Import Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../data/all_asian_banks_preprocessed_vfinal.json',) #can change to os directory method later to process all text\n",
    "data = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffd0467",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[17]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1462d04",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a202e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 filtering, processing for relevant text\n",
    "relevant_text = get_indices_filter_nondigits(data, decarb_terms)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "008e83f9660cc2c6b74ec749cb321b12384d3e1ed2527a0588d1b73db62f72fc"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
