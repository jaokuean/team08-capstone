{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee7b3682",
   "metadata": {},
   "source": [
    "This notebook serves to extract information from parsed PDF text. The steps are as follows.\n",
    "\n",
    "1. Filter sentences with numbers, new lines in them (if aim is to extract number + key metrics)\n",
    "2. Remove stop words, punctuations, year etc\n",
    "3. Apply part of speech tagging - generate some rules that will allow for extraction of number and metrics. To remember that negative words means that a minus needs to be added in front of the number\n",
    "4. Output for each source -> The metrics, and value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eae53e",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82cf6c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import re\n",
    "import en_core_web_sm\n",
    "import string\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc20368",
   "metadata": {},
   "source": [
    "Retrieve stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52f3b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "stopwords_to_keep = set(['above', 'below', 'up', 'down', 'over', 'under'])\n",
    "final_stopwords = nltk_stopwords - stopwords_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e232c552",
   "metadata": {},
   "source": [
    "Retrieve punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1cc70ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_punc = string.punctuation\n",
    "final_punc = ''.join(list(i for i in string_punc if i not in ['%', '$', '&']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b59ed",
   "metadata": {},
   "source": [
    "Decarbonisation Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a4e44f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decarb_terms = set([\"carbon\",\"co2\",\"environment\",\"GHG emissions\",\"Greenhouse Gas\",\"carbon footprint\",\"carbon emissions\",\"Scope 1\",\"Scope 2\",\"Scope 3\", \"WACI\",\"Carbon Intensity\",\"carbon pricing\",\n",
    "                                  \"net-zero\",\"metrics and targets\",\"TCFD\", \"sustainability goals\",\"decarbonisation\",\"climate\",'energy', 'emission',\n",
    "                                  'emissions', 'renewable', 'carbon', 'fuel','power consumption','green', 'gas', 'green energy', 'sustainable', 'climate',\n",
    "                                  'sustainability', 'environmental', 'environment','GHG','decarbon', 'energy consumption', 'paper consumption',\n",
    "                                  'water consumption', 'carbon intensity', 'waste management', 'electricity consumption', 'cdp', 'global warming',\n",
    "                                  'business travel','climate solutions', \"decarbonization\", 'cvar', \"climate value-at-risk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5b1fe",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46bc5ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this processed json file, flattens and store indices for preprocessed text for easier retrieval of original text later\n",
    "def get_indices_filter_nondigits(data, filter_corpus):\n",
    "    d = {}\n",
    "    for company in data:\n",
    "        d[company['url']] = []\n",
    "        for page_ind in range(len(company['report_sentences_preprocessed'])):\n",
    "            for sentence_ind in range(len(company['report_sentences_preprocessed'][page_ind])):\n",
    "                sentence = company['report_sentences_preprocessed'][page_ind][sentence_ind]\n",
    "                if line_has_digits(sentence) and line_has_decarbonisation(sentence, filter_corpus):\n",
    "                    text = preprocess(sentence)\n",
    "                    d[company['url']].append((sentence, text, (page_ind, sentence_ind)))\n",
    "    return d\n",
    "\n",
    "# this filters out those lines with possible metrics\n",
    "def line_has_digits(sentence): \n",
    "    line = remove_year_co2(sentence)\n",
    "    for j in line:\n",
    "        if j.isdigit(): # this step filters 25% of the data\n",
    "            return True\n",
    "\n",
    "def line_has_decarbonisation(sentence, filter_corpus): #replace with word embeddings later on\n",
    "    words = sentence.split(\" \")\n",
    "    for i in words:\n",
    "        if i in filter_corpus:\n",
    "            return True\n",
    "\n",
    "# preproc for pos-tagging\n",
    "def preprocess(text):\n",
    "    removed = re.sub(r'(\\d{2})/(\\d{2})/(\\d{4})', 'date_dummy', text) # replace with dummy\n",
    "    removed = re.sub(r'(\\d{4})/(\\d{4})', 'year_dummy', removed)\n",
    "    removed = re.sub(r'[1-2][0-9]{3}', 'year_dummy', removed)\n",
    "    removed = removed.strip()\n",
    "    return removed \n",
    "\n",
    "# this removes years from the string (prevent inteference with pos extraction)\n",
    "def remove_year_co2(text):\n",
    "    removed = preprocess(text)\n",
    "    removed = removed.replace(\"co2\", \"\")\n",
    "    return removed\n",
    "\n",
    "# this remove stop words specific to use case\n",
    "# def remove_stopwords(text, stopwords):\n",
    "#     words = word_tokenize(text)\n",
    "#     keep = []\n",
    "#     for i in words:\n",
    "#         if i not in stopwords:\n",
    "#             keep.append(i)\n",
    "#     return ' '.join(keep)\n",
    "    \n",
    "# # this removes punctuations \n",
    "# def remove_punctuations(text, punc):\n",
    "#     subbed = text.translate(str.maketrans('', '', punc))\n",
    "#     #remove extra white space\n",
    "#     formatted_string = re.sub('\\s{2,}', ' ', subbed)\n",
    "#     return formatted_string\n",
    "\n",
    "# this extracts out the pos tag for each token in thhe cleaned words\n",
    "nlp = en_core_web_sm.load()\n",
    "def pos_extraction(text):\n",
    "    tokens, pos, tag = [], [], []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        tokens.append(token)\n",
    "        pos.append(token.pos_)\n",
    "        tag.append(token.tag_)\n",
    "    return tokens, pos, tag \n",
    "\n",
    "# function to tag both original and processed versions of text\n",
    "def tag_both_text(org_text, processed_text):\n",
    "    tk_org, pos_org, tag_org = pos_extraction(org_text)\n",
    "    tk_proc, pos_proc, tag_proc = pos_extraction(processed_text)\n",
    "    return [org_text, processed_text, tk_org, pos_proc, tag_proc]\n",
    "\n",
    "# helper function to print output of pos tagging\n",
    "def pos_printer(token, pos, tag):\n",
    "    for i in range(len(token)):\n",
    "        print(str(token[i])+' -> ' + pos[i] +',' +tag[i])\n",
    "\n",
    "# this extracts out the length of each sentence for better info extraction\n",
    "def filter_length(processed, limit):\n",
    "    pos = []\n",
    "    for i in processed:\n",
    "        if len(i.split(' ')) <=limit:\n",
    "            pos.append(i)\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea18553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(processed):\n",
    "    # get lengths\n",
    "    le = [len(i.split(' ')) for i in processed]\n",
    "\n",
    "    # plot histogram\n",
    "    fig, ax = plt.subplots(figsize =(10, 7))\n",
    "    ax.hist(le, bins = [i for i in range(0,101,5)])\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d244f797",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d47db8",
   "metadata": {},
   "source": [
    "# Rule mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe61fcb7",
   "metadata": {},
   "source": [
    "test.txt from compiled sentences from word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32a22757",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('test.txt', 'r')\n",
    "lines = file1.readlines()\n",
    "test_set_preproc = []\n",
    "for line in lines:\n",
    "    a = line.replace(\"\\n\", \"\")\n",
    "    if a != '':\n",
    "        test_set_preproc.append([a.lower(), preprocess(a.lower(), final_stopwords, final_punc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34a56f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = []\n",
    "for i,j in test_set_preproc: \n",
    "    tk_org, pos_org, tag_org = pos_extraction(i)\n",
    "    tk, pos, tag = pos_extraction(j)\n",
    "    tags.append([i, j, tk_org, pos, tag]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c078fc44",
   "metadata": {},
   "source": [
    "# Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dcad99",
   "metadata": {},
   "source": [
    "1. Store a list, keeping the number and extracted portion\n",
    "> Alternative is to store a set and disregard the number in case multiple numbers in one sentence and we dedup the extracted sentences\n",
    "2. Check left and right until we hit a verb not of finer pos tag 'VBP' or 'VB'. If in between this checking there are no NOUNS, continue checking after this VERB (exclude noun directly before and after). \n",
    "3. Get the indices of the tag we need to slice to retrieve from the tokens_list (must use this method since pos now has punctuations & stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41b026b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(tags, verb_exclude): \n",
    "    tokens, pos_list, tag_list = tags[2], tags[3], tags[4]\n",
    "    results = []\n",
    "    for i in range(len(pos_list)):\n",
    "        pos = pos_list[i]\n",
    "        if pos == 'NUM' and line_has_digits(tokens[i].text): #million recognised as a NUM\n",
    "            j = max(i-1,0)\n",
    "            k = min(i+1, len(pos_list)-1)\n",
    "            noun_flag_left, noun_flag_right = False, False\n",
    "            if j != 0:\n",
    "                while pos_list[j] != 'VERB' or noun_flag_left == False or tag_list[j] in verb_exclude:\n",
    "                    if pos_list[j] == 'NOUN':\n",
    "                        noun_flag_left = True\n",
    "                    j -= 1\n",
    "                    if j == 0:\n",
    "                        break\n",
    "            if k != len(pos_list)-1:\n",
    "                while pos_list[k] != 'VERB' or noun_flag_right == False or tag_list[k] in verb_exclude:\n",
    "                    if pos_list[k] == 'NOUN':\n",
    "                        noun_flag_right = True\n",
    "                    k += 1\n",
    "                    if k == len(pos_list)-1:\n",
    "                        break\n",
    "            results.append([tokens[i].text, generate_extracted_text(tokens, pos_list, j,k)])\n",
    "    return results    \n",
    "\n",
    "def generate_extracted_text(tokens, pos_list, j, k): #need to write if not simply joining will give extra spaces\n",
    "    extracted_text = ''\n",
    "    for tk in range(j,k):\n",
    "        if pos_list[tk] != 'PUNCT' and pos_list[tk] != 'PART':\n",
    "            extracted_text += ' '\n",
    "        extracted_text += tokens[tk].text\n",
    "    return extracted_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9e0a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for info in tags:\n",
    "    result_noexclusion = extract_text(info, [])\n",
    "    result_verbfg_exclusion1 = extract_text(info, ['VBP']) # best\n",
    "    result_verbfg_exclusion2 = extract_text(info, ['VB'])\n",
    "    result_verbfg_exclusion3 = extract_text(info, ['VBP', 'VB'])\n",
    "    l.append([info[0], result_noexclusion, result_verbfg_exclusion1, result_verbfg_exclusion2, result_verbfg_exclusion3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "701bd5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(l).to_csv('test3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbde620",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fbaaed",
   "metadata": {},
   "source": [
    "# Import Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf94c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../data/all_pension_funds_preprocessed.json',) #can change to os directory method later to process all text\n",
    "data = json.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c01db7",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251443c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 filtering, processing for relevant text\n",
    "relevant_text = get_indices_filter_nondigits(data, decarb_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facda2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "d: dictionary storing original text, (page_ind, sentence_ind), [['num being extracted', 'full_text'], ....]\n",
    "'''\n",
    "d = {}\n",
    "for k,v in relevant_text.items():\n",
    "#     print('processing' + k)\n",
    "    for sentence_info in v:\n",
    "        try:\n",
    "            tagged = tag_both_text(sentence_info[0], sentence_info[1]) # [org_text, processed_text, tk_org, pos_proc, tag_proc]\n",
    "            extracted = extract_text(tagged, ['VBP']) #token, extracted portion\n",
    "            final_info = [sentence_info[0], sentence_info[2], extracted]\n",
    "            if k not in d:\n",
    "                d[k] = [final_info]\n",
    "            else:\n",
    "                d[k].append(final_info)\n",
    "        except:\n",
    "            print('error' + ' at ' + sentence_info[0] + ' ' + sentence_info[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
